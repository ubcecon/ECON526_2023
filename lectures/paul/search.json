[
  {
    "objectID": "feed.html",
    "href": "feed.html",
    "title": "Slides",
    "section": "",
    "text": "Difference in Differences II\n\n\nECON526\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nDouble / Debiased Machine Learning\n\n\nECON526\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nECON 526: Quantitative Economics with Data Science Applications\n\n\n\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\nECON526\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nInstrumental Variables\n\n\nECON526\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Causality and Potential Outcomes\n\n\nECON526\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Difference in Differences\n\n\nECON526\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\nECON526\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nMatching\n\n\nECON526\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks\n\n\nECON526\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nSynthetic Control\n\n\nECON526\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nTreatment Heterogeneity and Conditional Effects\n\n\nECON526\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty Quantification\n\n\nECON526\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "moredid.html#setup",
    "href": "moredid.html#setup",
    "title": "Difference in Differences II",
    "section": "Setup",
    "text": "Setup\n\nTwo Many periods, binary treatment in second some periods\nPotential outcomes \\(\\{y_{it}(0),y_{it}(1)\\}_{t=1}^T\\) for \\(i=1,...,N\\)\nTreatment \\(D_{it} \\in \\{0,1\\}\\),\n\n\\(D_{i0} = 0\\) \\(\\forall i\\)\n\\(D_{i1} = 1\\) for some, \\(0\\) for others\n\nObserve \\(y_{it} = y_{it}(0)(1-D_{it}) + D_{it} y_{it}(1)\\)"
  },
  {
    "objectID": "moredid.html#identification",
    "href": "moredid.html#identification",
    "title": "Difference in Differences II",
    "section": "Identification",
    "text": "Identification\n\nSame logic as before, \\[\n\\begin{align*}\nATT_{t,t-s} & = \\Er[y_{it}(1) - \\color{red}{y_{it}(0)} | D_{it} = 1, D_{it-s}=0] \\\\\n& = \\Er[y_{it}(1) - y_{it-s}(0) | D_{it} = 1, D_{it-s}=0] - \\\\\n& \\;\\; -  \\Er[\\color{red}{y_{it}(0)} - y_{t-s}(0) | D_{it}=1, D_{it-s}=0]\n\\end{align*}\n\\]\n\nassume \\(\\Er[\\color{red}{y_{it}(0)} - y_{it-s}(0) | D_{it}=1,  D_{it-s}=0] = \\Er[y_{it}(0) - y_{it-s}(0) | D_{it}=0, D_{it-s}=0]\\)\n\n\n\\[\n\\begin{align*}\nATT_{t,t-s}& = \\Er[y_{it} - y_{it-s} | D_{it}=1, D_{it-s}=0] - \\Er[y_{it} - y_{it-s} | D_{it}=0, D_{it-s}=0]\n\\end{align*}\n\\] - Similarly, can identify various other interpretable average treatment effects conditional on being treated at some times and not others"
  },
  {
    "objectID": "moredid.html#estimation",
    "href": "moredid.html#estimation",
    "title": "Difference in Differences II",
    "section": "Estimation",
    "text": "Estimation\n\nPlugin\nFixed effects? \\[\ny_{it} = \\beta D_{it} + \\alpha_i + \\delta_t + \\epsilon_{it}\n\\] When will \\(\\hat{\\beta}^{FE}\\) consistently estimate some interpretable conditional average of treatment effects?"
  },
  {
    "objectID": "moredid.html#fixed-effects",
    "href": "moredid.html#fixed-effects",
    "title": "Difference in Differences II",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nAs with matching, \\[\n\\begin{align*}\n\\hat{\\beta} = & \\sum_{i=1,t=1}^{n,T} y_{it} \\overbrace{\\frac{\\tilde{D}_{it}}{ \\sum_{i,t} \\tilde{D}_{it}^2 }}^{\\hat{\\omega}_{it}} = \\sum_{i=1,t=1}^{n,T} y_{it}(0) \\hat{\\omega}_{it} + \\sum_{i=1,t=1}^{n,T} D_{it} (y_{it}(1) - y_{it}(0)) \\hat{\\omega}_{it}\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n\\tilde{D}_{it} & = D_{it} - \\frac{1}{n} \\sum_{j=1}^n (D_{jt} - \\frac{1}{T} \\sum_{s=1}^T D_{js}) - \\frac{1}{T} \\sum_{s=1}^T D_{is} \\\\\n& = D_{it} - \\frac{1}{n} \\sum_{j=1}^n D_{jt} - \\frac{1}{T} \\sum_{s=1}^T D_{is} + \\frac{1}{nT} \\sum_{j,s} D_{js}\n\\end{align*}\n\\]\n\n\n\nimports\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nstyle.use(\"fivethirtyeight\")"
  },
  {
    "objectID": "moredid.html#weights",
    "href": "moredid.html#weights",
    "title": "Difference in Differences II",
    "section": "Weights",
    "text": "Weights\n\n\ndef assigntreat(n, T, portiontreated):\n    treated = np.zeros((n, T), dtype=bool)\n    for t in range(1, T):\n        treated[:, t] = treated[:, t - 1]\n        if portiontreated[t] &gt; 0:\n            treated[:, t] = np.logical_or(treated[:, t-1], np.random.rand(n) &lt; portiontreated[t])\n    return treated\n\ndef weights(D):\n    D̈ = D - np.mean(D, axis=0) - np.mean(D, axis=1)[:, np.newaxis] + np.mean(D)\n    ω = D̈ / np.sum(D̈**2)\n    return ω\n\nn = 100\nT = 9\npt = np.zeros(T)\npt[T//2 + 1] = 0.5\nD = assigntreat(n, T,pt)\ny = np.random.randn(n, T)\nweighted_sum = np.sum(y * weights(D))\nprint(weighted_sum)\n\n\n0.08147854863434313"
  },
  {
    "objectID": "moredid.html#weights-with-single-treatment-time",
    "href": "moredid.html#weights-with-single-treatment-time",
    "title": "Difference in Differences II",
    "section": "Weights with Single Treatment Time",
    "text": "Weights with Single Treatment Time\n\n\nCode\ndef plotD(D,ax):\n    n, T = D.shape\n    ax.set(xlabel='time',ylabel='portiontreated')\n    ax.plot(range(1,T+1),D.mean(axis=0))\n    ax\n\ndef plotweights(D, ax):\n    n, T = D.shape\n    ω = weights(D)\n    groups = np.unique(D, axis=0)\n    ax.set(xlabel='time', ylabel='weight')\n\n    for g in groups:\n        i = np.where(np.all(D == g, axis=1))[0][0]\n        wt = ω[i, :]\n        ax.plot(range(1, T+1), wt, marker='o', label=f'Treated {np.sum(g)} times')\n\n    ax.legend()\n    ax\n\ndef plotwd(D):\n    fig, ax = plt.subplots(2,1)\n    ax[0]=plotD(D,ax[0])\n    ax[1]=plotweights(D,ax[1])\n    plt.show()\n\nplotwd(D)"
  },
  {
    "objectID": "moredid.html#weights-with-early-and-late-treated",
    "href": "moredid.html#weights-with-early-and-late-treated",
    "title": "Difference in Differences II",
    "section": "Weights with Early and Late Treated",
    "text": "Weights with Early and Late Treated\n\n\nCode\npt = np.zeros(T)\npt[1] = 0.3\npt[T-2] = 0.6\nD = assigntreat(n,T,pt)\nplotwd(D)"
  },
  {
    "objectID": "moredid.html#sign-reversal",
    "href": "moredid.html#sign-reversal",
    "title": "Difference in Differences II",
    "section": "Sign Reversal",
    "text": "Sign Reversal\n\n\nCode\ndvals = np.unique(D,axis=0)\ndvals.sort()\nATT = np.ones(T)\nATT[0] = 0.0\nATT[T-2:T] = 6.0\nnp.random.seed(6798)\n\ndef simulate(n,T,pt,ATT,sigma=1.0):\n    D = assigntreat(n,T,pt)\n    y = np.random.randn(n,T)*sigma + ATT[np.cumsum(D, axis=1)]\n    df = pd.DataFrame({\n        'id': np.repeat(np.arange(1, n + 1), T),\n        't': np.tile(np.arange(1, T + 1), n),\n        'y': y.flatten(),\n        'D': D.flatten()\n    })\n    return(df)\n\ndf = simulate(n,T,pt,ATT)\nresult = pf.feols('y ~ D | id + t',  df, vcov={\"CRV1\": \"id\"})\nresult.summary()\n\n\n###\n\nEstimation:  OLS\nDep. var.: y, Fixed effects: id+t\nInference:  CRV1\nObservations:  900\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| D             |     -0.526 |        0.249 |    -2.107 |      0.038 | -1.020 |  -0.031 |\n---\nRMSE: 1.287 R2: 0.51 R2 Within: 0.009"
  },
  {
    "objectID": "moredid.html#when-to-worry",
    "href": "moredid.html#when-to-worry",
    "title": "Difference in Differences II",
    "section": "When to worry",
    "text": "When to worry\n\nIf multiple treatment times and treatment heterogeneity\nEven if weights do not have wrong sign, the fixed effects estimate is hard to interpret\nSame logic applies more generally – not just to time\n\nE.g. if have group effects, some treated units in multiple groups, and \\(E[y(1) - y(0) | group]\\) varies"
  },
  {
    "objectID": "moredid.html#what-to-do",
    "href": "moredid.html#what-to-do",
    "title": "Difference in Differences II",
    "section": "What to Do?",
    "text": "What to Do?\n\nFollow identification \\[\n\\begin{align*}\nATT_{t,t-s}& = \\Er[y_{it} - y_{it-s} | D_{it}=1, D_{it-s}=0] - \\Er[y_{it} - y_{it-s} | D_{it}=0, D_{it-s}=0]\n\\end{align*}\n\\] and estimate \\[\n\\begin{align*}\n\\widehat{ATT}_{t,t-s} = & \\frac{\\sum_i y_{it} D_{it}(1-D_{it-s})}{\\sum_i D_{it}(1-D_{it-s})} \\\\\n& - \\frac{\\sum_i y_{it} (1-D_{it})(1-D_{it-s})}{\\sum_i (1-D_{it})(1-D_{it-s})}\n\\end{align*}\n\\] and perhaps some average, e.g. (there are other reasonable weighted averages) \\[\n\\sum_{t=1}^T \\frac{\\sum_i D_{it}}{\\sum_{i,s} D_{i,s}} \\frac{1}{t-1} \\sum_{s=1}^{t-1} \\widehat{ATT}_{t,t-s}\n\\]\n\nCode? Inference? Optimal? (could create it, but there’s an easier way)"
  },
  {
    "objectID": "moredid.html#what-to-do-1",
    "href": "moredid.html#what-to-do-1",
    "title": "Difference in Differences II",
    "section": "What to Do?",
    "text": "What to Do?\n\nOption 1: Use an appropriate package\n\ndifferences\npyfixest (Gardner’s 2-stage estimator or Dube et al (2023) local projections)\ndoubleml (see chapter 16 of Chernozhukov et al. (2024))\nsee https://asjadnaqvi.github.io/DiD/ for more options (but none are python)"
  },
  {
    "objectID": "moredid.html#what-to-do-2",
    "href": "moredid.html#what-to-do-2",
    "title": "Difference in Differences II",
    "section": "What to Do?",
    "text": "What to Do?\n\nOption 2: estimate a correctly specified fixed effects regression (my preferred approach)\nProblem is possible correlation of \\((y_{it}(1) - y_{it}(0))D_{it}\\) with \\(\\tilde{D}_{it}\\)\n\\(\\tilde{D}_{it}\\) is function of \\(t\\) and \\((D_{i1}, ..., D_{iT})\\)\nEstimating separate coefficient for each combination of \\(t\\) and \\((D_{i1}, ..., D_{iT})\\) will eliminate correlation / flexibly model treatment effect heterogeneity"
  },
  {
    "objectID": "moredid.html#regression-with-cohort-time-interactions",
    "href": "moredid.html#regression-with-cohort-time-interactions",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort-time Interactions",
    "text": "Regression with Cohort-time Interactions\n\nEstimate: \\[\ny_{it} = \\sum_{c=1}^C D_{it} 1\\{C_i=c\\} \\beta_{ct} + \\alpha_i + \\delta_t + \\epsilon_{it}\n\\]\n\\(\\hat{\\beta}_{ct}\\) consistently estimates \\(\\Er[y_{it}(1) - y_{it}(0) | C_{i}=c, D_{it}=1]\\) assuming parallel trends holds for all periods \\[\n\\Er[y_{it}(0) - y_{it-s}(0) | C_i=c] = \\Er[y_{it}(0) - y_{it-s}(0) | C_i=c']\n\\] for all \\(t, s, c, c'\\)"
  },
  {
    "objectID": "moredid.html#regression-with-cohort-treat-time-interactions",
    "href": "moredid.html#regression-with-cohort-treat-time-interactions",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort-Treat-Time Interactions",
    "text": "Regression with Cohort-Treat-Time Interactions\n\n\nCode\ndef definecohort(df):\n    # convert dummies into categorical\n    n = len(df.id.unique())\n    T = len(df.t.unique())\n    df = df.set_index(['id','t'])\n    dmat=np.array(df.sort_index().D)\n    dmat=np.array(df.D).reshape(n,T)\n    cohort=dmat.dot(1 &lt;&lt; np.arange(dmat.shape[-1] - 1, -1, -1))\n    cdf = pd.DataFrame({\"id\":np.array(df.index.levels[0]), \"cohort\":pd.Categorical(cohort)})\n    cdf =cdf.set_index('id')\n    df = df.reset_index().set_index('id')\n    df=pd.merge(df, cdf, left_index=True, right_index=True)\n    df=df.reset_index()\n    return(df)\n\ndf = definecohort(df)\n\ndef defineinteractions(df):\n    df['dct'] = 'untreated'\n    df['dct'] = df.apply(lambda x: f\"t{x['t']},c{x['cohort']}\" if x['D'] else f\"untreated\", axis=1)\n    return(df)\n\ndf = defineinteractions(df)\n\npf.feols(\"y ~  dct | id + t\", df, vcov={\"CRV1\": \"id\"}).summary()\n\n\n###\n\nEstimation:  OLS\nDep. var.: y, Fixed effects: id+t\nInference:  CRV1\nObservations:  900\n\n| Coefficient      |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:-----------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| dct[T.t3,c255]   |      0.001 |        0.003 |     0.235 |      0.814 | -0.006 |   0.008 |\n| dct[T.t4,c255]   |     -0.002 |        0.004 |    -0.518 |      0.606 | -0.009 |   0.005 |\n| dct[T.t5,c255]   |     -0.000 |        0.003 |    -0.020 |      0.984 | -0.006 |   0.006 |\n| dct[T.t6,c255]   |     -0.003 |        0.003 |    -1.109 |      0.270 | -0.009 |   0.003 |\n| dct[T.t7,c255]   |     -0.004 |        0.004 |    -0.904 |      0.368 | -0.011 |   0.004 |\n| dct[T.t8,c255]   |      4.996 |        0.003 |  1514.399 |      0.000 |  4.990 |   5.003 |\n| dct[T.t8,c3]     |     -0.001 |        0.004 |    -0.221 |      0.826 | -0.008 |   0.006 |\n| dct[T.t9,c255]   |      4.994 |        0.004 |  1361.232 |      0.000 |  4.987 |   5.002 |\n| dct[T.t9,c3]     |     -0.003 |        0.004 |    -0.682 |      0.497 | -0.010 |   0.005 |\n| dct[T.untreated] |     -1.001 |        0.003 |  -308.141 |      0.000 | -1.007 |  -0.994 |\n---\nRMSE: 0.009 R2: 1.0 R2 Within: 1.0"
  },
  {
    "objectID": "moredid.html#regression-with-cohort-time-interactions-1",
    "href": "moredid.html#regression-with-cohort-time-interactions-1",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort-Time Interactions",
    "text": "Regression with Cohort-Time Interactions\n\nIf just want to assume parallel trends for treated and never treated, i.e. \\[\n\\Er[y_{it}(0) - y_{it-s}(0) | C_i=c] = \\Er[y_{it}(0) - y_{it-s}(0) | C_i=c']\n\\] when \\(c\\) treated at \\(t\\), untreated at \\(t-s\\) and \\(c'\\) never treated\nEstimate \\[\ny_{it} = \\sum_{c=1}^C 1\\{C_i=c\\} \\delta_{c,t} + \\alpha_i + \\epsilon_{it}\n\\]"
  },
  {
    "objectID": "moredid.html#regression-with-cohort-time-interactions-2",
    "href": "moredid.html#regression-with-cohort-time-interactions-2",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort-Time Interactions",
    "text": "Regression with Cohort-Time Interactions\n\\[\ny_{it} = \\sum_{c=1}^C 1\\{C_i=c\\} \\delta_{c,t} + \\alpha_i + \\epsilon_{it}\n\\]\n\n\\(\\hat{\\delta}_{c,t} + \\frac{\\sum \\alpha_i 1\\{C_i=c\\}}{\\sum 1\\{C_i =\nc\\}}\\) consistently estimates \\(\\Er[y_{it} | C_{i} = c]\\)\n\\(\\hat{\\delta}_{c,t} -\\hat{\\delta}_{c,t-s}\\) consistently estimates \\(\\Er[y_{it} - y_{i,t-s}| C_{i} = c]\\)\nIf \\(c\\) treated at \\(t\\), not at \\(t-s\\), and \\(c'\\) not treated at either and assume parallel trends, \\[\n\\hat{\\delta}_{c,t} - \\hat{\\delta}_{c,t-s} - (\\hat{\\delta}_{c',t} -\\hat{\\delta}_{c',t-s}) \\inprob \\Er[y_{it}(1)-y{it}(0)| C_i =c]\n\\]"
  },
  {
    "objectID": "moredid.html#pre-trends-1",
    "href": "moredid.html#pre-trends-1",
    "title": "Difference in Differences II",
    "section": "Pre-trends",
    "text": "Pre-trends\n\nParallel trends assumption\n\n\\[\n\\Er[\\color{red}{y_{it}(0)} - y_{it-s}(0) | D_{it}=1,  D_{it-s}=0] = \\Er[y_{it}(0) - y_{it-s}(0) | D_{it}=0, D_{it-s}=0]\n\\]\n\nMore plausible if there are parallel pre-trends\n\n\\[\n\\begin{align*}\n& \\Er[y_{it-r}(0) - y_{it-s}(0) | D_{it}=1, D_{it-r}=0,  D_{it-s}=0] = \\\\\n& = \\Er[y_{it-r}(0) - y_{it-s}(0) | D_{it}=0, D_{it-r}=0, D_{it-s}=0]\n\\end{align*}\n\\]\n\nAlways at least plot pre-trends"
  },
  {
    "objectID": "moredid.html#testing-for-pre-trends",
    "href": "moredid.html#testing-for-pre-trends",
    "title": "Difference in Differences II",
    "section": "Testing for Pre-trends",
    "text": "Testing for Pre-trends\n\nIs it a good idea to test\n\n\\[\n\\begin{align*}\nH_0 : & \\Er[y_{it-r} - y_{it-s} | D_{it}=1, D_{it-r}=0,  D_{it-s}=0] = \\\\\n& = \\Er[y_{it-r} - y_{it-s} | D_{it}=0, D_{it-r}=0, D_{it-s}=0]?\n\\end{align*}\n\\] - Even if not testing formally, we do it informally by plotting"
  },
  {
    "objectID": "moredid.html#testing-for-pre-trends-1",
    "href": "moredid.html#testing-for-pre-trends-1",
    "title": "Difference in Differences II",
    "section": "Testing for Pre-trends",
    "text": "Testing for Pre-trends\n\nDistribution of \\(\\hat{ATT}\\) conditional on fail to reject parallel pre-trends is not normal\nRoth (2022) : test can have low power, and in plausible violations, \\(\\widehat{ATT}_{3,2}\\) conditional on failing to reject is biased"
  },
  {
    "objectID": "moredid.html#bounds-from-pre-trends",
    "href": "moredid.html#bounds-from-pre-trends",
    "title": "Difference in Differences II",
    "section": "Bounds from Pre-trends",
    "text": "Bounds from Pre-trends\n\nLet \\(\\Delta\\) be violation of parallel trends \\[\n\\Delta = \\Er[\\color{red}{y_{it}(0)} - y_{it-1}(0) | D_{it}=1,  D_{it-1}=0] - \\Er[y_{it}(0) - y_{it-1}(0) | D_{it}=0, D_{it-1}=0]\n\\]\nAssume \\(\\Delta\\) is bounded by deviation from parallel of pre-trends \\[\n|\\Delta| \\leq M \\max_{r} \\left\\vert \\tau^{1t}_{t-r,t-r-1} - \\tau^{0t}_{t-r,t-r-1} \\right\\vert\n\\] for some chosen \\(M\\)\nSee Rambachan and Roth (2023)"
  },
  {
    "objectID": "moredid.html#doubly-robust-difference-in-differences",
    "href": "moredid.html#doubly-robust-difference-in-differences",
    "title": "Difference in Differences II",
    "section": "Doubly Robust Difference in Differences",
    "text": "Doubly Robust Difference in Differences\n\nLinear covariates could lead to same problem as with matching\nDoubly robust estimator Sant’Anna and Zhao (2020)\n\ndoubleml package implements it\nsee example notebook from chapter 16 of Chernozhukov et al. (2024)"
  },
  {
    "objectID": "moredid.html#sources-and-further-reading",
    "href": "moredid.html#sources-and-further-reading",
    "title": "Difference in Differences II",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nFacure (2022, chap. 1)\nHuntington-Klein (2021, chap. 16)\nBook: C. de Chaisemartin and D’Haultfœuille (2023)\nRecent reviews: Roth et al. (2023), Clément de Chaisemartin and D’Haultfœuille (2022), Arkhangelsky and Imbens (2023)\nEarly work pointing to problems with fixed effects:\n\nLaporte and Windmeijer (2005), Wooldridge (2005)\n\nExplosion of papers written just before 2020, published just after:\n\nBorusyak and Jaravel (2018)\nClément de Chaisemartin and D’Haultfœuille (2020)\nCallaway and Sant’Anna (2021)\nGoodman-Bacon (2021)\nSun and Abraham (2021)"
  },
  {
    "objectID": "moredid.html#references",
    "href": "moredid.html#references",
    "title": "Difference in Differences II",
    "section": "References",
    "text": "References\n\n\n\n\nArkhangelsky, Dmitry, and Guido Imbens. 2023. “Causal Models for Longitudinal and Panel Data: A Survey.”\n\n\nBorusyak, Kirill, and Xavier Jaravel. 2018. “Revisiting Event Study Designs.” https://scholar.harvard.edu/files/borusyak/files/borusyak_jaravel_event_studies.pdf.\n\n\nCallaway, Brantly, and Pedro H. C. Sant’Anna. 2021. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics 225 (2): 200–230. https://doi.org/https://doi.org/10.1016/j.jeconom.2020.12.001.\n\n\nChaisemartin, C de, and X D’Haultfœuille. 2023. Credible Answers to Hard Questions: Differences-in-Differences for Natural Experiments. https://dx.doi.org/10.2139/ssrn.4487202.\n\n\nChaisemartin, Clément de, and Xavier D’Haultfœuille. 2020. “Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects.” American Economic Review 110 (9): 2964–96. https://doi.org/10.1257/aer.20181169.\n\n\n———. 2022. “Two-way fixed effects and differences-in-differences with heterogeneous treatment effects: a survey.” The Econometrics Journal 26 (3): C1–30. https://doi.org/10.1093/ectj/utac017.\n\n\nChernozhukov, V., C. Hansen, N. Kallus, M. Spindler, and V. Syrgkanis. 2024. Applied Causal Inference Powered by ML and AI. https://causalml-book.org/.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with Variation in Treatment Timing.” Journal of Econometrics 225 (2): 254–77. https://doi.org/https://doi.org/10.1016/j.jeconom.2021.03.014.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. https://theeffectbook.net/.\n\n\nLaporte, Audrey, and Frank Windmeijer. 2005. “Estimation of Panel Data Models with Binary Indicators When Treatment Effects Are Not Constant over Time.” Economics Letters 88 (3): 389–96. https://doi.org/https://doi.org/10.1016/j.econlet.2005.04.002.\n\n\nRambachan, Ashesh, and Jonathan Roth. 2023. “A More Credible Approach to Parallel Trends.” The Review of Economic Studies 90 (5): 2555–91. https://doi.org/10.1093/restud/rdad018.\n\n\nRoth, Jonathan. 2022. “Pretest with Caution: Event-Study Estimates After Testing for Parallel Trends.” American Economic Review: Insights 4 (3): 305–22. https://doi.org/10.1257/aeri.20210236.\n\n\nRoth, Jonathan, Pedro H. C. Sant’Anna, Alyssa Bilinski, and John Poe. 2023. “What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature.” Journal of Econometrics 235 (2): 2218–44. https://doi.org/https://doi.org/10.1016/j.jeconom.2023.03.008.\n\n\nSant’Anna, Pedro H. C., and Jun Zhao. 2020. “Doubly Robust Difference-in-Differences Estimators.” Journal of Econometrics 219 (1): 101–22. https://doi.org/https://doi.org/10.1016/j.jeconom.2020.06.003.\n\n\nSun, Liyang, and Sarah Abraham. 2021. “Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects.” Journal of Econometrics 225 (2): 175–99. https://doi.org/https://doi.org/10.1016/j.jeconom.2020.09.006.\n\n\nWooldridge, Jeffrey M. 2005. “Fixed-Effects and Related Estimators for Correlated Random-Coefficient and Treatment-Effect Panel Data Models.” The Review of Economics and Statistics 87 (2): 385–90. https://doi.org/10.1162/0034653053970320."
  },
  {
    "objectID": "neuralnets.html#neural-networks",
    "href": "neuralnets.html#neural-networks",
    "title": "Neural Networks",
    "section": "Neural Networks",
    "text": "Neural Networks\n\nFlexible function approximation & regression\nAutomated feature engineering\nExceptional performance on high dimensional data with low noise\n\nText\nImages\nAudio\nVideo"
  },
  {
    "objectID": "neuralnets.html#single-layer-perceptron",
    "href": "neuralnets.html#single-layer-perceptron",
    "title": "Neural Networks",
    "section": "Single Layer Perceptron",
    "text": "Single Layer Perceptron\n\n\\(x_i \\in \\R^d\\), want to approximate some \\(f: \\R^d \\to \\R\\)\nApproximate by \\[\n\\begin{align*}\nf(x_i; \\mathbf{w},\\mathbf{b}) =\n\\psi_1 \\left( \\sum_{u=1}^m w_{u,1} \\psi_0( x_i'w_{u,0} + b_{u,0}) + b_{u,1} \\right)\n\\end{align*}\n\\] where\n\nWeights: \\(w_{u,1} \\in \\R\\), \\(w_{u,0} \\in \\R^d\\)\nBiases: \\(b_{u,1}, b_{u,0} \\in \\R\\)\nActivation functions \\(\\psi_1, \\psi_0: \\R \\to \\R\\)\nWidth: \\(m\\)"
  },
  {
    "objectID": "neuralnets.html#activation-functions",
    "href": "neuralnets.html#activation-functions",
    "title": "Neural Networks",
    "section": "Activation functions",
    "text": "Activation functions\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch\n# plot 4 activation functions\nx = torch.linspace(-5, 5, 100)\nfig, ax = plt.subplots(2, 2, figsize=(10, 8))\nax = ax.flatten()\nax[0].plot(x, nn.ReLU()(x))\nax[0].set_title(\"ReLU\")\nax[1].plot(x, nn.Tanh()(x))\nax[1].set_title(\"Tanh\")\nax[2].plot(x, nn.Sigmoid()(x))\nax[2].set_title(\"Sigmoid\")\nax[3].plot(x, nn.Softplus()(x))\nax[3].set_title(\"Softplus\")\nfig.tight_layout()\nfig.show()"
  },
  {
    "objectID": "neuralnets.html#single-layer-perceptron-1",
    "href": "neuralnets.html#single-layer-perceptron-1",
    "title": "Neural Networks",
    "section": "Single Layer Perceptron",
    "text": "Single Layer Perceptron\n\nimport torch.nn as nn\nimport torch\nclass SingleLayerPerceptron(nn.Module):\n    def __init__(self, d, m, activation=nn.ReLU()):\n        super().__init__()\n        self.d = d\n        self.m = m\n        self.activation = activation\n        self.w0 = nn.Parameter(torch.randn(d, m))\n        self.b0 = nn.Parameter(torch.randn(m))\n        self.w1 = nn.Parameter(torch.randn(m))\n        self.b1 = nn.Parameter(torch.randn(1))\n\n    def forward(self, x):\n        return self.activation(x @ self.w0 + self.b0) @ self.w1 + self.b1"
  },
  {
    "objectID": "neuralnets.html#regression",
    "href": "neuralnets.html#regression",
    "title": "Neural Networks",
    "section": "Regression",
    "text": "Regression\n\nData \\(\\{y_i, x_i\\}_{i=1}^n\\), \\(x_i \\in \\R^d\\), \\(y_i \\in \\R\\)\nLeast squares: \\[\n\\hat{\\mathbf{w}}, \\hat{\\mathbf{b}} \\in \\argmin_{\\mathbf{w},\\mathbf{b}} \\underbrace{\\frac{1}{n} \\sum_{i=1}^n \\left(y_i - f(x_i; \\mathbf{w},\\mathbf{b}) \\right)^2}_{\\text{loss function}}\n\\]\nHard to find global minimum:\n\nLoss not convex\nParameters very high dimensional"
  },
  {
    "objectID": "neuralnets.html#gradient-descent",
    "href": "neuralnets.html#gradient-descent",
    "title": "Neural Networks",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nFind approximate minimum instead \\[\n\\overbrace{\\hat{\\theta}}^{\\hat{\\mathbf{w}}, \\hat{\\mathbf{b}}} \\approx \\argmin_{\\theta} \\ell(y, f(x;\\theta))\n\\]\nStart with random \\(\\theta_1\\)\nRepeatedly update: \\[\n\\theta_{i+1} = \\theta_i - \\gamma \\underbrace{\\nabla \\ell(y,f(;\\theta))}_{\\text{gradient}}\n\\]\nContinue until loss stops improving\n\nOptionally modify \\(\\gamma\\) based on progress"
  },
  {
    "objectID": "neuralnets.html#computing-gradients",
    "href": "neuralnets.html#computing-gradients",
    "title": "Neural Networks",
    "section": "Computing Gradients",
    "text": "Computing Gradients\n\nAutomatic differentiation: automatically use chainrule on each step of computation\nE.g. \\(\\ell(\\theta) = f(g(h(\\theta)))\\)\n\n\\(\\ell : \\R^p \\to \\R\\), \\(h: \\R^p \\to \\R^q\\), \\(g: \\R^q \\to \\R^j\\), \\(f: \\R^j \\to \\R\\) \\[\n\\left(\\nabla \\ell(\\theta)\\riight)^T = \\underbrace{Df_{g(h(\\theta))}}_{1 \\times j} \\underbrace{Dg_{h(\\theta)}}_{j \\times q} \\underbrace{Dh_\\theta}_{q \\times p}\n\\]\n\nForward mode:\n\nCalculate \\(h(\\theta)\\) and \\(D_1=Dh_\\theta\\)\nCalculate \\(g(h(\\theta))\\) and \\(Dg_{h(\\theta)}\\), multiply \\(D_2=Dg_{h(\\theta)} D_1\\) (\\(jqp\\) scalar products and additions)\nCalculate \\(f(g(h(\\theta)))\\) and \\(Df_{g(h(\\theta))}\\), multiply \\(Df_{g(h(\\theta))} D_2\\) (\\(1jp\\) scalar products and additions)\n\n\nWork to propagate derivative \\(\\propto jqp + 1jp\\)"
  },
  {
    "objectID": "neuralnets.html#computing-gradients-1",
    "href": "neuralnets.html#computing-gradients-1",
    "title": "Neural Networks",
    "section": "Computing Gradients",
    "text": "Computing Gradients\n\nReverse mode:\n\n“Forward pass” calculate and save \\(h(\\theta)\\), \\(g(h(\\theta))\\) and \\(f(g(h(\\theta)))\\)\n“Back propagation”:\n\nCalculate \\(D^r_1 = Df_{g(h(\\theta))}\\)\nCalculate \\(D^r_2 = D^r_1 Dg_{h(\\theta)}\\) (\\(1jq\\) scalar products and additions)\nCalculate \\(D^r_3 Dh_{\\theta}\\) (\\(1qp\\) scalar products and additions)\n\n\n\nWork to propagate derivative \\(\\propto 1jq + 1qp\\)\n\nReverse mode much less work when \\(p\\) large"
  },
  {
    "objectID": "neuralnets.html#gradient-descent-1",
    "href": "neuralnets.html#gradient-descent-1",
    "title": "Neural Networks",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\n\nCode\nn = 100\nsigma = 0.1\ntorch.manual_seed(987)\nx = torch.randn(n,1)\nf = np.vectorize(lambda x:  np.sin(x)+np.exp(-x*x)/(1+np.abs(x)))\ny = torch.tensor(f(x), dtype=torch.float32) + sigma*torch.randn(x.shape)\nslp = SingleLayerPerceptron(1, 20, nn.ReLU())\nlossfn = nn.MSELoss()\nxlin = torch.linspace(-3,3,100)\nfig, ax = plt.subplots()\nepochs = 1000\nstepscale = 0.1\nfor i in range(epochs):\n    if (i % 50) == 0 :\n        ax.plot(xlin,slp(xlin.reshape(-1,1)).data, alpha=i/epochs, color='k')\n        slp.zero_grad()\n    loss = lossfn(y.flatten(),slp(x))\n    #print(f\"{i}: {loss.item()}\")\n    # calculate gradient\n    loss.backward()\n    # access gradient & use to descend\n    for p in slp.parameters():\n        if p.requires_grad:\n            p.data = p.data - stepscale*p.grad.data\n            p.grad.data.zero_()\n    slp.zero_grad()\n\nax.plot(xlin, f(xlin), label=\"f\", lw=8)\nax.scatter(x.flatten(),y.flatten())\nfig.show()"
  },
  {
    "objectID": "neuralnets.html#multi-layer-perceptron",
    "href": "neuralnets.html#multi-layer-perceptron",
    "title": "Neural Networks",
    "section": "Multi Layer Perceptron",
    "text": "Multi Layer Perceptron\n\ndef multilayer(d,width,depth,activation=nn.ReLU()):\n    mlp = nn.Sequential(\n        nn.Linear(d,width),\n        activation\n    )\n    for i in range(depth-1):\n        mlp.add_module(f\"layer {i+1}\",nn.Linear(width, width))\n        mlp.add_module(f\"activation {i+1}\",activation)\n\n\n    mlp.add_module(\"output\",nn.Linear(width,1))\n    return(mlp)\n\nmlp=multilayer(2, 10, 1, nn.ReLU())\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
  },
  {
    "objectID": "neuralnets.html#multi-layer-perceptron-1",
    "href": "neuralnets.html#multi-layer-perceptron-1",
    "title": "Neural Networks",
    "section": "Multi Layer Perceptron",
    "text": "Multi Layer Perceptron\n\n\nCode\nmlp = multilayer(1,4,4,nn.ReLU())\nprint(count_parameters(mlp))\noptimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)\nfig, ax = plt.subplots()\nepochs = 1000\nfor i in range(epochs):\n    if (i % 50) == 0 :\n        ax.plot(xlin,mlp(xlin.reshape(-1,1)).data, alpha=i/epochs, color='k')\n        mlp.zero_grad()\n    loss = lossfn(y,mlp(x))\n    #print(f\"{i}: {loss.item()}\")\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nax.plot(xlin, f(xlin), label=\"f\", lw=8)\nax.scatter(x.flatten(),y.flatten())\nfig.show()\n\n\n73"
  },
  {
    "objectID": "neuralnets.html#overparameterization",
    "href": "neuralnets.html#overparameterization",
    "title": "Neural Networks",
    "section": "Overparameterization",
    "text": "Overparameterization\n\nClassic statistics wisdom about nonparametric regression: as parameters increase:\n\nBias decreases\nVariance increases\nNeed parameters \\(&lt;\\) sample size for good performance\n\nNeural networks: often see best performance when parameters \\(&gt;\\) sample size"
  },
  {
    "objectID": "neuralnets.html#double-descent",
    "href": "neuralnets.html#double-descent",
    "title": "Neural Networks",
    "section": "Double Descent",
    "text": "Double Descent\n\n\nCode\nfrom joblib import Parallel, delayed\n#device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice = 'cpu' # problem is small, so not much help from cuda\nnum_epochs = 50\nsims = 100\nmaxw = 10\n\ndef doubledescentdemo(x, y, xtest, ytest, f, device=device,\n                      num_epochs=num_epochs, sims=sims,\n                      maxw=maxw, lr=0.1):\n    x=x.to(device)\n    y=y.to(device).reshape(x.shape[0],1)\n    fx = f(x.T).reshape(y.shape).to(device)\n    ytest = ytest.to(device).reshape(xtest.shape[0],1)\n    xtest = xtest.to(device)\n    loss_fn = nn.MSELoss().to(device)\n\n    losses = np.zeros([maxw,num_epochs,sims])\n    nonoise = np.zeros([maxw,num_epochs,sims])\n\n    def dd(w):\n        mlp = multilayer(x.shape[1],w+1,1,nn.ReLU()).to(device)\n        optimizer = torch.optim.Adam(mlp.parameters(), lr=lr)\n        losses = np.zeros(num_epochs)\n        nonoise=np.zeros(num_epochs)\n        for n in range(num_epochs):\n            y_pred = mlp(x)\n            loss = loss_fn(y_pred, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            losses[n] = loss_fn(mlp(xtest),ytest).item()\n            nonoise[n] = loss_fn(y_pred, fx)\n            mlp.zero_grad()\n        return([losses, nonoise])\n\n    for w in range(maxw):\n        foo=lambda s: dd(w)\n        print(f\"width {w}\")\n        results = Parallel(n_jobs=20)(delayed(foo)(s) for s in range(sims))\n\n        for s in range(sims):\n            losses[w,:,s] = results[s][0]\n            nonoise[w,:,s] = results[s][1]\n    return([losses, nonoise])"
  },
  {
    "objectID": "neuralnets.html#double-descent-1",
    "href": "neuralnets.html#double-descent-1",
    "title": "Neural Networks",
    "section": "Double Descent",
    "text": "Double Descent\n\nf = lambda x: np.exp(x[0]-x[1])\nn = 20\ntorch.manual_seed(1234)\ndimx = 5\nx = torch.rand(n,dimx)\nxtest = torch.rand(n,dimx)\nsigma = 1.5\ny = f(x.T) + sigma*torch.randn(x.shape[0])\nytest = f(xtest.T) + sigma*torch.randn(xtest.shape[0])\nlr = 0.01\ndd = doubledescentdemo(x, y, xtest, ytest, f, lr)\n\nwidth 0\n\n\nwidth 1\nwidth 2\nwidth 3\nwidth 4\nwidth 5\nwidth 6\nwidth 7\nwidth 8\nwidth 9"
  },
  {
    "objectID": "neuralnets.html#double-descent-2",
    "href": "neuralnets.html#double-descent-2",
    "title": "Neural Networks",
    "section": "Double Descent",
    "text": "Double Descent\n\n\nCode\ndef plotdd(losses, nonoise):\n    fig, ax = plt.subplots(2,2)\n    ax = ax.flatten()\n    ax[0].imshow(losses, cmap='plasma')\n    ax[0].set_title('Test Loss')\n    ax[0].set_xlabel('epoch')\n    ax[0].set_ylabel('width')\n    ax[1].imshow(nonoise, cmap='plasma')\n    ax[1].set_title('error in f')\n    ax[1].set_xlabel('epoch')\n    ax[1].set_ylabel('width')\n\n    ws = [0,1, 2, 4, 8]  #10, 12, 19]\n    for w in ws:\n        ax[2].plot(range(losses.shape[1]),losses[w,:], label=f\"width={w+1}\")\n        ax[3].plot(range(nonoise.shape[1]),nonoise[w,:], label=f\"width={w+1}\")\n\n    ax[2].set_xlabel('epoch')\n    ax[2].set_ylabel('test loss')\n    ax[2].legend()\n    ax[3].set_xlabel('epoch')\n    ax[3].set_ylabel('error in f')\n    ax[3].legend()\n\n    fig.tight_layout()\n\n    return(fig)\n\nfig=plotdd(dd[0].mean(axis=2), dd[1].mean(axis=2))\nfig.show()"
  },
  {
    "objectID": "neuralnets.html#double-descent-low-noise",
    "href": "neuralnets.html#double-descent-low-noise",
    "title": "Neural Networks",
    "section": "Double Descent: Low Noise",
    "text": "Double Descent: Low Noise\n\n\nCode\nsigma = 0.01\ny = f(x.T) + sigma*torch.randn(x.shape[0])\nytest = f(xtest.T) + sigma*torch.randn(xtest.shape[0])\nddlowsig = doubledescentdemo(x,y,xtest,ytest,f, lr=0.05)\n\n\nwidth 0\nwidth 1\nwidth 2\nwidth 3\nwidth 4\nwidth 5\nwidth 6\nwidth 7\nwidth 8\nwidth 9"
  },
  {
    "objectID": "neuralnets.html#double-descent-low-noise-1",
    "href": "neuralnets.html#double-descent-low-noise-1",
    "title": "Neural Networks",
    "section": "Double Descent: Low Noise",
    "text": "Double Descent: Low Noise\n\n\nCode\nfig=plotdd(ddlowsig[0].mean(axis=2), ddlowsig[1].mean(axis=2))\nfig.show()"
  },
  {
    "objectID": "neuralnets.html#double-descent-3",
    "href": "neuralnets.html#double-descent-3",
    "title": "Neural Networks",
    "section": "Double Descent",
    "text": "Double Descent\n\nFigures show double descent in number of epochs, but has also been demonstrated with respect to number of parameters1\nThese simulations are quite fragile and depend on:\n\nForm of \\(f\\)\ndimension of \\(x\\)\nnoise in \\(y\\)\nlearning rate\noptimizer\n\n\nIf you squint, you can sort of see double descent in parameters in these simulations too."
  },
  {
    "objectID": "neuralnets.html#software-1",
    "href": "neuralnets.html#software-1",
    "title": "Neural Networks",
    "section": "Software",
    "text": "Software\n\nPyTorch\n\nUsed above\nVery popular\nMiddle ground between convenience and flexibility\nCould be used via higher level wrapper (e.g. skorch)\n\nJAX\n\nMore extensible than pytorch\n\nOthers: Tensorflow, Theano, Keras, and more"
  },
  {
    "objectID": "neuralnets.html#uses-for-neural-networks-1",
    "href": "neuralnets.html#uses-for-neural-networks-1",
    "title": "Neural Networks",
    "section": "Uses for Neural Networks",
    "text": "Uses for Neural Networks\n\nFlexible regression and/or classification method, in double/debiased learning or elsewhere\n\nConsider carefully whether difficulties of fitting are worth benefits\n\nFlexible function approximation\n\nUseful for computing solutions to games, dynamic models, etc\n\nText, images, audio\n\nUse transfer learning"
  },
  {
    "objectID": "neuralnets.html#transfer-learning",
    "href": "neuralnets.html#transfer-learning",
    "title": "Neural Networks",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\nFit (or let researchers or company with more resources fit) large model on large, general dataset\nFind specialized data for your task\nFine tune: take large model and update parameters with your data\nhttps://huggingface.co/ good source for large models to fine-tune"
  },
  {
    "objectID": "neuralnets.html#other-architectures",
    "href": "neuralnets.html#other-architectures",
    "title": "Neural Networks",
    "section": "Other Architectures",
    "text": "Other Architectures\n\nMulti-layer perceptrons / feed forward networks are the simplest neural networks, many extensions and variations exist\nTricks to help with vanishing gradients and numeric stability:\n\nNormalization\nResidual connections\n\nVariations motivated by sequential data:\n\nRecurrent\nTransformers\n\nVariations motivated by images:\n\nConvolutions\nGAN\nDiffusion1\n\n\nThe motivating idea of diffusion models is different than neural networks, but neural networks are commonly used as one of the parts of diffusion models."
  },
  {
    "objectID": "neuralnets.html#sources-and-further-reading",
    "href": "neuralnets.html#sources-and-further-reading",
    "title": "Neural Networks",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nQuantEcon Datascience: Regression - Neural Networks\nQuantEcon: Intro to Artificial Neural Nets\npytorch notebooks from 323"
  },
  {
    "objectID": "neuralnets.html#references",
    "href": "neuralnets.html#references",
    "title": "Neural Networks",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "did.html#introduction-1",
    "href": "did.html#introduction-1",
    "title": "Introduction to Difference in Differences",
    "section": "Introduction",
    "text": "Introduction\n\nHave some policy applied to some observations but not others, and observe outcome before and after policy\nIdea: compare outcome before and after policy in treated and untreated group\nChange in outcome in treated group reflects both effect of policy and time trend, change in untreated group captures time trend"
  },
  {
    "objectID": "did.html#example-impact-of-billboards",
    "href": "did.html#example-impact-of-billboards",
    "title": "Introduction to Difference in Differences",
    "section": "Example: Impact of Billboards",
    "text": "Example: Impact of Billboards\n\nFrom Facure (2022) chapter 13\nBank placed billboards advertising savings accounts in Porto Alegre in July\nData on deposits in May and July in Porto Alegre and Florianopolis"
  },
  {
    "objectID": "did.html#example-impact-of-billboards-1",
    "href": "did.html#example-impact-of-billboards-1",
    "title": "Introduction to Difference in Differences",
    "section": "Example: Impact of Billboards",
    "text": "Example: Impact of Billboards\n\n\nCode\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport statsmodels.formula.api as smf\n\ndatadir=\"./data\"\n\n\n\ndata = pd.read_csv(datadir + \"/billboard_impact.csv\")\ndata.head()\n\n\n\n\n\n\n\n\ndeposits\npoa\njul\n\n\n\n\n0\n42\n1\n0\n\n\n1\n0\n1\n0\n\n\n2\n52\n1\n0\n\n\n3\n119\n1\n0\n\n\n4\n21\n1\n0"
  },
  {
    "objectID": "did.html#means-and-differences",
    "href": "did.html#means-and-differences",
    "title": "Introduction to Difference in Differences",
    "section": "Means and Differences",
    "text": "Means and Differences\n\ntbl = data.groupby(['jul','poa']).mean().unstack()\ntbl\n\n\n\n\n\n\n\n\ndeposits\n\n\npoa\n0\n1\n\n\njul\n\n\n\n\n\n\n0\n171.642308\n46.01600\n\n\n1\n206.165500\n87.06375\n\n\n\n\n\n\n\n\ntbl.diff(axis=0).iloc[1,:]\n\n          poa\ndeposits  0      34.523192\n          1      41.047750\nName: 1, dtype: float64\n\n\n\ntbl.diff(axis=1).iloc[:,1]\n\njul\n0   -125.626308\n1   -119.101750\nName: (deposits, 1), dtype: float64\n\n\n\ntbl.diff(axis=0).diff(axis=1).iloc[1,1]\n\nnp.float64(6.524557692307688)"
  },
  {
    "objectID": "did.html#setup",
    "href": "did.html#setup",
    "title": "Introduction to Difference in Differences",
    "section": "Setup",
    "text": "Setup\n\nTwo periods, binary treatment in second period\nPotential outcomes \\(\\{y_{it}(0),y_{it}(1)\\}_{t=0}^1\\) for \\(i=1,...,N\\)\nTreatment \\(D_{it} \\in \\{0,1\\}\\),\n\n\\(D_{i0} = 0\\) \\(\\forall i\\)\n\\(D_{i1} = 1\\) for some, \\(0\\) for others\n\nObserve \\(y_{it} = y_{it}(0)(1-D_{it}) + D_{it} y_{it}(1)\\)"
  },
  {
    "objectID": "did.html#identification",
    "href": "did.html#identification",
    "title": "Introduction to Difference in Differences",
    "section": "Identification",
    "text": "Identification\n\nAverage treatment effect on the treated:\n\n\\[ %\n\\begin{align*}\nATT & = \\Er[y_{i1}(1) - \\color{red}{y_{i1}(0)} | D_{i1} = 1] \\\\\n& = \\Er[y_{i1}(1) - y_{i0}(0) | D_{i1} = 1] - \\Er[\\color{red}{y_{i1}(0)} - y_{i0}(0) | D_{i1}=1] \\\\\n& \\text{ assume } \\Er[\\color{red}{y_{i1}(0)} - y_{i0}(0) | D_{i1}=1] =  \\Er[y_{i1}(0) - y_{i0}(0) | D_{i1}=0] \\\\\n& = \\Er[y_{i1}(1) - y_{i0}(0) | D_{i1} = 1] - \\Er[y_{i1}(0) - y_{i0}(0) | D_{i1}=0] \\\\\n& = \\Er[y_{i1} - y_{i0} | D_{i1}=1, D_{i0}=0] - \\Er[y_{i1} - y_{i0} | D_{i1}=0, D_{i0}=0]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "did.html#important-assumptions",
    "href": "did.html#important-assumptions",
    "title": "Introduction to Difference in Differences",
    "section": "Important Assumptions",
    "text": "Important Assumptions\n\nNo anticipation: \\(D_{i1}=1\\) does not affect \\(y_{i0}\\)\n\nbuilt into the potential outcomes notation we used, relax by allowing potential outcomes given sequence of \\(D\\), i.e. \\(y_{it}(D_{i0},D_{i1})\\)\n\nParallel trends: \\(\\Er[\\color{red}{y_{i1}(0)} - y_{i0}(0) |D_{i1}=1,D_{i0}=0] =  \\Er[y_{i1}(0) - y_{i0}(0) | D_{i1}=0], D_{i0}=0]\\)\n\nnot invariant to tranformations of \\(y\\)"
  },
  {
    "objectID": "did.html#estimation",
    "href": "did.html#estimation",
    "title": "Introduction to Difference in Differences",
    "section": "Estimation",
    "text": "Estimation\n\n\nPlugin: \\[ %\n\\widehat{ATT} = \\frac{ \\sum_{i=1}^n (y_{i1} - y_{i0})D_{i1}(1-D_{i0})}{\\sum_{i=1}^n D_{i1}(1-D_{i0})} -  \\frac{ \\sum_{i=1}^n (y_{i1} - y_{i0})(1-D_{i1})(1-D_{i0})}{\\sum_{i=1}^n (1-D_{i1})(1-D_{i0})}\n\\]\nRegression: \\[ %\ny_{it} = \\delta_t + \\alpha 1\\{D_{i1}=1\\} + \\beta D_{it} + \\epsilon_{it}\n\\] then \\(\\hat{\\beta} = \\widehat{ATT}\\)"
  },
  {
    "objectID": "did.html#visualizing-difference-in-differences",
    "href": "did.html#visualizing-difference-in-differences",
    "title": "Introduction to Difference in Differences",
    "section": "Visualizing Difference in Differences",
    "text": "Visualizing Difference in Differences\n\npoa_before = data.query(\"poa==1 & jul==0\")[\"deposits\"].mean()\npoa_after = data.query(\"poa==1 & jul==1\")[\"deposits\"].mean()\nfl_before = data.query(\"poa==0 & jul==0\")[\"deposits\"].mean()\nfl_after = data.query(\"poa==0 & jul==1\")[\"deposits\"].mean()\nplt.figure(figsize=(10,5))\nplt.plot([\"May\", \"Jul\"], [fl_before, fl_after], label=\"FL\", lw=2)\nplt.plot([\"May\", \"Jul\"], [poa_before, poa_after], label=\"POA\", lw=2)\n\nplt.plot([\"May\", \"Jul\"], [poa_before, poa_before+(fl_after-fl_before)],\n         label=\"Counterfactual\", lw=2, color=\"C2\", ls=\"-.\")\n\nplt.legend();"
  },
  {
    "objectID": "did.html#estimation-via-regression",
    "href": "did.html#estimation-via-regression",
    "title": "Introduction to Difference in Differences",
    "section": "Estimation via Regression",
    "text": "Estimation via Regression\n\nfrom statsmodels.iolib.summary2 import summary_col\nsummary_col(smf.ols('deposits ~ poa*jul', data=data).fit(cov_type=\"HC3\"))\n\n\n\n\n\ndeposits\n\n\nIntercept\n171.6423\n\n\n\n(2.4989)\n\n\npoa\n-125.6263\n\n\n\n(3.0774)\n\n\njul\n34.5232\n\n\n\n(3.3431)\n\n\npoa:jul\n6.5246\n\n\n\n(4.2478)\n\n\nR-squared\n0.3126\n\n\nR-squared Adj.\n0.3122\n\n\n\n\nStandard errors in parentheses."
  },
  {
    "objectID": "did.html#further-topics",
    "href": "did.html#further-topics",
    "title": "Introduction to Difference in Differences",
    "section": "Further Topics",
    "text": "Further Topics\n\nMore periods, more groups\nCovariates\nPre-trends"
  },
  {
    "objectID": "did.html#reading",
    "href": "did.html#reading",
    "title": "Introduction to Difference in Differences",
    "section": "Reading",
    "text": "Reading\n\nChapter 13 of Facure (2022)"
  },
  {
    "objectID": "did.html#references",
    "href": "did.html#references",
    "title": "Introduction to Difference in Differences",
    "section": "References",
    "text": "References\n\n\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html."
  },
  {
    "objectID": "fe.html#panel-data",
    "href": "fe.html#panel-data",
    "title": "Fixed Effects",
    "section": "Panel Data",
    "text": "Panel Data\n\nUnits \\(i=1,..., n\\)\n\nEx: people, firms, cities, countries\n\nTime \\(t=1,..., T\\)\nObserve \\(\\left\\{(y_{it}, X_{it})\\right\\}_{i=1,t=1}^{n,T}\\)"
  },
  {
    "objectID": "fe.html#linear-model",
    "href": "fe.html#linear-model",
    "title": "Fixed Effects",
    "section": "Linear Model",
    "text": "Linear Model\n\n\nModel \\[\ny_{it} = X_{it}'\\beta + \\overbrace{U_i'\\gamma + \\epsilon_{it}}^{\\text{unobserved}}\n\\]\n\nTime invariant confounders \\(U_i\\)\n\nSubtract individual averages \\[\n\\begin{align*}\ny_{it} - \\bar{y}_i & = (X_{it} - \\bar{X}_i)'\\beta + (\\epsilon_{it} -\n                   \\bar{\\epsilon}_i) \\\\\n\\ddot{y}_{it} & = \\ddot{X}_{it}' \\beta + \\ddot{\\epsilon}_{it}\n\\end{align*}\n\\]\nEquivalent to estimating with individual dummies \\[\ny_{it} = X_{it}'\\beta + \\alpha_i + \\epsilon_{it}\n\\]\n\n\n\n\nEliminates \\(U_i\\) and any time invariant observed \\(X_i\\)"
  },
  {
    "objectID": "fe.html#ols",
    "href": "fe.html#ols",
    "title": "Fixed Effects",
    "section": "OLS",
    "text": "OLS\n\n\nimports\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nimport statsmodels.formula.api as smf\nstyle.use(\"fivethirtyeight\")\n\n\n1\n\n\nCode\ntoy_panel = pd.DataFrame({\n    \"mkt_costs\":[5,4,3.5,3, 10,9.5,9,8, 4,3,2,1, 8,7,6,4],\n    \"purchase\":[12,9,7.5,7, 9,7,6.5,5, 15,14.5,14,13, 11,9.5,8,5],\n    \"city\":[\"C0\",\"C0\",\"C0\",\"C0\", \"C2\",\"C2\",\"C2\",\"C2\", \"C1\",\"C1\",\"C1\",\"C1\", \"C3\",\"C3\",\"C3\",\"C3\"]\n})\n\nm = smf.ols(\"purchase ~ mkt_costs\", data=toy_panel).fit()\n\nplt.scatter(toy_panel.mkt_costs, toy_panel.purchase)\nplt.plot(toy_panel.mkt_costs, m.fittedvalues, c=\"C5\", label=\"Regression Line\")\nplt.xlabel(\"Marketing Costs (in 1000)\")\nplt.ylabel(\"In-app Purchase (in 1000)\")\nplt.title(\"Simple OLS Model\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nCode from Facure (2022)"
  },
  {
    "objectID": "fe.html#fixed-effects-within",
    "href": "fe.html#fixed-effects-within",
    "title": "Fixed Effects",
    "section": "Fixed Effects / Within",
    "text": "Fixed Effects / Within\n1\n\n\nCode\nfe = smf.ols(\"purchase ~ mkt_costs + C(city)\", data=toy_panel).fit()\n\nfe_toy = toy_panel.assign(y_hat = fe.fittedvalues)\n\nplt.scatter(toy_panel.mkt_costs, toy_panel.purchase, c=toy_panel.city)\nfor city in fe_toy[\"city\"].unique():\n    plot_df = fe_toy.query(f\"city=='{city}'\")\n    plt.plot(plot_df.mkt_costs, plot_df.y_hat, c=\"C5\")\n\nplt.title(\"Fixed Effect Model\")\nplt.xlabel(\"Marketing Costs (in 1000)\")\nplt.ylabel(\"In-app Purchase (in 1000)\");\n\n\n\n\n\n\n\n\n\nCode from Facure (2022)"
  },
  {
    "objectID": "fe.html#large-n-small-t",
    "href": "fe.html#large-n-small-t",
    "title": "Fixed Effects",
    "section": "Large \\(n\\), Small \\(T\\)",
    "text": "Large \\(n\\), Small \\(T\\)\n\nOften \\(n&gt;&gt;T\\)\nUsual analysis of fixed effects uses asymptotics with \\(n \\to \\infty\\), \\(T\\) fixed\n\nWe will mostly stick to that, but if you have data with \\(n \\approx T\\), other approaches can be better"
  },
  {
    "objectID": "fe.html#strict-exogeneity",
    "href": "fe.html#strict-exogeneity",
    "title": "Fixed Effects",
    "section": "Strict Exogeneity",
    "text": "Strict Exogeneity\n\nIn fixed effect model \\[\ny_{it} - \\bar{y}_i  = (X_{it} - \\bar{X}_i)'\\beta + (\\epsilon_{it} - \\bar{\\epsilon}_i)\n\\] for \\(\\hat{\\beta}^{FE} \\inprob \\beta\\), need \\(\\Er[(X_{it} - \\bar{X}_i)(\\epsilon_{it} - \\bar{\\epsilon}_i)]=0\\)\nI.e. \\(\\Er[X_{it} \\epsilon_{is}] = 0\\) for all \\(t, s\\)"
  },
  {
    "objectID": "fe.html#strict-exogeneity-1",
    "href": "fe.html#strict-exogeneity-1",
    "title": "Fixed Effects",
    "section": "Strict Exogeneity",
    "text": "Strict Exogeneity\n\nProblematic with dynamics, e.g.\n\n\\(X_{it}\\) including lagged \\(y_{it-1}\\)\n\\(X_{it}\\) affected by past \\(y\\)\n“Nickell bias”\n\nSee Chen, Chernozhukov, and Fernández-Val (2019) for bias correction under weak exogeneity, \\(\\Er[X_{it} \\epsilon_{is}] = 0\\) for \\(t \\leq s\\)"
  },
  {
    "objectID": "fe.html#standard-errors",
    "href": "fe.html#standard-errors",
    "title": "Fixed Effects",
    "section": "Standard Errors",
    "text": "Standard Errors\n\nGenerally, good idea to use clustered standard errors, clustered on \\(i\\)\nSee MacKinnon, Nielsen, and Webb (2023) for guide to clustered standard errors\n\n\nimport pyfixest as pf\npf.feols('purchase ~ mkt_costs | city', data=toy_panel, vcov={'CRV1': 'city'}).summary()\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n###\n\nEstimation:  OLS\nDep. var.: purchase, Fixed effects: city\nInference:  CRV1\nObservations:  16\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| mkt_costs     |      1.441 |        0.307 |     4.700 |      0.018 |  0.465 |   2.417 |\n---\nRMSE: 0.689 R2: 0.954 R2 Within: 0.832"
  },
  {
    "objectID": "fe.html#sources-and-further-reading",
    "href": "fe.html#sources-and-further-reading",
    "title": "Fixed Effects",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nFacure (2022) chapter 14\nHuntington-Klein (2021) chapter 16"
  },
  {
    "objectID": "fe.html#references",
    "href": "fe.html#references",
    "title": "Fixed Effects",
    "section": "References",
    "text": "References\n\n\n\n\nChen, Shuowen, Victor Chernozhukov, and Iván Fernández-Val. 2019. “Mastering Panel Metrics: Causal Impact of Democracy on Growth.” AEA Papers and Proceedings 109 (May): 77–82. https://doi.org/10.1257/pandp.20191071.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. https://theeffectbook.net/.\n\n\nMacKinnon, James G., Morten Ørregaard Nielsen, and Matthew D. Webb. 2023. “Cluster-Robust Inference: A Guide to Empirical Practice.” Journal of Econometrics 232 (2): 272–99. https://doi.org/https://doi.org/10.1016/j.jeconom.2022.04.001."
  },
  {
    "objectID": "syntheticcontrol.html#setup",
    "href": "syntheticcontrol.html#setup",
    "title": "Synthetic Control",
    "section": "Setup",
    "text": "Setup\n\n1 treated unit, observed \\(T_0\\) periods before treatment, \\(T_1\\) periods after\n\\(J\\) untreated units\n\\(J\\), \\(T_0\\) moderate in size\nFormalisation of comparative case study"
  },
  {
    "objectID": "syntheticcontrol.html#example-california-tobacco-control-program",
    "href": "syntheticcontrol.html#example-california-tobacco-control-program",
    "title": "Synthetic Control",
    "section": "Example: California Tobacco Control Program",
    "text": "Example: California Tobacco Control Program\n\nCode and copy of data from Facure (2022)\nData used in Abadie, Diamond, and Hainmueller (2010)\n\n\n\nCode\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n\n#if 'ipykernel' in sys.modules:\n#    %matplotlib inline\n\npd.set_option(\"display.max_columns\", 20)\nstyle.use(\"fivethirtyeight\")"
  },
  {
    "objectID": "syntheticcontrol.html#data-california-tobacco-control-program",
    "href": "syntheticcontrol.html#data-california-tobacco-control-program",
    "title": "Synthetic Control",
    "section": "Data: California Tobacco Control Program",
    "text": "Data: California Tobacco Control Program\n\ncigar = pd.read_csv(\"data/smoking.csv\")\ncigar.query(\"california\").head()\n\n\n\n\n\n\n\n\nstate\nyear\ncigsale\nlnincome\nbeer\nage15to24\nretprice\ncalifornia\nafter_treatment\n\n\n\n\n62\n3\n1970\n123.000000\nNaN\nNaN\n0.178158\n38.799999\nTrue\nFalse\n\n\n63\n3\n1971\n121.000000\nNaN\nNaN\n0.179296\n39.700001\nTrue\nFalse\n\n\n64\n3\n1972\n123.500000\n9.930814\nNaN\n0.180434\n39.900002\nTrue\nFalse\n\n\n65\n3\n1973\n124.400002\n9.955092\nNaN\n0.181572\n39.900002\nTrue\nFalse\n\n\n66\n3\n1974\n126.699997\n9.947999\nNaN\n0.182710\n41.900002\nTrue\nFalse"
  },
  {
    "objectID": "syntheticcontrol.html#cigarette-sales-trends",
    "href": "syntheticcontrol.html#cigarette-sales-trends",
    "title": "Synthetic Control",
    "section": "Cigarette Sales Trends",
    "text": "Cigarette Sales Trends\n\nax = plt.subplot(1, 1, 1)\nfor gdf in cigar.groupby(\"state\"):\n    ax.plot(gdf[1]['year'],gdf[1]['cigsale'], alpha=0.2, lw=1, color=\"k\")\n\nax.set_ylim(40, 150)\nax.plot(cigar.query(\"california\")['year'], cigar.query(\"california\")['cigsale'], label=\"California\")\ncigar.query(\"not california\").groupby(\"year\")['cigsale'].mean().plot(ax=ax, label=\"Mean Other States\")\n\nplt.vlines(x=1988, ymin=40, ymax=140, linestyle=\":\", lw=2, label=\"Proposition 99\")\nplt.ylabel(\"Per-capita cigarette sales (in packs)\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "syntheticcontrol.html#which-comparison",
    "href": "syntheticcontrol.html#which-comparison",
    "title": "Synthetic Control",
    "section": "Which Comparison?",
    "text": "Which Comparison?\n\n\nAll states?\nStates bordering California?\nStates with similar characteristics? Which characteristics?"
  },
  {
    "objectID": "syntheticcontrol.html#synthetic-control",
    "href": "syntheticcontrol.html#synthetic-control",
    "title": "Synthetic Control",
    "section": "Synthetic Control",
    "text": "Synthetic Control\n\nCompare treated unit (California) to weighted average of untreated units\nWeights chosen to make sythetic control close to California"
  },
  {
    "objectID": "syntheticcontrol.html#synthetic-control-1",
    "href": "syntheticcontrol.html#synthetic-control-1",
    "title": "Synthetic Control",
    "section": "Synthetic Control",
    "text": "Synthetic Control\n\nPotential outcomes \\(Y_{it}(0), Y_{it}(1)\\)\nFor \\(t&gt;T_0\\), estimate treatment effect on treated unit as \\[\n\\hat{\\tau}_{1t} = Y_{1t} -\n\\underbrace{\\sum_{j=2}^{J+1} \\hat{w}_j Y_{jt}}_{\\hat{Y}_{1t}(0)}\n\\]\n\\(\\hat{w}_j\\) chosen to make synthetic control close to treated unit in before treatment"
  },
  {
    "objectID": "syntheticcontrol.html#variables-to-match",
    "href": "syntheticcontrol.html#variables-to-match",
    "title": "Synthetic Control",
    "section": "Variables to Match",
    "text": "Variables to Match\n\nVector of pretreatment variables for treated unit \\[\n\\mathbf{X}_1 = \\left(Y_{11}, \\cdots Y_{1T_0}, z_{11}, \\cdots, z_1K \\right)^T\n\\]\nMatrix of same variabels for untreated \\[\n\\mathbf{X}_0 = \\begin{pmatrix}\nY_{21} & \\cdots & Y_{2T_0} & z_{21} & \\cdots & z_{2K} \\\\\n\\vdots &        &          &        &        & \\vdots \\\\\nY_{J+1,1} & \\cdots & Y_{J+1,T_0} & z_{J+1,1} & \\cdots & z_{J+1,K}\n\\end{pmatrix}^T\n\\]"
  },
  {
    "objectID": "syntheticcontrol.html#weights",
    "href": "syntheticcontrol.html#weights",
    "title": "Synthetic Control",
    "section": "Weights",
    "text": "Weights\n\nWeights minimize difference \\[\n\\begin{align*}\n\\hat{W} = & \\textrm{arg}\\min_{W \\in \\R^J} \\Vert \\mathbf{X}_1 - \\mathbf{X}_0 W \\Vert_V \\\\\n& s.t. \\sum_{j=2}^{J+1} w_j = 1 \\\\\n& \\;\\;\\; 0 \\leq w_j \\leq 1 \\;\\; \\forall j\n\\end{align*}\n\\]\n\n\n\n\\(\\Vert x \\Vert_V = x' V x\\) is a weighted norm. Choose \\(V\\) to e.g. weight by inverse variance"
  },
  {
    "objectID": "syntheticcontrol.html#computing-weights",
    "href": "syntheticcontrol.html#computing-weights",
    "title": "Synthetic Control",
    "section": "Computing Weights",
    "text": "Computing Weights\n\nfeatures = [\"cigsale\", \"retprice\"]\ninverted = (cigar.query(\"~after_treatment\") # filter pre-intervention period\n            .pivot(index='state', columns=\"year\")[features] # make one column per year and one row per state\n            .T) # flip the table to have one column per state\ninverted.head()\n\n\n\n\n\n\n\n\nstate\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncigsale\n1970\n89.800003\n100.300003\n123.000000\n124.800003\n120.000000\n155.000000\n109.900002\n102.400002\n124.800003\n134.600006\n...\n103.599998\n92.699997\n99.800003\n106.400002\n65.500000\n122.599998\n124.300003\n114.500000\n106.400002\n132.199997\n\n\n1971\n95.400002\n104.099998\n121.000000\n125.500000\n117.599998\n161.100006\n115.699997\n108.500000\n125.599998\n139.300003\n...\n115.000000\n96.699997\n106.300003\n108.900002\n67.699997\n124.400002\n128.399994\n111.500000\n105.400002\n131.699997\n\n\n1972\n101.099998\n103.900002\n123.500000\n134.300003\n110.800003\n156.300003\n117.000000\n126.099998\n126.599998\n149.199997\n...\n118.699997\n103.000000\n111.500000\n108.599998\n71.300003\n138.000000\n137.000000\n117.500000\n108.800003\n140.000000\n\n\n1973\n102.900002\n108.000000\n124.400002\n137.899994\n109.300003\n154.699997\n119.800003\n121.800003\n124.400002\n156.000000\n...\n125.500000\n103.500000\n109.699997\n110.400002\n72.699997\n146.800003\n143.100006\n116.599998\n109.500000\n141.199997\n\n\n1974\n108.199997\n109.699997\n126.699997\n132.800003\n112.400002\n151.300003\n123.699997\n125.599998\n131.899994\n159.600006\n...\n129.699997\n108.400002\n114.800003\n114.699997\n75.599998\n151.800003\n149.600006\n119.900002\n111.800003\n145.800003\n\n\n\n\n5 rows × 39 columns"
  },
  {
    "objectID": "syntheticcontrol.html#computing-weights-1",
    "href": "syntheticcontrol.html#computing-weights-1",
    "title": "Synthetic Control",
    "section": "Computing Weights",
    "text": "Computing Weights\n\nfrom scipy.optimize import fmin_slsqp\nfrom toolz import reduce, partial\n\nX1 = inverted[3].values # state of california\nX0 = inverted.drop(columns=3).values  # other states\n\ndef loss_w(W, X0, X1) -&gt; float:\n    return np.sqrt(np.mean((X1 - X0.dot(W))**2))\n\n\ndef get_w(X0, X1):\n    w_start = [1/X0.shape[1]]*X0.shape[1]\n    weights = fmin_slsqp(partial(loss_w, X0=X0, X1=X1),\n                         np.array(w_start),\n                         f_eqcons=lambda x: np.sum(x) - 1,\n                         bounds=[(0.0, 1.0)]*len(w_start),\n                         disp=False)\n    return weights"
  },
  {
    "objectID": "syntheticcontrol.html#examining-weights",
    "href": "syntheticcontrol.html#examining-weights",
    "title": "Synthetic Control",
    "section": "Examining Weights",
    "text": "Examining Weights\n\nWeights tend to be sparse\nGood idea to examine which untreated units get positive weight\nShould look at state names, but the data does not have them, and the state variable is not FIPs code or any standard identifier\n\n\ncalif_weights = get_w(X0, X1)\nprint(\"Sum:\", calif_weights.sum())\nnp.round(calif_weights, 4)\n\nSum: 1.000000000000424\n\n\narray([0.    , 0.    , 0.    , 0.0852, 0.    , 0.    , 0.    , 0.    ,\n       0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n       0.    , 0.    , 0.    , 0.113 , 0.1051, 0.4566, 0.    , 0.    ,\n       0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n       0.2401, 0.    , 0.    , 0.    , 0.    , 0.    ])"
  },
  {
    "objectID": "syntheticcontrol.html#effect-of-california-tobacco-control-program",
    "href": "syntheticcontrol.html#effect-of-california-tobacco-control-program",
    "title": "Synthetic Control",
    "section": "Effect of California Tobacco Control Program",
    "text": "Effect of California Tobacco Control Program\n\ncalif_synth = cigar.query(\"~california\").pivot(index='year', columns=\"state\")[\"cigsale\"].values.dot(calif_weights)\nplt.figure(figsize=(10,6))\nplt.plot(cigar.query(\"california\")[\"year\"], cigar.query(\"california\")[\"cigsale\"], label=\"California\")\nplt.plot(cigar.query(\"california\")[\"year\"], calif_synth, label=\"Synthetic Control\")\nplt.vlines(x=1988, ymin=40, ymax=140, linestyle=\":\", lw=2, label=\"Proposition 99\")\nplt.ylabel(\"Per-capita cigarette sales (in packs)\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "syntheticcontrol.html#when-does-this-work",
    "href": "syntheticcontrol.html#when-does-this-work",
    "title": "Synthetic Control",
    "section": "When Does this Work?",
    "text": "When Does this Work?\n\nIf data generated by linear factor model: \\[\nY_{jt}(0) = \\delta_t + \\theta_t Z_j + \\lambda_t \\mu_j + \\epsilon_{jt}\n\\]\n\nObserved \\(Z_j\\)\nUnobserved\\(\\lambda_t\\), \\(\\mu_j\\)\n\nAs \\(T_0\\) increases or variance of \\(\\epsilon_{jt}\\) decreases, bias of \\(\\hat{\\tau}_{it}\\) decreases\n\nNeeds \\(\\mathbf{X}_1 \\approx \\mathbf{X}_0 W\\)"
  },
  {
    "objectID": "syntheticcontrol.html#choices",
    "href": "syntheticcontrol.html#choices",
    "title": "Synthetic Control",
    "section": "Choices",
    "text": "Choices\n\n\nVariables in \\(\\mathbf{X}\\) to match\n\nFewer make eaiser to have \\(\\mathbf{X}_1 \\approx \\mathbf{X}_0 W\\)\nBut fewer make \\(W\\) depend more on \\(\\epsilon\\)\n\nSet of untreated units to consider\n\nMore units makes eaiser to have \\(\\mathbf{X}_1 \\approx \\mathbf{X}_0 W\\)\nBut risk of overfitting"
  },
  {
    "objectID": "syntheticcontrol.html#inference-1",
    "href": "syntheticcontrol.html#inference-1",
    "title": "Synthetic Control",
    "section": "Inference",
    "text": "Inference\n\nEstimator \\[\n\\hat{\\tau}_{1t} = Y_{1t} -\n\\underbrace{\\sum_{j=2}^{J+1} \\hat{w}_j Y_{jt}}_{\\hat{Y}_{1t}(0)}\n\\]\n\nsingle observation of \\(Y_{1t}\\)\n\\(\\hat{w}_j\\) depends on pre-treatment variables \\(\\underbrace{\\mathbf{X}_1, \\mathbf{X}_0}_{(J+1) \\times (T_0 + K)}\\)\n\\(J\\) values of \\(Y_{jt}\\)\n\nUsual asymptotics not applicable"
  },
  {
    "objectID": "syntheticcontrol.html#treatment-permutation",
    "href": "syntheticcontrol.html#treatment-permutation",
    "title": "Synthetic Control",
    "section": "Treatment Permutation",
    "text": "Treatment Permutation\n\nCompute estimate pretending each of the \\(J\\) untreated units were treated instead\nUse as distribution of \\(\\hat{\\tau}_{1t}\\) under \\(H_0: \\tau_{1t} = 0\\)"
  },
  {
    "objectID": "syntheticcontrol.html#treatment-permutation-1",
    "href": "syntheticcontrol.html#treatment-permutation-1",
    "title": "Synthetic Control",
    "section": "Treatment Permutation",
    "text": "Treatment Permutation\n\ndef synthetic_control(state: int, data: pd.DataFrame) -&gt; np.array:\n    features = [\"cigsale\", \"retprice\"]\n    inverted = (data.query(\"~after_treatment\")\n                .pivot(index='state', columns=\"year\")[features]\n                .T)\n    X1 = inverted[state].values # treated\n    X0 = inverted.drop(columns=state).values # donor pool\n\n    weights = get_w(X0, X1)\n    synthetic = (data.query(f\"~(state=={state})\")\n                 .pivot(index='year', columns=\"state\")[\"cigsale\"]\n                 .values.dot(weights))\n    return (data\n            .query(f\"state=={state}\")[[\"state\", \"year\", \"cigsale\", \"after_treatment\"]]\n            .assign(synthetic=synthetic))\n\nfrom joblib import Parallel, delayed\n\ncontrol_pool = cigar[\"state\"].unique()\nparallel_fn = delayed(partial(synthetic_control, data=cigar))\nsynthetic_states = Parallel(n_jobs=20)(parallel_fn(state) for state in control_pool);"
  },
  {
    "objectID": "syntheticcontrol.html#treatment-permutation-2",
    "href": "syntheticcontrol.html#treatment-permutation-2",
    "title": "Synthetic Control",
    "section": "Treatment Permutation",
    "text": "Treatment Permutation\n\n\nCode\nplt.figure(figsize=(12,7))\nfor state in synthetic_states:\n    plt.plot(state[\"year\"], state[\"cigsale\"] - state[\"synthetic\"], color=\"C5\",alpha=0.4)\n\nplt.plot(cigar.query(\"california\")[\"year\"], cigar.query(\"california\")[\"cigsale\"] - calif_synth,\n        label=\"California\");\n\nplt.vlines(x=1988, ymin=-50, ymax=120, linestyle=\":\", lw=2, label=\"Proposition 99\")\nplt.hlines(y=0, xmin=1970, xmax=2000, lw=3)\nplt.ylabel(\"Gap in per-capita cigarette sales (in packs)\")\nplt.title(\"State - Synthetic Across Time\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "syntheticcontrol.html#treatment-permutation-inference",
    "href": "syntheticcontrol.html#treatment-permutation-inference",
    "title": "Synthetic Control",
    "section": "Treatment Permutation Inference",
    "text": "Treatment Permutation Inference\n\n\nCode\ndef pre_treatment_error(state):\n    pre_treat_error = (state.query(\"~after_treatment\")[\"cigsale\"]\n                       - state.query(\"~after_treatment\")[\"synthetic\"]) ** 2\n    return pre_treat_error.mean()\n\nplt.figure(figsize=(12,7))\nfor state in synthetic_states:\n\n    # remove units with mean error above 80.\n    if pre_treatment_error(state) &lt; 80:\n        plt.plot(state[\"year\"], state[\"cigsale\"] - state[\"synthetic\"], color=\"C5\",alpha=0.4)\n\nplt.plot(cigar.query(\"california\")[\"year\"], cigar.query(\"california\")[\"cigsale\"] - calif_synth,\n        label=\"California\");\n\nplt.vlines(x=1988, ymin=-50, ymax=120, linestyle=\":\", lw=2, label=\"Proposition 99\")\nplt.hlines(y=0, xmin=1970, xmax=2000, lw=3)\nplt.ylabel(\"Gap in per-capita cigarette sales (in packs)\")\nplt.title(\"Distribution of Effects\")\nplt.title(\"State - Synthetic Across Time (Large Pre-Treatment Errors Removed)\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "syntheticcontrol.html#treatment-permutation-inference-testing-h_0-tau0",
    "href": "syntheticcontrol.html#treatment-permutation-inference-testing-h_0-tau0",
    "title": "Synthetic Control",
    "section": "Treatment Permutation Inference: Testing \\(H_0: \\tau=0\\)",
    "text": "Treatment Permutation Inference: Testing \\(H_0: \\tau=0\\)\n\ncalif_number = 3\n\neffects = [state.query(\"year==2000\").iloc[0][\"cigsale\"] - state.query(\"year==2000\").iloc[0][\"synthetic\"]\n           for state in synthetic_states\n           if pre_treatment_error(state) &lt; 80] # filter out noise\n\ncalif_effect = cigar.query(\"california & year==2000\").iloc[0][\"cigsale\"] - calif_synth[-1]\n\nprint(\"California Treatment Effect for the Year 2000:\", calif_effect)\nnp.array(effects)\n\nnp.mean(np.array(effects) &lt; calif_effect)\n\nCalifornia Treatment Effect for the Year 2000: -24.83015975607075\n\n\nnp.float64(0.02857142857142857)"
  },
  {
    "objectID": "syntheticcontrol.html#design-based-inference",
    "href": "syntheticcontrol.html#design-based-inference",
    "title": "Synthetic Control",
    "section": "Design Based Inference",
    "text": "Design Based Inference\n\n\nSampling based inference:\n\nSpecify data generating process (DGP)\nFind distribution of estimator under repeated sampling of datasets from DGP\n\n\n\n\n\nDesign based inference:\n\nCondition on dataset you have\nRandomness of estimator from random assignment of treatment\nSee Abadie et al. (2020) for more information\n\n\n\n\n\nTreatment permutation inference is design based inference assuming the treated unit was chosen uniformly at random from all units"
  },
  {
    "objectID": "syntheticcontrol.html#design-based-treatment-permutation-inference",
    "href": "syntheticcontrol.html#design-based-treatment-permutation-inference",
    "title": "Synthetic Control",
    "section": "Design Based / Treatment Permutation Inference",
    "text": "Design Based / Treatment Permutation Inference\n\nPros:\n\nEasy to implement and explain\nIntuitive appeal, similar to placebo tests\nMinimal assumptions about DGP\n\nCons:\n\nAssumption about randomized treatment assignment is generally false\nNeeds modification to test hypotheses other than \\(H_0: \\tau=0\\) & to construct confidence intervals"
  },
  {
    "objectID": "syntheticcontrol.html#prediction-intervals",
    "href": "syntheticcontrol.html#prediction-intervals",
    "title": "Synthetic Control",
    "section": "Prediction Intervals",
    "text": "Prediction Intervals\n\nEstimation error is a prediction error \\[\n\\begin{align*}\n\\hat{\\tau}_{1t} = & Y_{1t} -\n\\underbrace{\\hat{Y}_{1t}(0)}_{\\text{prediction given $Y_{jt}, \\mathbf{X}_0$}}\n\\\\\n= &  \\underbrace{Y_{1t}(1) - \\color{grey}{Y_{1t}(0)}}_{\\tau_{1t}} + \\underbrace{\\color{grey}{Y_{1t}(0)} - \\hat{Y}_{1t}(0)}_{\\text{prediction error}}\n\\end{align*}\n\\]\n\n\n\nMany statistical methods for calculating distribution of prediction error\nDifficulties:\n\nHigh-dimensional: number of weights comparable to number of observations and dimension of predictors\nModerate sample sizes"
  },
  {
    "objectID": "syntheticcontrol.html#prediction-intervals-1",
    "href": "syntheticcontrol.html#prediction-intervals-1",
    "title": "Synthetic Control",
    "section": "Prediction Intervals",
    "text": "Prediction Intervals\n\nModern approaches accomodate high-dimensionality and give non-asymptotic results\nChernozhukov, Wüthrich, and Zhu (2021) : construct prediction intervals by permuting residuals (conformal inference)\nCattaneo, Feng, and Titiunik (2021) : divide prediction error into two pieces: estimation of \\(\\hat{w}\\) and unpredictable randomness in \\(Y_{1t}(0)\\)"
  },
  {
    "objectID": "syntheticcontrol.html#catteneo-feng-titiunik",
    "href": "syntheticcontrol.html#catteneo-feng-titiunik",
    "title": "Synthetic Control",
    "section": "Catteneo, Feng, & Titiunik",
    "text": "Catteneo, Feng, & Titiunik\n\nGiven coverage level \\(\\alpha\\), gives interval \\(\\mathcal{I}_{1-\\alpha}\\) such that \\[\nP\\left[ P\\left(\\tau_{1t} \\in \\mathcal{I}_{1-\\alpha} | \\mathbf{X}_0, \\{y_{jt}\\}_{j=1}^J\\right) &gt; 1-\\alpha-\\epsilon(T_0) \\right] &gt; 1 - \\pi(T_0)\n\\] where \\(\\epsilon(T_0) \\to 0\\) and \\(\\pi(T_0) \\to 0\\) as \\(T_0 \\to \\infty\\)\nprediction error comes from\n\nestimation of \\(\\hat{w}\\), options refer to u_ in scpi_pkg\nunobservable stochastic error in \\(Y_{1t}(0)\\), options begin with e_ in scpi_pkg\n\nvalid under broad range of DGPs, but appropriate interval does depend on dependence of data over time, stationarity, assumptions about distribution of error given predictors"
  },
  {
    "objectID": "syntheticcontrol.html#software",
    "href": "syntheticcontrol.html#software",
    "title": "Synthetic Control",
    "section": "Software",
    "text": "Software\n\nscpi_pkg recommended, created by leading econometricians\npysyncon actively maintained, well-documented, but appears not popular\nSpareSC created by researchers at Microsoft, uses particular variant\nscinference R package accompanying Chernozhukov, Wüthrich, and Zhu (2021)"
  },
  {
    "objectID": "syntheticcontrol.html#data-preparation",
    "href": "syntheticcontrol.html#data-preparation",
    "title": "Synthetic Control",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nimport random\nfrom scpi_pkg.scdata import scdata\nfrom scpi_pkg.scest import scest\nfrom scpi_pkg.scplot import scplot\nscdf = scdata(df=cigar, id_var=\"state\", time_var=\"year\", outcome_var=\"cigsale\",\n              period_pre=cigar.query(\"not after_treatment\").year.unique(),\n              period_post=cigar.query(\"after_treatment\").year.unique(),\n              unit_tr=calif_number,\n              unit_co=cigar.query(\"not california\").state.unique(),\n              features=[\"cigsale\",\"retprice\"],\n              cov_adj=None, cointegrated_data=True,\n              constant=False)"
  },
  {
    "objectID": "syntheticcontrol.html#point-estimation",
    "href": "syntheticcontrol.html#point-estimation",
    "title": "Synthetic Control",
    "section": "Point Estimation",
    "text": "Point Estimation\n\n\nCode\nest_si = scest(scdf, w_constr={'name': \"simplex\"})\nprint(est_si)\nest_si2 = scest(scdf, w_constr={'p': 'L1', 'dir': '==', 'Q': 1, 'lb': 0})\nprint(est_si2)\nscplot(est_si)\n\n\n-----------------------------------------------------------------------\nCall: scest\nSynthetic Control Estimation - Setup\n\nConstraint Type:                                                simplex\nConstraint Size (Q):                                                  1\nTreated Unit:                                                         3\nSize of the donor pool:                                              38\nFeatures                                                              2\nPre-treatment period                                          1970-1988\nPre-treatment periods used in estimation per feature:\n Feature  Observations\n cigsale            19\nretprice            19\nCovariates used for adjustment per feature:\n Feature  Num of Covariates\n cigsale                  0\nretprice                  0\n\nSynthetic Control Estimation - Results\n\nActive donors: 5\n\nCoefficients:\n                    Weights\nTreated Unit Donor         \n3            1        0.000\n             10       0.000\n             11       0.000\n             12       0.000\n             13       0.000\n             14       0.000\n             15       0.000\n             16       0.000\n             17       0.000\n             18       0.000\n             19       0.000\n             2        0.000\n             20       0.000\n             21       0.113\n             22       0.105\n             23       0.457\n             24       0.000\n             25       0.000\n             26       0.000\n             27       0.000\n             28       0.000\n             29       0.000\n             30       0.000\n             31       0.000\n             32       0.000\n             33       0.000\n             34       0.240\n             35       0.000\n             36       0.000\n             37       0.000\n             38       0.000\n             39       0.000\n             4        0.000\n             5        0.085\n             6        0.000\n             7        0.000\n             8        0.000\n             9        0.000\n\n-----------------------------------------------------------------------\nCall: scest\nSynthetic Control Estimation - Setup\n\nConstraint Type:                                          user provided\nConstraint Size (Q):                                                  1\nTreated Unit:                                                         3\nSize of the donor pool:                                              38\nFeatures                                                              2\nPre-treatment period                                          1970-1988\nPre-treatment periods used in estimation per feature:\n Feature  Observations\n cigsale            19\nretprice            19\nCovariates used for adjustment per feature:\n Feature  Num of Covariates\n cigsale                  0\nretprice                  0\n\nSynthetic Control Estimation - Results\n\nActive donors: 5\n\nCoefficients:\n                    Weights\nTreated Unit Donor         \n3            1        0.000\n             10       0.000\n             11       0.000\n             12       0.000\n             13       0.000\n             14       0.000\n             15       0.000\n             16       0.000\n             17       0.000\n             18       0.000\n             19       0.000\n             2        0.000\n             20       0.000\n             21       0.113\n             22       0.105\n             23       0.457\n             24       0.000\n             25       0.000\n             26       0.000\n             27       0.000\n             28       0.000\n             29       0.000\n             30       0.000\n             31       0.000\n             32       0.000\n             33       0.000\n             34       0.240\n             35       0.000\n             36       0.000\n             37       0.000\n             38       0.000\n             39       0.000\n             4        0.000\n             5        0.085\n             6        0.000\n             7        0.000\n             8        0.000\n             9        0.000"
  },
  {
    "objectID": "syntheticcontrol.html#inference-2",
    "href": "syntheticcontrol.html#inference-2",
    "title": "Synthetic Control",
    "section": "Inference",
    "text": "Inference\n\n\nCode\nfrom scpi_pkg.scpi import scpi\nimport random\nw_constr = {'name': 'simplex', 'Q': 1}\nu_missp = True\nu_sigma = \"HC1\"\nu_order = 1\nu_lags = 0\ne_method = \"gaussian\"\ne_order = 1\ne_lags = 0\ne_alpha = 0.05\nu_alpha = 0.05\nsims = 200\ncores = 1\n\nrandom.seed(8894)\nresult = scpi(scdf, sims=sims, w_constr=w_constr, u_order=u_order, u_lags=u_lags,\n              e_order=e_order, e_lags=e_lags, e_method=e_method, u_missp=u_missp,\n              u_sigma=u_sigma, cores=cores, e_alpha=e_alpha, u_alpha=u_alpha)\nscplot(result, e_out=True, x_lab=\"year\", y_lab=\"per-capita cigarette sales (in packs)\")\n\n\n-----------------------------------------------\nEstimating Weights...\nQuantifying Uncertainty\nMaximum expected execution time: 2 minutes.\n \n\n20/200 iterations completed (10%)\n40/200 iterations completed (20%)\n60/200 iterations completed (30%)\n80/200 iterations completed (40%)\n100/200 iterations completed (50%)\n120/200 iterations completed (60%)\n140/200 iterations completed (70%)\n160/200 iterations completed (80%)\n180/200 iterations completed (90%)\n200/200 iterations completed (100%)"
  },
  {
    "objectID": "syntheticcontrol.html#sources-and-further-reading",
    "href": "syntheticcontrol.html#sources-and-further-reading",
    "title": "Synthetic Control",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nexample code from Facure (2022) chapter 15\nnotation and theory follows Abadie (2021)\nHuntington-Klein (2021) chapter 21.2\nMore technical:\n\nCattaneo et al. (2022) user guide for scpi_pkg\nAbadie, Diamond, and Hainmueller (2010) important paper popularizing synthetic control and permutation inference\nAbadie and Cattaneo (2021) lists recent statistical advances in synthetic control"
  },
  {
    "objectID": "syntheticcontrol.html#references",
    "href": "syntheticcontrol.html#references",
    "title": "Synthetic Control",
    "section": "References",
    "text": "References\n\n\n\n\nAbadie, Alberto. 2021. “Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects.” Journal of Economic Literature 59 (2): 391–425. https://doi.org/10.1257/jel.20191450.\n\n\nAbadie, Alberto, Susan Athey, Guido W. Imbens, and Jeffrey M. Wooldridge. 2020. “Sampling-Based Versus Design-Based Uncertainty in Regression Analysis.” Econometrica 88 (1): 265–96. https://doi.org/https://doi.org/10.3982/ECTA12675.\n\n\nAbadie, Alberto, and Matias D. Cattaneo. 2021. “Introduction to the Special Section on Synthetic Control Methods.” Journal of the American Statistical Association 116 (536): 1713–15. https://doi.org/10.1080/01621459.2021.2002600.\n\n\nAbadie, Alberto, Alexis Diamond, and Jens Hainmueller. 2010. “Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program.” Journal of the American Statistical Association 105 (490): 493–505. https://doi.org/10.1198/jasa.2009.ap08746.\n\n\nCattaneo, Matias D., Yingjie Feng, Filippo Palomba, and Rocio Titiunik. 2022. “Scpi: Uncertainty Quantification for Synthetic Control Methods.”\n\n\nCattaneo, Matias D., Yingjie Feng, and Rocio Titiunik. 2021. “Prediction Intervals for Synthetic Control Methods.” Journal of the American Statistical Association 116 (536): 1865–80. https://doi.org/10.1080/01621459.2021.1979561.\n\n\nChernozhukov, Victor, Kaspar Wüthrich, and Yinchu Zhu. 2021. “An Exact and Robust Conformal Inference Method for Counterfactual and Synthetic Controls.” Journal of the American Statistical Association 116 (536): 1849–64. https://doi.org/10.1080/01621459.2021.1920957.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. https://theeffectbook.net/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ECON 526: Quantitative Economics with Data Science Applications",
    "section": "",
    "text": "Slides\n\nIntroduction to Causality, notebook\n\nReading: Chapter 2 of Chernozhukov et al. (2024) or Chapter 1 of Facure (2022)\n\nUncertainty Quantification, notebook\n\nChapter 3 of Facure (2022)\n\nLinear regression, notebook\n\nChapters 5-7 of Facure (2022) or Chapters 1-2 of Chernozhukov et al. (2024)\n\nMatching slides, notebook\n\nChapters 10-12 of Facure (2022)\nChapter 14 of Huntington-Klein (2021)\nChapters 5 and 10 of Chernozhukov et al. (2024)\n\nIntroduction to difference in differences, notebook\n\nReading: chapter 13 of Facure (2022)\n\nFixed Effects, notebook\n\nReading: chapter 14 of Facure (2022)\n\nAdvanced difference in differences, notebook\n\nReading: chapter 24 of Facure (2022), Roth et al. (2023), Chaisemartin and D’Haultfœuille (2022)\n\nInstrumental Variables, notebook\n\nReading: Chapter 8 and 9 of Facure (2022)\n\nSynthetic Control, notebook\n\nReading: Abadie (2021),chapter 15 of Facure (2022)\n\nDebiased Machine Learning, notebook\n\n\nReading: QuantEcon: ML in Economics\n\n\nTreatment Heterogeneity and Conditional Effects, notebook\n\n\nReading: QuantEcon: Heterogeneity\n\n\nNeural Networks, notebook\n\n\nReading: QuantEcon Datascience: Regression - Neural Networks\n\n\nRegression discontinuity\n\n\n\n\n\n\nReferences\n\nAbadie, Alberto. 2021. “Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects.” Journal of Economic Literature 59 (2): 391–425. https://doi.org/10.1257/jel.20191450.\n\n\nChaisemartin, Clément de, and Xavier D’Haultfœuille. 2022. “Two-way fixed effects and differences-in-differences with heterogeneous treatment effects: a survey.” The Econometrics Journal 26 (3): C1–30. https://doi.org/10.1093/ectj/utac017.\n\n\nChernozhukov, V., C. Hansen, N. Kallus, M. Spindler, and V. Syrgkanis. 2024. Applied Causal Inference Powered by ML and AI. https://causalml-book.org/.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. https://theeffectbook.net/.\n\n\nRoth, Jonathan, Pedro H. C. Sant’Anna, Alyssa Bilinski, and John Poe. 2023. “What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature.” Journal of Econometrics 235 (2): 2218–44. https://doi.org/https://doi.org/10.1016/j.jeconom.2023.03.008."
  },
  {
    "objectID": "conditionaleffect.html#conditional-average-effecst",
    "href": "conditionaleffect.html#conditional-average-effecst",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Conditional Average Effecst",
    "text": "Conditional Average Effecst\n\n\nPreviously, mostly focused on average effects, e.g. \\[\nATE = \\Er[Y_i(1) - Y_i(0)]\n\\]\nAlso care about conditional average effects, e.g. \\[\nCATE(x) = \\Er[Y_i(1) - Y_i(0)|X_i = x]\n\\]\n\nMore detailed description\nSuggest mechanism for how treatment affects outcome\nGive treatment assignment rule, e.g. \\[\nD_i = 1\\{CATE(X_i) &gt; 0 \\}\n\\]"
  },
  {
    "objectID": "conditionaleffect.html#conditional-average-effects-challenges",
    "href": "conditionaleffect.html#conditional-average-effects-challenges",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Conditional Average Effects: Challenges",
    "text": "Conditional Average Effects: Challenges\n\\[\nCATE(x) = \\Er[Y_i(1) - Y_i(0)|X_i = x]\n\\]\n\nHard to communicate, espeically when \\(x\\) high dimensional\nWorse statistical properties, especially when \\(x\\) high dimensional and/or continuous\nMore demanding of data\nFocus on useful summaries of \\(CATE(x)\\)"
  },
  {
    "objectID": "conditionaleffect.html#program-keluarga-harapan",
    "href": "conditionaleffect.html#program-keluarga-harapan",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Program Keluarga Harapan",
    "text": "Program Keluarga Harapan\n\nAlatas et al. (2011) , Triyana (2016)\nRandomized experiment in Indonesia\nConditional cash transfer for pregnant women\n\n60-220USD (15-20% quarterly consumption)\nConditions: 4 pre, 2 post natal medical visits, baby delivered by doctor or midwife\n\nRandomly assigned at kecamatan (district) level\n\n\n\nimports\nimport pandas as pd\nimport numpy as np\nimport patsy\nfrom sklearn import linear_model, ensemble, base, neural_network\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n#from sklearn.utils._testing import ignore_warnings\n#from sklearn.exceptions import ConvergenceWarning\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "conditionaleffect.html#data",
    "href": "conditionaleffect.html#data",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Data",
    "text": "Data\n\nurl = \"https://datascience.quantecon.org/assets/data/Triyana_2016_price_women_clean.csv\"\ndf = pd.read_csv(url)\ndf.describe()\n\n\n\n\n\n\n\n\nrid_panel\nprov\nLocation_ID\ndist\nwave\nedu\nagecat\nlog_xp_percap\nrhr031\nrhr032\n...\nhh_xp_all\ntv\nparabola\nfridge\nmotorbike\ncar\npig\ngoat\ncow\nhorse\n\n\n\n\ncount\n1.225100e+04\n22768.000000\n2.277100e+04\n22771.000000\n22771.000000\n22771.000000\n22771.000000\n22771.000000\n22771.000000\n22771.000000\n...\n22771.000000\n22771.000000\n22771.000000\n22771.000000\n22771.000000\n22771.000000\n22771.000000\n22771.00000\n22771.000000\n22771.000000\n\n\nmean\n3.406884e+12\n42.761156\n4.286882e+06\n431842.012033\n1.847174\n52.765799\n4.043081\n13.420404\n0.675157\n0.754908\n...\n3.839181\n0.754908\n0.482148\n0.498661\n0.594792\n0.470511\n0.536691\n0.53858\n0.515041\n0.470247\n\n\nstd\n1.944106e+12\n14.241982\n1.423541e+06\n143917.353784\n0.875323\n45.833778\n1.280589\n1.534089\n0.468326\n0.430151\n...\n1.481982\n0.430151\n0.499692\n0.500009\n0.490943\n0.499141\n0.498663\n0.49852\n0.499785\n0.499125\n\n\nmin\n1.100103e+10\n31.000000\n3.175010e+06\n3524.000000\n1.000000\n6.000000\n0.000000\n7.461401\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.000000\n0.000000\n\n\n25%\n1.731008e+12\n32.000000\n3.210180e+06\n323210.000000\n1.000000\n6.000000\n3.000000\n11.972721\n0.000000\n1.000000\n...\n3.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.000000\n0.000000\n\n\n50%\n3.491004e+12\n35.000000\n3.517171e+06\n353517.000000\n2.000000\n12.000000\n5.000000\n12.851639\n1.000000\n1.000000\n...\n5.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n1.000000\n1.00000\n1.000000\n0.000000\n\n\n75%\n5.061008e+12\n53.000000\n5.307020e+06\n535307.000000\n3.000000\n99.000000\n5.000000\n15.018967\n1.000000\n1.000000\n...\n5.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.00000\n1.000000\n1.000000\n\n\nmax\n6.681013e+12\n75.000000\n7.571030e+06\n757571.000000\n3.000000\n99.000000\n5.000000\n15.018967\n1.000000\n1.000000\n...\n5.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.00000\n1.000000\n1.000000\n\n\n\n\n8 rows × 121 columns"
  },
  {
    "objectID": "conditionaleffect.html#average-treatment-effects",
    "href": "conditionaleffect.html#average-treatment-effects",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Average Treatment Effects",
    "text": "Average Treatment Effects\n\n\ndata prep\n# some data prep for later\nformula = \"\"\"\nbw ~ pkh_kec_ever +\n  C(edu)*C(agecat) + log_xp_percap + hh_land + hh_home + C(dist) +\n  hh_phone + hh_rf_tile + hh_rf_shingle + hh_rf_fiber +\n  hh_wall_plaster + hh_wall_brick + hh_wall_wood + hh_wall_fiber +\n  hh_fl_tile + hh_fl_plaster + hh_fl_wood + hh_fl_dirt +\n  hh_water_pam + hh_water_mechwell + hh_water_well + hh_water_spring + hh_water_river +\n  hh_waterhome +\n  hh_toilet_own + hh_toilet_pub + hh_toilet_none +\n  hh_waste_tank + hh_waste_hole + hh_waste_river + hh_waste_field +\n  hh_kitchen +\n  hh_cook_wood + hh_cook_kerosene + hh_cook_gas +\n  tv + fridge + motorbike + car + goat + cow + horse\n\"\"\"\nbw, X = patsy.dmatrices(formula, df, return_type=\"dataframe\")\n# some categories are empty after dropping rows with Null, drop now\nX = X.loc[:, X.sum() &gt; 0]\nbw = bw.iloc[:, 0]\ntreatment_variable = \"pkh_kec_ever\"\ntreatment = X[\"pkh_kec_ever\"]\nXl = X.drop([\"Intercept\", \"pkh_kec_ever\", \"C(dist)[T.313175]\"], axis=1)\nloc_id = df.loc[X.index, \"Location_ID\"].astype(\"category\")\n\nimport re\n# remove [ ] from names for compatibility with lightgbm\nXl = Xl.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\n\n\nfrom statsmodels.iolib.summary2 import summary_col\ntmp = pd.DataFrame(dict(birthweight=bw,treatment=treatment,assisted_delivery=df.loc[X.index, \"good_assisted_delivery\"]))\nusage = smf.ols(\"assisted_delivery ~ treatment\", data=tmp).fit(cov_type=\"cluster\", cov_kwds={'groups':loc_id})\nhealth= smf.ols(\"bw ~ treatment\", data=tmp).fit(cov_type=\"cluster\", cov_kwds={'groups':loc_id})\nsummary_col([usage, health])\n\n\n\n\n\nassisted_delivery\nbw\n\n\nIntercept\n0.7827\n3173.4067\n\n\n\n(0.0124)\n(10.2323)\n\n\ntreatment\n0.0235\n-14.8992\n\n\n\n(0.0192)\n(24.6304)\n\n\nR-squared\n0.0004\n0.0001\n\n\nR-squared Adj.\n0.0002\n-0.0001\n\n\n\n\nStandard errors in parentheses."
  },
  {
    "objectID": "conditionaleffect.html#conditional-average-treatment-effects",
    "href": "conditionaleffect.html#conditional-average-treatment-effects",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Conditional Average Treatment Effects",
    "text": "Conditional Average Treatment Effects\n\nCan never recover individual treatment effect, \\(y_i(1)- y_i(0)\\)\nCan estimate conditional averages: \\[\n\\begin{align*}\nE[y_i(1) - y_i(0) |X_i=x] = & E[y_i(1)|X_i = x] - E[y_i(0)|X_i=x] \\\\\n& \\text{random assignment } \\\\\n= & E[y_i(1) | d_i = 1, X_i=x] - E[y_i(0) | d_i = 0, X_i=x] \\\\\n= & E[y_i | d_i = 1, X_i = x] - E[y_i | d_i = 0, X_i=x ]\n\\end{align*}\n\\]\nBut, inference and communication difficult"
  },
  {
    "objectID": "conditionaleffect.html#generic-machine-learning-for-heterogeneous-effects-in-randomized-experiments",
    "href": "conditionaleffect.html#generic-machine-learning-for-heterogeneous-effects-in-randomized-experiments",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Generic Machine Learning for Heterogeneous Effects in Randomized Experiments",
    "text": "Generic Machine Learning for Heterogeneous Effects in Randomized Experiments\n\nChernozhukov et al. (2023)\nDesigned based inference Imai and Li (2022)\nIdea: use any machine learning estimator for \\(E[y_i | d_i = 0, X_i=x ]\\)\nReport and do inference on lower dimensional summaries of \\(E[y_i(1) - y_i(0) |X_i=x]\\)"
  },
  {
    "objectID": "conditionaleffect.html#best-linear-projection-of-cate",
    "href": "conditionaleffect.html#best-linear-projection-of-cate",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Best Linear Projection of CATE",
    "text": "Best Linear Projection of CATE\n\nTrue \\(CATE(x)\\), noisy proxy \\(\\widehat{CATE}(x)\\)\nBest linear projection: \\[\n\\beta_0, \\beta_1 = \\argmin_{b_0, b_1} \\Er\\left[\\left(CATE(x) - b_0 - b_1(\\widehat{CATE}(x) - E[\\widehat{CATE}(x)])\\right)^2 \\right]\n\\]\n\n\\(\\beta_0 = \\Er[y_i(1) - y_i(0)]\\)\n\\(\\beta_1\\) measures how well \\(\\widehat{CATE}(x)\\) proxies \\(CATE(x)\\)\n\nUseful for comparing two proxies \\(\\widehat{CATE}(x)\\) and \\(\\widetilde{CATE}(x)\\)"
  },
  {
    "objectID": "conditionaleffect.html#grouped-average-treatment-effects",
    "href": "conditionaleffect.html#grouped-average-treatment-effects",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Grouped Average Treatment Effects",
    "text": "Grouped Average Treatment Effects\n\nGroup observations by \\(\\widehat{CATE}(x)\\), reported averages conditional on group\nGroups \\(G_{k}(x) = 1\\{\\ell_{k-1} \\leq \\widehat{CATE}(x) \\leq \\ell_k \\}\\)\nGrouped average treatment effects: \\[\n\\gamma_k = E[y(1) - y(0) | G_k(X)=1]\n\\]"
  },
  {
    "objectID": "conditionaleffect.html#estimation",
    "href": "conditionaleffect.html#estimation",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Estimation",
    "text": "Estimation\n\nRegression with sample-splitting\nBLP: \\[\ny_i = \\alpha_0 + \\alpha_1 \\widehat{B}(x_i) + \\beta_0 (d_i-P(d=1)) + \\beta_1\n(d_i-P(d=1))(\\widehat{CATE}(x_i) - \\overline{\\widehat{CATE}(x_i)}) + \\epsilon_i\n\\]\n\nwhere \\(\\widehat{B}(x_i)\\) is an estimate of \\(\\Er[y_i(0) | X_i=x]\\)\n\nGATE: \\[\ny_i = \\alpha_0 + \\alpha_1 \\widehat{B}(x_i) + \\sum_k \\gamma_k (d_i-P(d=1)) 1(G_k(x_i)) +\nu_i\n\\]\nEstimates asymptotically normal with usual standard errors"
  },
  {
    "objectID": "conditionaleffect.html#code",
    "href": "conditionaleffect.html#code",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Code",
    "text": "Code\n\n# for clustering standard errors\ndef get_treatment_se(fit, cluster_id, rows=None):\n    if cluster_id is not None:\n        if rows is None:\n            rows = [True] * len(cluster_id)\n        vcov = sm.stats.sandwich_covariance.cov_cluster(fit, cluster_id.loc[rows])\n        return np.sqrt(np.diag(vcov))\n\n    return fit.HC0_se"
  },
  {
    "objectID": "conditionaleffect.html#code-1",
    "href": "conditionaleffect.html#code-1",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Code",
    "text": "Code\n\ndef generic_ml_model(x, y, treatment, model, n_split=10, n_group=5, cluster_id=None):\n    nobs = x.shape[0]\n\n    blp = np.zeros((n_split, 2))\n    blp_se = blp.copy()\n    gate = np.zeros((n_split, n_group))\n    gate_se = gate.copy()\n\n    baseline = np.zeros((nobs, n_split))\n    cate = baseline.copy()\n    lamb = np.zeros((n_split, 2))\n\n    for i in range(n_split):\n        main = np.random.rand(nobs) &gt; 0.5\n        rows1 = ~main & (treatment == 1)\n        rows0 = ~main & (treatment == 0)\n\n        mod1 = base.clone(model).fit(x.loc[rows1, :], (y.loc[rows1]))\n        mod0 = base.clone(model).fit(x.loc[rows0, :], (y.loc[rows0]))\n\n        B = mod0.predict(x)\n        S = mod1.predict(x) - B\n        baseline[:, i] = B\n        cate[:, i] = S\n        ES = S.mean()\n\n        ## BLP\n        # assume P(treat|x) = P(treat) = mean(treat)\n        p = treatment.mean()\n        reg_df = pd.DataFrame(dict(\n            y=y, B=B, treatment=treatment, S=S, main=main, excess_S=S-ES\n        ))\n        reg = smf.ols(\"y ~ B + I(treatment-p) + I((treatment-p)*(S-ES))\", data=reg_df.loc[main, :])\n        reg_fit = reg.fit()\n        blp[i, :] = reg_fit.params.iloc[2:4]\n        blp_se[i, :] = get_treatment_se(reg_fit, cluster_id, main)[2:]\n\n        lamb[i, 0] = reg_fit.params.iloc[-1]**2 * S.var()\n\n        ## GATEs\n        cutoffs = np.quantile(S, np.linspace(0,1, n_group + 1))\n        cutoffs[-1] += 1\n        for k in range(n_group):\n            reg_df[f\"G{k}\"] = (cutoffs[k] &lt;= S) & (S &lt; cutoffs[k+1])\n\n        g_form = \"y ~ B + \" + \" + \".join([f\"I((treatment-p)*G{k})\" for k in range(n_group)])\n        g_reg = smf.ols(g_form, data=reg_df.loc[main, :])\n        g_fit = g_reg.fit()\n        gate[i, :] = g_fit.params.values[2:] #g_fit.params.filter(regex=\"G\").values\n        gate_se[i, :] = get_treatment_se(g_fit, cluster_id, main)[2:]\n\n        lamb[i, 1] = (gate[i,:]**2).sum()/n_group\n\n    out = dict(\n        gate=gate, gate_se=gate_se,\n        blp=blp, blp_se=blp_se,\n        Lambda=lamb, baseline=baseline, cate=cate,\n        name=type(model).__name__\n    )\n    return out\n\n\ndef generic_ml_summary(generic_ml_output):\n    out = {\n        x: np.nanmedian(generic_ml_output[x], axis=0)\n        for x in [\"blp\", \"blp_se\", \"gate\", \"gate_se\", \"Lambda\"]\n    }\n    out[\"name\"] = generic_ml_output[\"name\"]\n    return out"
  },
  {
    "objectID": "conditionaleffect.html#code-2",
    "href": "conditionaleffect.html#code-2",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Code",
    "text": "Code\n\ndef generate_report(results):\n    summaries = list(map(generic_ml_summary, results))\n    df_plot = pd.DataFrame({\n        mod[\"name\"]: np.median(mod[\"cate\"], axis=1)\n        for mod in results\n    })\n\n    corrfig=sns.pairplot(df_plot, diag_kind=\"kde\", kind=\"reg\")\n\n    df_cate = pd.concat({\n        s[\"name\"]: pd.DataFrame(dict(blp=s[\"blp\"], se=s[\"blp_se\"]))\n        for s in summaries\n    }).T.stack()\n\n    df_groups = pd.concat({\n        s[\"name\"]: pd.DataFrame(dict(gate=s[\"gate\"], se=s[\"gate_se\"]))\n        for s in summaries\n    }).T.stack()\n    return({\"corr\":df_plot.corr(), \"pairplot\":corrfig, \"BLP\":df_cate,\"GATE\":df_groups})"
  },
  {
    "objectID": "conditionaleffect.html#code-3",
    "href": "conditionaleffect.html#code-3",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Code",
    "text": "Code\n\nimport lightgbm as lgb\nimport io\nfrom contextlib import redirect_stdout, redirect_stderr\nmodels = [\n    linear_model.LassoCV(cv=10, n_alphas=25, max_iter=500, tol=1e-4, n_jobs=20),\n    ensemble.RandomForestRegressor(n_estimators=200, min_samples_leaf=20, n_jobs=20),\n    lgb.LGBMRegressor(n_estimators=200, max_depth=4, reg_lambda=1.0, reg_alpha=0.0, n_jobs=20),\n    neural_network.MLPRegressor(hidden_layer_sizes=(20, 10), max_iter=500, activation=\"logistic\",\n                                solver=\"adam\", tol=1e-3, early_stopping=True, alpha=0.0001)\n]\n\nkw = dict(x=Xl, treatment=treatment, n_split=11, n_group=5, cluster_id=loc_id)\ndef evaluate_models(models, y, **other_kw):\n    all_kw = kw.copy()\n    all_kw[\"y\"] = y\n    all_kw.update(other_kw)\n    # hide many warnings while fitting\n    with io.StringIO() as obuf, redirect_stdout(obuf):\n        with io.StringIO() as ebuf, redirect_stderr(ebuf):\n           results=list(map(lambda x: generic_ml_model(model=x, **all_kw), models))\n           sout = obuf.getvalue()\n           serr = ebuf.getvalue()\n    return([results,sout,serr])"
  },
  {
    "objectID": "conditionaleffect.html#results-birthweight",
    "href": "conditionaleffect.html#results-birthweight",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Results: Birthweight",
    "text": "Results: Birthweight\n\nresults = evaluate_models(models, y=bw);\nreport=generate_report(results[0])\nreport[\"pairplot\"].fig.show()"
  },
  {
    "objectID": "conditionaleffect.html#results-birthweight-1",
    "href": "conditionaleffect.html#results-birthweight-1",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Results: Birthweight",
    "text": "Results: Birthweight\n\nreport[\"corr\"]\n\n\n\n\n\n\n\n\nLassoCV\nRandomForestRegressor\nLGBMRegressor\nMLPRegressor\n\n\n\n\nLassoCV\n1.000000\n0.283262\n0.195667\n0.024557\n\n\nRandomForestRegressor\n0.283262\n1.000000\n0.596180\n-0.155473\n\n\nLGBMRegressor\n0.195667\n0.596180\n1.000000\n-0.089839\n\n\nMLPRegressor\n0.024557\n-0.155473\n-0.089839\n1.000000"
  },
  {
    "objectID": "conditionaleffect.html#results-birthweight-2",
    "href": "conditionaleffect.html#results-birthweight-2",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Results: Birthweight",
    "text": "Results: Birthweight\n\nreport[\"BLP\"]\n\n\n\n\n\n\n\n\n\nLassoCV\nRandomForestRegressor\nLGBMRegressor\nMLPRegressor\n\n\n\n\nblp\n0\n-5.070703e+00\n-7.877588\n-15.150926\n-15.289697\n\n\n1\n3.943529e-15\n0.050554\n0.008801\n-1565.379909\n\n\nse\n0\n3.250762e+01\n32.062612\n32.594646\n33.039435\n\n\n1\n6.188115e-01\n0.274722\n0.120375\n3214.431421"
  },
  {
    "objectID": "conditionaleffect.html#results-birthweight-3",
    "href": "conditionaleffect.html#results-birthweight-3",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Results: Birthweight",
    "text": "Results: Birthweight\n\nreport[\"GATE\"]\n\n\n\n\n\n\n\n\n\nLassoCV\nRandomForestRegressor\nLGBMRegressor\nMLPRegressor\n\n\n\n\ngate\n0\n0.000000e+00\n6.806915\n-10.649634\n-1.788870\n\n\n1\n0.000000e+00\n-17.484799\n-6.989461\n-20.864722\n\n\n2\n0.000000e+00\n-71.999452\n-15.407855\n-0.502629\n\n\n3\n0.000000e+00\n-27.436748\n-70.814267\n-45.545684\n\n\n4\n-1.374993e+01\n35.168126\n2.692431\n-37.768674\n\n\nse\n0\n6.922868e-14\n66.933201\n65.686446\n83.811558\n\n\n1\n6.056362e+01\n69.575429\n68.657312\n67.389681\n\n\n2\n0.000000e+00\n68.445318\n68.883673\n69.075477\n\n\n3\n0.000000e+00\n72.971604\n71.278058\n65.236928\n\n\n4\n3.551728e+01\n73.410124\n75.470398\n66.160513"
  },
  {
    "objectID": "conditionaleffect.html#results-assisted-delivery",
    "href": "conditionaleffect.html#results-assisted-delivery",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Results: Assisted Delivery",
    "text": "Results: Assisted Delivery\n\nad = df.loc[X.index, \"good_assisted_delivery\"]\nresults_ad = evaluate_models(models, y=ad)\nreport_ad=generate_report(results_ad[0])\nreport_ad[\"pairplot\"].fig.show()"
  },
  {
    "objectID": "conditionaleffect.html#results-assisted-delivery-1",
    "href": "conditionaleffect.html#results-assisted-delivery-1",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Results: Assisted Delivery",
    "text": "Results: Assisted Delivery\n\nreport_ad[\"corr\"]\n\n\n\n\n\n\n\n\nLassoCV\nRandomForestRegressor\nLGBMRegressor\nMLPRegressor\n\n\n\n\nLassoCV\n1.000000\n0.843195\n0.740900\n0.832495\n\n\nRandomForestRegressor\n0.843195\n1.000000\n0.757497\n0.708102\n\n\nLGBMRegressor\n0.740900\n0.757497\n1.000000\n0.566446\n\n\nMLPRegressor\n0.832495\n0.708102\n0.566446\n1.000000"
  },
  {
    "objectID": "conditionaleffect.html#results-assisted-delivery-2",
    "href": "conditionaleffect.html#results-assisted-delivery-2",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Results: Assisted Delivery",
    "text": "Results: Assisted Delivery\n\nreport_ad[\"BLP\"]\n\n\n\n\n\n\n\n\n\nLassoCV\nRandomForestRegressor\nLGBMRegressor\nMLPRegressor\n\n\n\n\nblp\n0\n0.049976\n0.047267\n0.036169\n0.040557\n\n\n1\n0.518060\n0.412872\n0.272907\n0.409752\n\n\nse\n0\n0.021068\n0.020897\n0.022371\n0.020618\n\n\n1\n0.139788\n0.142686\n0.094551\n0.135706"
  },
  {
    "objectID": "conditionaleffect.html#results-assisted-delivery-3",
    "href": "conditionaleffect.html#results-assisted-delivery-3",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Results: Assisted Delivery",
    "text": "Results: Assisted Delivery\n\nreport_ad[\"GATE\"]\n\n\n\n\n\n\n\n\n\nLassoCV\nRandomForestRegressor\nLGBMRegressor\nMLPRegressor\n\n\n\n\ngate\n0\n-0.019896\n-0.006858\n-0.040657\n-0.021221\n\n\n1\n-0.001354\n0.005503\n0.022481\n0.011613\n\n\n2\n0.036199\n0.018358\n-0.011400\n0.002561\n\n\n3\n0.103179\n0.070919\n0.078308\n0.061261\n\n\n4\n0.172855\n0.197846\n0.161808\n0.176111\n\n\nse\n0\n0.045238\n0.045984\n0.051473\n0.033738\n\n\n1\n0.043951\n0.043944\n0.047367\n0.041222\n\n\n2\n0.042064\n0.037539\n0.045388\n0.047464\n\n\n3\n0.044775\n0.043148\n0.046665\n0.050273\n\n\n4\n0.054797\n0.055317\n0.047589\n0.053421"
  },
  {
    "objectID": "conditionaleffect.html#covariate-means-by-group",
    "href": "conditionaleffect.html#covariate-means-by-group",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Covariate Means by Group",
    "text": "Covariate Means by Group\n\ndef cov_mean_by_group(y, res, cluster_id):\n    n_group = res[\"gate\"].shape[1]\n    gate = res[\"gate\"].copy()\n    gate_se = gate.copy()\n    dat = y.to_frame()\n\n    for i in range(res[\"cate\"].shape[1]):\n        S = res[\"cate\"][:, i]\n        cutoffs = np.quantile(S, np.linspace(0, 1, n_group+1))\n        cutoffs[-1] += 1\n        for k in range(n_group):\n            dat[f\"G{k}\"] = ((cutoffs[k] &lt;= S) & (S &lt; cutoffs[k+1])) * 1.0\n\n        g_form = \"y ~ -1 + \" + \" + \".join([f\"G{k}\" for k in range(n_group)])\n        g_reg = smf.ols(g_form, data=dat.astype(float))\n        g_fit = g_reg.fit()\n        gate[i, :] = g_fit.params.filter(regex=\"G\").values\n        rows = ~y.isna()\n        gate_se[i, :] = get_treatment_se(g_fit, cluster_id, rows)\n\n    out = pd.DataFrame(dict(\n        mean=np.nanmedian(gate, axis=0),\n        se=np.nanmedian(gate_se, axis=0),\n        group=list(range(n_group))\n    ))\n\n    return out\n\ndef compute_group_means_for_results(results, variables, df2):\n    to_cat = []\n    for res in results:\n        for v in variables:\n            to_cat.append(\n                cov_mean_by_group(df2[v], res, loc_id)\n                .assign(method=res[\"name\"], variable=v)\n            )\n\n    group_means = pd.concat(to_cat, ignore_index=True)\n    group_means[\"plus2sd\"] = group_means.eval(\"mean + 1.96*se\")\n    group_means[\"minus2sd\"] = group_means.eval(\"mean - 1.96*se\")\n    return group_means\n\ndef groupmeanfig(group_means):\n    g = sns.FacetGrid(group_means, col=\"variable\", col_wrap=min(3,group_means.variable.nunique()), hue=\"method\", sharey=False)\n    g.map(plt.plot, \"group\", \"mean\")\n    g.map(plt.plot, \"group\", \"plus2sd\", ls=\"--\")\n    g.map(plt.plot, \"group\", \"minus2sd\", ls=\"--\")\n    g.add_legend();\n    return(g)"
  },
  {
    "objectID": "conditionaleffect.html#covariate-means-by-group-1",
    "href": "conditionaleffect.html#covariate-means-by-group-1",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Covariate Means by Group",
    "text": "Covariate Means by Group\n\ndf2 = df.loc[X.index, :]\ndf2[\"edu99\"] = df2.edu == 99\ndf2[\"educ\"] = df2[\"edu\"]\ndf2.loc[df2[\"edu99\"], \"educ\"] = np.nan\n\nvariables1 = [\"log_xp_percap\",\"agecat\",\"educ\"]\nvariables2 =[\"tv\",\"goat\",\"cow\",\n             \"motorbike\", \"hh_cook_wood\",\"hh_toilet_own\"]\ngroup_means_ad = compute_group_means_for_results(results_ad[0], variables1, df2)"
  },
  {
    "objectID": "conditionaleffect.html#covariate-means-by-group-2",
    "href": "conditionaleffect.html#covariate-means-by-group-2",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Covariate Means by Group",
    "text": "Covariate Means by Group\n\ng = groupmeanfig(group_means_ad)\ng.fig.show()"
  },
  {
    "objectID": "conditionaleffect.html#covariate-means-by-group-3",
    "href": "conditionaleffect.html#covariate-means-by-group-3",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Covariate Means by Group",
    "text": "Covariate Means by Group\n\ng = groupmeanfig(compute_group_means_for_results(results_ad[0], variables2, df2))\ng.fig.show()"
  },
  {
    "objectID": "conditionaleffect.html#treatment-participation-by-group",
    "href": "conditionaleffect.html#treatment-participation-by-group",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Treatment Participation by Group",
    "text": "Treatment Participation by Group\n\ng = groupmeanfig(compute_group_means_for_results(results_ad[0], [\"pkh_ever\"], df2))\ng.fig.show()"
  },
  {
    "objectID": "conditionaleffect.html#sources-and-further-reading",
    "href": "conditionaleffect.html#sources-and-further-reading",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nSection on generic ML for heterogeneous effects is based on Chernozhukov et al. (2023) and my earlier notes"
  },
  {
    "objectID": "conditionaleffect.html#references",
    "href": "conditionaleffect.html#references",
    "title": "Treatment Heterogeneity and Conditional Effects",
    "section": "References",
    "text": "References\n\n\n\n\nAlatas, Vivi, Nur Cahyadi, Elisabeth Ekasari, Sarah Harmoun, Budi Hidayat, Edgar Janz, Jon Jellema, H Tuhiman, and M Wai-Poi. 2011. “Program Keluarga Harapan : Impact Evaluation of Indonesia’s Pilot Household Conditional Cash Transfer Program.” World Bank. http://documents.worldbank.org/curated/en/589171468266179965/Program-Keluarga-Harapan-impact-evaluation-of-Indonesias-Pilot-Household-Conditional-Cash-Transfer-Program.\n\n\nChernozhukov, Victor, Mert Demirer, Esther Duflo, and Iván Fernández-Val. 2023. “Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments, with an Application to Immunization in India.”\n\n\nImai, Kosuke, and Michael Lingzhi Li. 2022. “Statistical Inference for Heterogeneous Treatment Effects Discovered by Generic Machine Learning in Randomized Experiments.” In. https://api.semanticscholar.org/CorpusID:247762848.\n\n\nTriyana, Margaret. 2016. “Do Health Care Providers Respond to Demand-Side Incentives? Evidence from Indonesia.” American Economic Journal: Economic Policy 8 (4): 255–88. https://doi.org/10.1257/pol.20140048."
  },
  {
    "objectID": "matching.html#setting",
    "href": "matching.html#setting",
    "title": "Matching",
    "section": "Setting",
    "text": "Setting\n\nPotential outcomes \\((Y(0), Y(1))\\)\nTreatment \\(T \\in \\{0,1\\}\\)\nObserve \\(Y = Y(T)\\)\nCovariates \\(X\\)\nAssume conditional independence \\((Y(0),Y(1)) \\perp T | X\\)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]"
  },
  {
    "objectID": "matching.html#why-not-regression",
    "href": "matching.html#why-not-regression",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\nAverage treatment effect \\[\nATE = \\int \\Er[Y|T=1,X=x] - \\Er[Y|T=0,X=x] dP(x)\n\\]\nRegression gives the best linear approximation to \\(\\Er[Y|T,X]\\), so why not just estimate linear regression \\[\nY_i = \\hat{\\alpha} T_i + X_i'\\hat{\\beta} + \\hat{\\epsilon}_i\n\\] and, and then use \\(\\hat{\\alpha}\\) as an estimate of the ATE?"
  },
  {
    "objectID": "matching.html#why-not-regression-1",
    "href": "matching.html#why-not-regression-1",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\nPartial out (Frish-Waugh-Lovell theorem) \\[\n\\begin{align*}\n\\hat{\\alpha} = & \\frac{\\frac{1}{n} \\sum_{i=1}^n Y_i (T_i - X_i'(X'X)^{-1}X'T)}\n  {\\frac{1}{n} \\sum_{i=1}^n (T_i - X_i'(X'X)^{-1}X'T)^2} \\\\\n  \\inprob & \\Er\\left[Y_i \\underbrace{\\frac{T_i - X_i'\\pi}{\\Er[(T_i - X_i'\\pi)^2]}}_{\\equiv \\omega(T_i,X_i)}\\right] \\\\\n  = & \\Er\\left[Y_{0,i} \\omega(T_i,X_i)\\right] + \\Er\\left[(Y_{1,i}-Y_{0,i}) \\omega(T_i,X_i)T_i\\right]\n\\end{align*}\n\\] where \\(\\pi = \\argmin_{\\tilde{\\pi}} \\Er[(T_i - X_i'\\tilde{\\pi})^2]\\)\nNote: \\(\\Er[\\omega(T,X)] = 0\\), \\(\\Er[T\\omega(T,X)] = 1\\)"
  },
  {
    "objectID": "matching.html#why-not-regression-2",
    "href": "matching.html#why-not-regression-2",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\n\\(\\plim \\hat{\\alpha} = \\Er\\left[Y_{0,i} \\omega(T_i,X_i)\\right] + \\Er\\left[(Y_{1,i}-Y_{0,i}) \\omega(T_i,X_i)T_i\\right]\\)\nWhat can be in the range of \\(\\omega(T,X) = \\frac{T - X'\\pi}{\\Er[(T_i - X_i'\\pi)^2]}\\)?"
  },
  {
    "objectID": "matching.html#why-not-regression-3",
    "href": "matching.html#why-not-regression-3",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\n\nimports\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nimport scipy\nstyle.use(\"fivethirtyeight\")\n\n\n\nnp.random.seed(1234)\n\ndef simulate(n, pi=np.array([0,1])):\n    X = np.random.randn(n, len(pi))\n    X[:,0] = 1\n    P = np.clip(scipy.stats.norm.cdf(X @ pi), 0.05, 0.95)\n    T = 1*(np.random.rand(n) &lt; P)\n    Ey0x = np.zeros(n)\n    Ey1x = np.exp(3*(X[:,1]-2))\n    y0 = Ey0x + np.random.randn(n)\n    y1 = Ey1x + np.random.randn(n)\n    y = T*y1 + (1-T)*y0\n    return(X,T,y,y0,y1, Ey0x, Ey1x, P)\n\nX,T,y,y0,y1,Ey0x,Ey1x,P = simulate(4000)\n\npihat = np.linalg.solve(X.T @ X, X.T @ T)\nw = T - X @ pihat\nw = w/np.mean(w**2);"
  },
  {
    "objectID": "matching.html#why-not-regression-4",
    "href": "matching.html#why-not-regression-4",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\nTX = np.hstack((T.reshape(len(T),1),X))\nabhat = np.linalg.solve(TX.T @ TX, TX.T @ y)\nahat = abhat[0]\nprint(ahat)\n\n-0.11952795827329041\n\n\n\nnp.mean(y1-y0)\n\nnp.float64(0.1172689998868494)\n\n\n\nWeights, \\(\\omega(T,X)\\), are not all positive, so the regression estimate can be negative even if \\(\\Er[Y(1) | X] - \\Er[Y(0)|X]\\) is positive everywhere"
  },
  {
    "objectID": "matching.html#why-not-regression-5",
    "href": "matching.html#why-not-regression-5",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\n\nplot\nimport matplotlib.cm as cm\nfig, axes = plt.subplots(2, 1, figsize=(6, 6))\n\n# Create a scatter plot for the first panel (left)\naxes[0].scatter(X[:,1], w, c=T, cmap=cm.Dark2)\naxes[0].set_xlabel(\"X\")\naxes[0].set_ylabel(\"ωT\")\naxes[0].set_title(\"Weights\")\n\naxes[1].scatter(X[:,1], y1-y0, label=\"TE\")\naxes[1].set_xlabel(\"X\")\naxes[1].set_ylabel(\"Y₁ - Y₀\")\naxes[1].set_title(\"Treatment effects\")\nsort_idx = np.argsort(X[:,1])\naxes[1].plot(X[sort_idx,1], Ey1x[sort_idx]-Ey0x[sort_idx], label=\"E[y1-y0|x]\")\n\n# Display the plot\nplt.tight_layout()  # Ensure proper layout spacing\nplt.show()"
  },
  {
    "objectID": "matching.html#matching-1",
    "href": "matching.html#matching-1",
    "title": "Matching",
    "section": "Matching",
    "text": "Matching\n\nIf not regression, then what? \\[\nATE = \\int \\Er[Y|T=1,X=x] - \\Er[Y|T=0,X=x] dP(x)\n\\]"
  },
  {
    "objectID": "matching.html#plug-in-estimator",
    "href": "matching.html#plug-in-estimator",
    "title": "Matching",
    "section": "Plug-in estimator",
    "text": "Plug-in estimator\n\nPlug in estimator: \\[\n\\widehat{ATE} = \\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{E}[Y|T=1,X=X_i] - \\hat{E}[Y|T=0,X=X_i] \\right)\n\\] where \\(\\hat{E}[Y|T,X]\\) is some flexible estimator for \\(\\Er[Y|T,X]\\)\n\nif \\(X\\) is discrete, \\(\\hat{E}\\) can be conditional averages or equivalently, “saturated” regression\nif \\(X\\) continuous, \\(\\hat{E}\\) can be some nonparametric regression estimator\nOriginal approaches to this problem used nearest neighbor matching to estimate \\(\\hat{E}[Y|T,X]\\)\n\nDownside:\n\nDifficult statistical properties — choice of tuning parameters, strong assumptions needed, failure of bootstrap for nearest neighbors Abadie and Imbens (2008)"
  },
  {
    "objectID": "matching.html#propensity-score",
    "href": "matching.html#propensity-score",
    "title": "Matching",
    "section": "Propensity Score",
    "text": "Propensity Score\n\nLet \\(p(X) = P(T=1|X=X)\\)\nNote: \\[\n\\begin{align*}\n\\Er[Y|X,T=1] - \\Er[Y|X,T=0] = & E\\left[\\frac{Y T}{p(X)}|X \\right] - E\\left[\\frac{Y(1-T)}{1-p(X)}|X \\right] \\\\\n= & E\\left[ Y \\frac{T - p(X)}{p(X)(1-p(X))} | X \\right]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "matching.html#propensity-score-1",
    "href": "matching.html#propensity-score-1",
    "title": "Matching",
    "section": "Propensity Score",
    "text": "Propensity Score\n\nso \\[\nATE = \\Er\\left[ \\frac{Y T}{p(X)} -  \\frac{Y(1-T)}{1-p(X)}\\right] = \\Er\\left[ Y \\frac{T - p(X)}{p(X)(1-p(X))} \\right]\n\\]"
  },
  {
    "objectID": "matching.html#inverse-propensity-weighting",
    "href": "matching.html#inverse-propensity-weighting",
    "title": "Matching",
    "section": "Inverse propensity weighting",
    "text": "Inverse propensity weighting\n\nEstimator \\[\n\\widehat{ATE}^{IPW} = \\frac{1}{n} \\sum_{i=1}^n \\frac{Y_iT_i}{\\hat{p}(X_i)} - \\frac{Y_i(1-T_i)}{1-\\hat{p}(X_i)}\n\\] where \\(\\hat{p}(X)\\) is some flexible estimator for \\(P(T=1|X)\\)\nDownside:\n\nDifficult statistical properties — choice of tuning parameters, strong assumptions needed"
  },
  {
    "objectID": "matching.html#doubly-robust-estimator",
    "href": "matching.html#doubly-robust-estimator",
    "title": "Matching",
    "section": "Doubly Robust Estimator",
    "text": "Doubly Robust Estimator\n\nCombines plug-in and IPW estimators\nEstimator \\[\n\\begin{align*}\n\\widehat{ATE}^{DR} = & \\frac{1}{n} \\sum_{i=1}^n \\hat{E}[Y|T=1,X=X_i] - \\hat{E}[Y|T=0,X=X_i] + \\\\\n& + \\frac{1}{n} \\sum_{i=1}^n  \\frac{T_i(Y_i - \\hat{E}[Y|T=1,X=X_i])}{\\hat{p}(X_i)} - \\\\\n& - \\frac{(1-T_i)(Y_i - \\hat{E}[Y|T=0,X=X_i])} {1-\\hat{p}(X_i)}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "matching.html#doubly-robust-estimator-1",
    "href": "matching.html#doubly-robust-estimator-1",
    "title": "Matching",
    "section": "Doubly Robust Estimator",
    "text": "Doubly Robust Estimator\n\nDoubly robust in that:\n\nConsistent as long as either \\(\\hat{p}(X) \\inprob p(X)\\) or \\(\\hat{E}[Y|T,X] \\inprob \\Er[Y|T,X]\\)\nInsensitive to small changes in \\(\\hat{p}(X)\\) or \\(\\hat{E}[Y|T,X]\\)\n\nAllows nicer statistical properties\n\nWeaker assumptions needed\nAsymptotic distribution is the same as if \\(p(X)\\) and \\(\\Er[Y|T,X]\\) were known"
  },
  {
    "objectID": "matching.html#software",
    "href": "matching.html#software",
    "title": "Matching",
    "section": "Software",
    "text": "Software\n\nAdvice: use the doubly robust estimator with nonparametric estimates for \\(\\hat{E}[Y|T,X]\\) and \\(\\hat{p}(X)\\)\nRecommended package:\n\ndoubleml"
  },
  {
    "objectID": "matching.html#software-1",
    "href": "matching.html#software-1",
    "title": "Matching",
    "section": "Software",
    "text": "Software\n\nOther packages:\n\ncausalinference has a double robust estimator, but it estimates \\(\\hat{E}[Y|T,X]\\) via linear regression and \\(\\hat{p}(X)\\) via logit (maybe probit, not sure)\n\ncan make nonparametric by adding e.g. powers of \\(x\\) to \\(X\\), but need to manage manually\n\nzEpid is similiar to causalinference, but has a formula interface, so slightly easier to make model more flexible"
  },
  {
    "objectID": "matching.html#example-simulation",
    "href": "matching.html#example-simulation",
    "title": "Matching",
    "section": "Example: simulation",
    "text": "Example: simulation\n\nInfeasible estimator: average of \\(Y(1) - Y(0)\\)\n\n\nse = np.sqrt(np.var(y1-y0)/len(y1))\nate = np.mean(y1-y0)\nprint(f\"Infeasible estimator with potential outcomes observed = {ate:.2} with 95% CI = [{ate-1.96*se:.2},{ate+1.96*se:.2}]\\n\")\n\nInfeasible estimator with potential outcomes observed = 0.12 with 95% CI = [0.058,0.18]\n\n\n\n\nInfeasible estimator: doubly robust with true \\(\\Er[Y(T)|X]\\) and \\(p(X)\\)\n\n\naipw = Ey1x - Ey0x + T*(y - Ey1x)/P - (1-T)*(y - Ey0x)/(1-P)\nse = np.sqrt(np.var(aipw)/len(aipw))\nprint(f\"Infeasible estimator with true E[y1-y0|x] = and p(x) {np.mean(aipw):.2} with 95% CI = [{np.mean(aipw)-1.96*se:.2},{np.mean(aipw)+1.96*se:.2}]\\n\")\n\nInfeasible estimator with true E[y1-y0|x] = and p(x) 0.15 with 95% CI = [0.057,0.24]"
  },
  {
    "objectID": "matching.html#example-simulation-1",
    "href": "matching.html#example-simulation-1",
    "title": "Matching",
    "section": "Example: simulation",
    "text": "Example: simulation\n\nimport doubleml as dml\n\ng = Pipeline([('poly',PolynomialFeatures(degree=10)),\n              ('scale', StandardScaler()),\n              ('lasso',LassoCV(max_iter=10_000, n_jobs=-1, selection='random', tol=1e-2,\n                               alphas=10**np.linspace(-3,3,20)))])\n\nm = Pipeline([('rbf',RBFSampler(gamma=gamma, random_state=1, n_components=nc)),\n              ('logistic',LogisticRegressionCV(n_jobs=-1, scoring='neg_log_loss'))])"
  },
  {
    "objectID": "matching.html#national-study-of-learning-mindsets",
    "href": "matching.html#national-study-of-learning-mindsets",
    "title": "Matching",
    "section": "National Study of Learning Mindsets",
    "text": "National Study of Learning Mindsets\n\nOriginal study by Yeager et al. (2019)\nSynthetic data created by Athey and Wager (2019), downloaded from Facure (2022)"
  },
  {
    "objectID": "matching.html#data",
    "href": "matching.html#data",
    "title": "Matching",
    "section": "Data",
    "text": "Data\n\n\nimports\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\nstyle.use(\"fivethirtyeight\")\npd.set_option(\"display.max_columns\", 20)\ndatadir=\"./data\"\n\n\n\ndata = pd.read_csv(datadir+\"/learning_mindset.csv\")\ndata.sample(5, random_state=431)\n\n\n\n\n\n\n\n\nschoolid\nintervention\nachievement_score\nsuccess_expect\nethnicity\ngender\nfrst_in_family\nschool_urbanicity\nschool_mindset\nschool_achievement\nschool_ethnic_minority\nschool_poverty\nschool_size\n\n\n\n\n9366\n9\n0\n1.137192\n6\n1\n1\n1\n4\n1.324323\n-1.311438\n1.930281\n0.281143\n0.362031\n\n\n7810\n27\n0\n-0.554268\n5\n2\n1\n1\n1\n0.240267\n-0.785287\n0.611807\n0.612568\n-0.116284\n\n\n7532\n29\n0\n-0.462576\n6\n1\n1\n1\n1\n-0.373087\n0.113096\n-0.833417\n-1.924778\n-1.147314\n\n\n10381\n1\n0\n-0.402644\n5\n2\n2\n1\n3\n1.185986\n-1.129889\n1.009875\n1.005063\n-1.174702\n\n\n1244\n57\n1\n1.528680\n6\n4\n1\n1\n2\n0.097162\n-0.292353\n-1.030865\n-0.813799\n0.184716"
  },
  {
    "objectID": "matching.html#evidence-of-confounding",
    "href": "matching.html#evidence-of-confounding",
    "title": "Matching",
    "section": "Evidence of Confounding",
    "text": "Evidence of Confounding\n\n\nCode\ndef std_error(x):\n    return np.std(x, ddof=1) / np.sqrt(len(x))\n\ngrouped = data.groupby('success_expect')['intervention'].agg(['mean', std_error])\ngrouped = grouped.reset_index()\n\nfig, ax = plt.subplots()\nplt.errorbar(grouped['success_expect'],grouped['mean'],yerr=1.96*grouped['std_error'],fmt=\"o\")\nax.set_xlabel('student expectation of success')\nax.set_ylabel('P(treatment)')\nplt.show()"
  },
  {
    "objectID": "matching.html#unadjusted-estimate-of-ate",
    "href": "matching.html#unadjusted-estimate-of-ate",
    "title": "Matching",
    "section": "Unadjusted estimate of ATE",
    "text": "Unadjusted estimate of ATE\n\nprint(smf.ols(\"achievement_score ~ intervention\", data=data).fit(cov_type=\"HC3\").summary().tables[1])\n\n================================================================================\n                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept       -0.1538      0.012    -13.289      0.000      -0.176      -0.131\nintervention     0.4723      0.021     22.968      0.000       0.432       0.513\n================================================================================\n\n\n\nprint(smf.ols(\"achievement_score ~ intervention\", data=data).fit(\n    cov_type=\"cluster\", cov_kwds={'groups': data['schoolid']}).summary().tables[1])\n\n================================================================================\n                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept       -0.1538      0.036     -4.275      0.000      -0.224      -0.083\nintervention     0.4723      0.025     19.184      0.000       0.424       0.521\n================================================================================"
  },
  {
    "objectID": "matching.html#unadjusted-estimate-of-ate-1",
    "href": "matching.html#unadjusted-estimate-of-ate-1",
    "title": "Matching",
    "section": "Unadjusted estimate of ATE",
    "text": "Unadjusted estimate of ATE\n\n\nCode\nfig,ax=plt.subplots()\nplt.hist(data.query(\"intervention==0\")[\"achievement_score\"], bins=20, alpha=0.3, color=\"C2\")\nplt.hist(data.query(\"intervention==1\")[\"achievement_score\"], bins=20, alpha=0.3, color=\"C3\")\nplt.vlines(-0.1538, 0, 300, label=\"Untreated\", color=\"C2\")\nplt.vlines(-0.1538+0.4723, 0, 300, label=\"Treated\", color=\"C3\")\nax.set_xlabel(\"Achievement Score\")\nax.set_ylabel(\"N\")\nplt.legend()\nplt.show();"
  },
  {
    "objectID": "matching.html#regression-estimate-of-ate",
    "href": "matching.html#regression-estimate-of-ate",
    "title": "Matching",
    "section": "Regression estimate of ATE",
    "text": "Regression estimate of ATE\n\nols = smf.ols(\"achievement_score ~ intervention + success_expect + ethnicity + gender + frst_in_family + school_urbanicity + school_mindset + school_achievement + school_ethnic_minority + school_poverty + school_size\",data=data).fit(cov_type=\"cluster\", cov_kwds={'groups': data['schoolid']})\nprint(ols.summary().tables[1])\n\n==========================================================================================\n                             coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------------\nIntercept                 -1.7786      0.110    -16.214      0.000      -1.994      -1.564\nintervention               0.3964      0.025     15.558      0.000       0.346       0.446\nsuccess_expect             0.3746      0.009     41.120      0.000       0.357       0.392\nethnicity                  0.0043      0.003      1.639      0.101      -0.001       0.009\ngender                    -0.2684      0.016    -16.299      0.000      -0.301      -0.236\nfrst_in_family            -0.1310      0.019     -6.770      0.000      -0.169      -0.093\nschool_urbanicity          0.0573      0.036      1.613      0.107      -0.012       0.127\nschool_mindset            -0.1484      0.044     -3.341      0.001      -0.235      -0.061\nschool_achievement        -0.0253      0.055     -0.457      0.647      -0.134       0.083\nschool_ethnic_minority     0.1197      0.034      3.471      0.001       0.052       0.187\nschool_poverty            -0.0154      0.054     -0.284      0.776      -0.122       0.091\nschool_size               -0.0467      0.044     -1.060      0.289      -0.133       0.040\n=========================================================================================="
  },
  {
    "objectID": "matching.html#regression-estimate-of-ate-weights",
    "href": "matching.html#regression-estimate-of-ate-weights",
    "title": "Matching",
    "section": "Regression estimate of ATE: weights",
    "text": "Regression estimate of ATE: weights\n\nlpm = smf.ols(\"intervention ~ success_expect + ethnicity + gender + frst_in_family + school_urbanicity + school_mindset + school_achievement + school_ethnic_minority + school_poverty + school_size\",data=data).fit(cov_type=\"cluster\", cov_kwds={'groups': data['schoolid']})\nw = lpm.resid / np.var(lpm.resid)\nprint(np.mean(data.achievement_score*w))\n\n0.39640236033389553"
  },
  {
    "objectID": "matching.html#regression-estimate-of-ate-weights-1",
    "href": "matching.html#regression-estimate-of-ate-weights-1",
    "title": "Matching",
    "section": "Regression estimate of ATE: weights",
    "text": "Regression estimate of ATE: weights\n\n\nCode\nfig,ax=plt.subplots()\nplt.hist(w[data.intervention==0], bins=20, alpha=0.3, color=\"C2\", label=\"Untreated\")\nplt.hist(w[data.intervention==1], bins=20, alpha=0.3, color=\"C3\", label=\"Treated\")\nax.set_xlabel(\"w\")\nax.set_ylabel(\"N\")\nplt.legend()\nplt.show();"
  },
  {
    "objectID": "matching.html#propensity-score-matching",
    "href": "matching.html#propensity-score-matching",
    "title": "Matching",
    "section": "Propensity Score Matching",
    "text": "Propensity Score Matching\n\ncateg = [\"ethnicity\", \"gender\", \"school_urbanicity\",\"success_expect\"]\ncont = [\"school_mindset\", \"school_achievement\", \"school_ethnic_minority\", \"school_poverty\", \"school_size\"]\n\ndata_with_categ = pd.concat([\n    data.drop(columns=categ), # dataset without the categorical features\n    pd.get_dummies(data[categ], columns=categ, drop_first=False)# categorical features converted to dummies\n], axis=1)\n\nprint(data_with_categ.shape)\nT = 'intervention'\nY = 'achievement_score'\nX = data_with_categ.columns.drop(['schoolid', T, Y])\n\n(10391, 38)"
  },
  {
    "objectID": "matching.html#propensity-score-matching-1",
    "href": "matching.html#propensity-score-matching-1",
    "title": "Matching",
    "section": "Propensity Score Matching",
    "text": "Propensity Score Matching\n\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsRegressor\nimport sklearn\n\ndef propensitymatching(T,Y,X,psmodel=LogisticRegressionCV(scoring='neg_log_loss'),neighbormodel=KNeighborsRegressor(n_neighbors=1,algorithm='auto',weights='uniform')):\n    pfit = psmodel.fit(X,T)\n    ps = pfit.predict_proba(X)[:,1]\n    ey1 = neighbormodel.fit(ps[T==1].reshape(-1,1),Y[T==1])\n    ey0 = sklearn.base.clone(neighbormodel).fit(ps[T==0].reshape(-1,1),Y[T==0])\n    tex = ey1.predict(ps.reshape(-1,1)) - ey0.predict(ps.reshape(-1,1))\n    ate = np.mean(tex)\n    return(ate, tex,ps)\n\nate,tex,ps=propensitymatching(data_with_categ[T],data_with_categ[Y],data_with_categ[X])\nprint(ate)\n\n0.4163456073819457"
  },
  {
    "objectID": "matching.html#propensity-score-matching-2",
    "href": "matching.html#propensity-score-matching-2",
    "title": "Matching",
    "section": "Propensity Score Matching",
    "text": "Propensity Score Matching\n\n\nCode\nfig, ax = plt.subplots(2,1)\ntreat = data.intervention\nax[0].scatter(ps[treat==0],tex[treat==0],color=\"C2\")\nax[0].scatter(ps[treat==1],tex[treat==1],color=\"C3\")\nax[1].hist(ps[treat==0],bins=20,color=\"C2\",label=\"Untreated\")\nax[1].hist(ps[treat==1],bins=20,color=\"C3\",label=\"Treated\")\nax[1].set_xlabel(\"P(Treatment)\")\nax[1].set_ylabel(\"N\")\nax[0].set_ylabel(\"E[Y|T=1,P(X)] - E[Y|T=0,P(X)]\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "matching.html#inverse-propensity-weighting-1",
    "href": "matching.html#inverse-propensity-weighting-1",
    "title": "Matching",
    "section": "Inverse Propensity Weighting",
    "text": "Inverse Propensity Weighting\n\ndef ipw(T,Y,X,psmodel=LogisticRegressionCV(scoring='neg_log_loss')):\n    pfit = psmodel.fit(X,T)\n    ps = pfit.predict_proba(X)[:,1]\n    ate=np.mean(Y*(T - ps)/(ps*(1-ps)))\n    return(ate,ps)\n\nate,ps = ipw(data_with_categ[T],data_with_categ[Y],data_with_categ[X])\nprint(ate)\n\n0.44256557543538383"
  },
  {
    "objectID": "matching.html#doubly-robust",
    "href": "matching.html#doubly-robust",
    "title": "Matching",
    "section": "Doubly Robust",
    "text": "Doubly Robust\n\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.linear_model import LassoCV, LogisticRegressionCV, ElasticNetCV\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef robustate(T,Y,X,psmodel=LogisticRegressionCV(scoring='neg_log_loss'),ymodel=LassoCV(), cluster=None):\n    pfit = psmodel.fit(X,T)\n    ps = pfit.predict_proba(X)[:,1]\n    ey1fit = ymodel.fit(X[T==1],Y[T==1])\n    ey0fit = sklearn.base.clone(ymodel).fit(X[T==0],Y[T==0])\n    ey1 = ey1fit.predict(X)\n    ey0 = ey0fit.predict(X)\n    ate_terms = ey1 - ey0 + T*(Y- ey1)/ps - (1-T)*(Y-ey0)/(1-ps)\n    ate = np.mean(ate_terms)\n    # check if cluster is None\n    if cluster is None :\n        ate_se = np.sqrt(np.var(ate_terms)/len(ate_terms))\n    else :\n        creg=smf.ols(\"y ~ 1\", pd.DataFrame({\"y\" : ate_terms})).fit(cov_type=\"cluster\", cov_kwds={'groups': cluster})\n        ate_se = np.sqrt(creg.cov_params().iloc[0,0])\n\n    return(ate, ate_se, ps, ey1,ey0)\n\nate,se,ps,ey1,ey0 = robustate(data_with_categ[T],data_with_categ[Y],data_with_categ[X],cluster=data_with_categ['schoolid'])\nprint(ate-1.96*se, ate, ate+1.96*se)\n\n0.3317146398974234 0.3832790165781086 0.4348433932587938\n\n\n1\nWe have glossed over some details needed for doubly robust estimation to have nice statistical properties. Those details matter and are not implemented correctly above. It is better to use doubleml instead."
  },
  {
    "objectID": "matching.html#doubly-robust-1",
    "href": "matching.html#doubly-robust-1",
    "title": "Matching",
    "section": "Doubly Robust",
    "text": "Doubly Robust\n\nbetter to use the doubleml package\n\n\nimport doubleml as dml\n\nm = LassoCV()\ng = LogisticRegressionCV(scoring='neg_log_loss')\ndmldata = dml.DoubleMLData(data_with_categ, Y,T,X.to_list())\ndmlATE = dml.DoubleMLAPOS(dmldata, m, g, treatment_levels=[0, 1])\ndmlATE.fit()\ndmlATE.summary\ndmlATE.causal_contrast(0).summary\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n2.5 %\n97.5 %\n\n\n\n\n1 vs 0\n0.382427\n0.017134\n22.319628\n0.0\n0.348844\n0.416009"
  },
  {
    "objectID": "matching.html#sources-and-further-reading",
    "href": "matching.html#sources-and-further-reading",
    "title": "Matching",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nUseful additional reading is chapters 10-12 of Facure (2022) and chapter 14 of Huntington-Klein (2021).1\nChapters 5 and 10 of Chernozhukov et al. (2024)\nThe representation of the estimate from a linear model as a weighted average is based on Borusyak and Jaravel (2018)\nThe growth mindset example is take from Facure (2022)\n\nThese slides do not mention the importance of overlap/balance, but hopefully I emphasized it during lecture. Overlap is very important in practice. The reading, especially Huntington-Klein (2021), cover it pretty well."
  },
  {
    "objectID": "matching.html#references",
    "href": "matching.html#references",
    "title": "Matching",
    "section": "References",
    "text": "References\n\n\n\n\nAbadie, Alberto, and Guido W. Imbens. 2008. “On the Failure of the Bootstrap for Matching Estimators.” Econometrica 76 (6): 1537–57. https://doi.org/https://doi.org/10.3982/ECTA6474.\n\n\nAthey, Susan, and Stefan Wager. 2019. “Estimating Treatment Effects with Causal Forests: An Application.”\n\n\nBorusyak, Kirill, and Xavier Jaravel. 2018. “Revisiting Event Study Designs.” https://scholar.harvard.edu/files/borusyak/files/borusyak_jaravel_event_studies.pdf.\n\n\nChernozhukov, V., C. Hansen, N. Kallus, M. Spindler, and V. Syrgkanis. 2024. Applied Causal Inference Powered by ML and AI. https://causalml-book.org/.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. https://theeffectbook.net/.\n\n\nYeager, David S., Paul Hanselman, Gregory M. Walton, Jared S. Murray, Robert Crosnoe, Chandra Muller, Elizabeth Tipton, et al. 2019. “A National Experiment Reveals Where a Growth Mindset Improves Achievement.” Nature 573 (7774): 364–69. https://doi.org/10.1038/s41586-019-1466-y."
  },
  {
    "objectID": "doubleml.html#introduction-1",
    "href": "doubleml.html#introduction-1",
    "title": "Double / Debiased Machine Learning",
    "section": "Introduction",
    "text": "Introduction\n\nMany estimation tasks involve:\n\nLow dimensional parameter of interest\nHigh dimensional nuisance parameters needed to recover parameter of interest\n\nExample: in matching\n\nInterested in \\(ATE = \\Er[Y(1) - Y(0)]\\)\nNuisance parameters \\(\\Er[Y|D,X]\\) and/or \\(P(D=1|X)\\)"
  },
  {
    "objectID": "doubleml.html#machine-learning-for-nuisance-parameters",
    "href": "doubleml.html#machine-learning-for-nuisance-parameters",
    "title": "Double / Debiased Machine Learning",
    "section": "Machine Learning for Nuisance Parameters",
    "text": "Machine Learning for Nuisance Parameters\n\nIn matching we said that we could use flexible machine learning estimators for \\(\\Er[Y|D,X]\\) and \\(P(D=1|X)\\) and plug them into the doubly robust estimator\nWe will cover:\n\nSome of the technical details behind this idea\nHow this idea can be applied to other estimators\n\n\n\nThese notes will examine the incorportion of machine learning methods in classic econometric techniques for estimating causal effects. More specifally, we will focus on estimating treatment effects using matching and instrumental variables. In these estimators (and many others) there is a low-dimensional parameter of interest, such as the average treatment effect, but estimating it requires also estimating a potentially high dimensional nuisance parameter, such as the propensity score. Machine learning methods were developed for prediction with high dimensional data. It is then natural to try to use machine learning for estimating high dimensional nuisance parameters. Care must be taken when doing so though because the flexibility and complexity that make machine learning so good at prediction also pose challenges for inference."
  },
  {
    "objectID": "doubleml.html#example-partially-linear-model",
    "href": "doubleml.html#example-partially-linear-model",
    "title": "Double / Debiased Machine Learning",
    "section": "Example: partially linear model",
    "text": "Example: partially linear model\n\\[\n    y_i = \\theta d_i + f(x_i) + \\epsilon_i\n\\]\n\nInterested in \\(\\theta\\)\nAssume \\(\\Er[\\epsilon|d,x] = 0\\)\nNuisance parameter \\(f()\\)"
  },
  {
    "objectID": "doubleml.html#example-matching",
    "href": "doubleml.html#example-matching",
    "title": "Double / Debiased Machine Learning",
    "section": "Example: Matching",
    "text": "Example: Matching\n\nBinary treatment \\(d_i \\in \\{0,1\\}\\)\nPotential outcomes \\(y_i(0), y_i(1)\\), observe \\(y_i = y_i(d_i)\\)\nInterested in average treatment effect : \\(\\theta = \\Er[y_i(1) -\ny_i(0)]\\)\nCovariates \\(x_i\\)\nAssume unconfoundedness : \\(d_i \\indep y_i(1), y_i(0) | x_i\\)\n\n\nThe partially linear and matching models are closely related. If the conditional mean independence assumption of the partially linear model is strengthing to conditional indepence then the partially linear model is a special case of the matching model with constant treatment effects, \\(y_i(1) - y_i(0) = \\theta\\). Thus the matching model can be viewed as a generalization of the partially linear model that allows for treatment effect heterogeneity."
  },
  {
    "objectID": "doubleml.html#example-matching-1",
    "href": "doubleml.html#example-matching-1",
    "title": "Double / Debiased Machine Learning",
    "section": "Example: Matching",
    "text": "Example: Matching\n\nEstimatable formulae for ATE : \\[\n\\begin{align*}\n\\theta = & \\Er\\left[\\frac{y_i d_i}{\\Pr(d = 1 | x_i)} - \\frac{y_i\n    (1-d_i)}{1-\\Pr(d=1|x_i)} \\right] \\\\\n\\theta = & \\Er\\left[\\Er[y_i | d_i = 1, x_i] - \\Er[y_i | d_i = 0 , x_i]\\right] \\\\\n\\theta = & \\Er\\left[ \\begin{array}{l} d_i \\frac{y_i - \\Er[y_i | d_i = 1,\n    x_i]}{\\Pr(d=1|x_i)} - (1-d_i)\\frac{y_i - \\Er[y_i | d_i = 0,\n    x_i]}{1-\\Pr(d=1|x_i)} + \\\\ + \\Er[y_i | d_i = 1, x_i] - \\Er[y_i | d_i = 0 ,\n    x_i]\\end{array}\\right]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "doubleml.html#example-iv",
    "href": "doubleml.html#example-iv",
    "title": "Double / Debiased Machine Learning",
    "section": "Example: IV",
    "text": "Example: IV\n\\[\n\\begin{align*}\ny_i = & \\theta d_i + f(x_i) + \\epsilon_i \\\\\nd_i = & g(x_i, z_i) + u_i\n\\end{align*}\n\\]\n\nInterested in \\(\\theta\\)\nAssume \\(\\Er[\\epsilon|x,z] = 0\\), \\(\\Er[u|x,z]=0\\)\nNuisance parameters \\(f()\\), \\(g()\\)"
  },
  {
    "objectID": "doubleml.html#example-late",
    "href": "doubleml.html#example-late",
    "title": "Double / Debiased Machine Learning",
    "section": "Example: LATE",
    "text": "Example: LATE\n\nBinary instrumet \\(z_i \\in \\{0,1\\}\\)\nPotential treatments \\(d_i(0), d_i(1) \\in \\{0,1\\}\\), \\(d_i = d_i(Z_i)\\)\nPotential outcomes \\(y_i(0), y_i(1)\\), observe \\(y_i = y_i(d_i)\\)\nCovariates \\(x_i\\)\n\\((y_i(1), y_i(0), d_i(1), d_i(0)) \\indep z_i | x_i\\)\nLocal average treatment effect: \\[\n\\begin{align*}\n\\theta = & \\Er\\left[\\Er[y_i(1) - y_i(0) | x, d_i(1) &gt; d_i(0)]\\right] \\\\\n     = & \\Er\\left[\\frac{\\Er[y|z=1,x] - \\Er[y|z=0,x]}\n                    {\\Er[d|z=1,x]-\\Er[d|z=0,x]} \\right]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "doubleml.html#general-setup",
    "href": "doubleml.html#general-setup",
    "title": "Double / Debiased Machine Learning",
    "section": "General setup",
    "text": "General setup\n\nParameter of interest \\(\\theta \\in \\R^{d_\\theta}\\)\nNuisance parameter \\(\\eta \\in T\\)\nMoment conditions \\[\n\\Er[\\psi(W;\\theta_0,\\eta_0) ] = 0 \\in \\R^{d_\\theta}\n\\] with \\(\\psi\\) known\nEstimate \\(\\hat{\\eta}\\) using some machine learning method\nEstimate \\(\\hat{\\theta}\\) from \\[\n\\En[\\psi(w_i;\\hat{\\theta},\\hat{\\eta}) ] = 0\n\\]\n\n\nWe are following the setup and notation of Chernozhukov et al. (2018). As in the examples, the dimension of \\(\\theta\\) is fixed and small. The dimension of \\(\\eta\\) is large and might be increasing with sample size. \\(T\\) is some normed vector space."
  },
  {
    "objectID": "doubleml.html#example-partially-linear-model-1",
    "href": "doubleml.html#example-partially-linear-model-1",
    "title": "Double / Debiased Machine Learning",
    "section": "Example: partially linear model",
    "text": "Example: partially linear model\n\\[\n    y_i = \\theta_0 d_i + f_0(x_i) + \\epsilon_i\n\\]\n\nCompare the estimates from\n\n\\(\\En[d_i(y_i - \\tilde{\\theta} d_i - \\hat{f}(x_i)) ] = 0\\)\n\nand\n\n\\(\\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) -  \\theta (d_i - \\hat{m}(x_i)))] = 0\\)\n\nwhere \\(m(x) = \\Er[d|x]\\) and \\(\\mu(y) = \\Er[y|x]\\)\n\n\nExample: partially linear model In the partially linear model,\n\\[\n    y_i = \\theta_0 d_i + f_0(x_i) + \\epsilon_i\n\\]\nwe can let \\(w_i = (y_i, x_i)\\) and \\(\\eta = f\\). There are a variety of candidates for \\(\\psi\\). An obvious (but flawed) one is \\(\\psi(w_i; \\theta,\n\\eta) = (y_i - \\theta_0 d_i - f_0(x_i))d_i\\). With this choice of \\(\\psi\\), we have\n\\[\n\\begin{align*}\n0 = & \\En[d_i(y_i - \\hat{\\theta} d_i - \\hat{f}(x_i)) ] \\\\\n\\hat{\\theta} = & \\En[d_i^2]^{-1} \\En[d_i (y_i - \\hat{f}(x_i))] \\\\\n(\\hat{\\theta} - \\theta_0) = &  \\En[d_i^2]^{-1} \\En[d_i \\epsilon_i] +\n    \\En[d_i^2]^{-1} \\En[d_i (f_0(x_i) - \\hat{f}(x_i))]\n\\end{align*}\n\\]\nThe first term of this expression is quite promising. \\(d_i\\) and \\(\\epsilon_i\\) are both finite dimensional random variables, so a law of large numbers will apply to \\(\\En[d_i^2]\\), and a central limit theorem would apply to \\(\\sqrt{n} \\En[d_i \\epsilon_i]\\). Unfortunately, the second expression is problematic. To accomodate high dimensional \\(x\\) and allow for flexible \\(f()\\), machine learning estimators must introduce some sort of regularization to control variance. This regularization also introduces some bias. The bias generally vanishes, but at a slower than \\(\\sqrt{n}\\) rate. Hence\n\\[\n\\sqrt{n} \\En[d_i (f_0(x_i) - \\hat{f}(x_i))] \\to \\infty.\n\\]\nTo get around this problem, we must modify our estimate of \\(\\theta\\). Let \\(m(x) = \\Er[d|x]\\) and \\(\\mu(y) = \\Er[y|x]\\). Let \\(\\hat{m}()\\) and \\(\\hat{\\mu}()\\) be some estimates. Then we can estimate \\(\\theta\\) by partialling out:\n\\[\n\\begin{align*}\n0 = & \\En[(d_i - \\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i) -  \\theta (d_i - \\hat{m}(x_i)))] \\\\\n\\hat{\\theta} = & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\En[(d_i -\n\\hat{m}(x_i))(y_i - \\hat{\\mu}(x_i))] \\\\\n(\\hat{\\theta} - \\theta_0) = & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\left(\\En[(d_i -\n\\hat{m}(x_i))\\epsilon_i] + \\En[(d_i - \\hat{m}(x_i))(\\mu(x_i) -\n\\hat{\\mu}(x_i))] \\right) \\\\\n= & \\En[(d_i -\\hat{m}(x_i))^2]^{-1} \\left( a + b +c + d \\right)\n\\end{align*}\n\\]\nwhere\n\\[\n\\begin{align*}\na = & \\En[(d_i -m(x_i))\\epsilon_i] \\\\\nb = & \\En[(m(x_i)-\\hat{m}(x_i))\\epsilon_i] \\\\\nc = & \\En[v_i(\\mu(x_i) - \\hat{\\mu}(x_i))] \\\\\nd = & \\En[(m(x_i) - \\hat{m}(x_i))(\\mu(x_i) - \\hat{\\mu}(x_i))]\n\\end{align*}\n\\]\nwith \\(v_i = d_i - \\Er[d_i | x_i]\\). The term \\(a\\) is well behaved and \\(\\sqrt{n}a \\leadsto N(0,\\Sigma)\\) under standard conditions. Although terms \\(b\\) and \\(c\\) appear similar to the problematic term in the initial estimator, they are better behaved because \\(\\Er[v|x] = 0\\) and \\(\\Er[\\epsilon|x] = 0\\). This makes it possible, but difficult to show that \\(\\sqrt{n}b \\to_p = 0\\) and \\(\\sqrt{n} c \\to_p = 0\\), see e.g. Belloni, Chernozhukov, and Hansen (2014). However, the conditions on \\(\\hat{m}\\) and \\(\\hat{\\mu}\\) needed to show this are slightly restrictive, and appropriate conditions might not be known for all estimators. Chernozhukov et al. (2018) describe a sample splitting modification to \\(\\hat{\\theta}\\) that allows \\(\\sqrt{n} b\\) and \\(\\sqrt{n} c\\) to vanish under weaker conditions (essentially the same rate condition as needed for \\(\\sqrt{n} d\\) to vanish.)\nThe last term, \\(d\\), is a considerable improvement upon the first estimator. Instead of involving the error in one estimate, it now involes the product of the error in two estimates. By the Cauchy-Schwarz inequality, \\[\nd \\leq \\sqrt{\\En[(m(x_i) - \\hat{m}(x_i))^2]} \\sqrt{\\En[(\\mu(x_i) - \\hat{\\mu}(x_i))^2]}.\n\\] So if the estimates of \\(m\\) and \\(\\mu\\) converge at rates faster than \\(n^{-1/4}\\), then \\(\\sqrt{n} d \\to_p 0\\). This \\(n^{-1/4}\\) rate is reached by many machine learning estimators."
  },
  {
    "objectID": "doubleml.html#lessons-from-the-example",
    "href": "doubleml.html#lessons-from-the-example",
    "title": "Double / Debiased Machine Learning",
    "section": "Lessons from the example",
    "text": "Lessons from the example\n\nNeed an extra condition on moments – Neyman orthogonality \\[\n\\partial \\eta \\Er[\\psi(W;\\theta_0,\\eta_0)](\\eta-\\eta_0) = 0\n\\]\nWant estimators faster than \\(n^{-1/4}\\) in the prediction norm, \\[\n\\sqrt{\\En[(\\hat{\\eta}(x_i) - \\eta(x_i))^2]} \\lesssim_P n^{-1/4}\n\\]\nAlso want estimators that satisfy something like \\[ \\sqrt{n} \\En[(\\eta(x_i)-\\hat{\\eta}(x_i))\\epsilon_i] = o_p(1) \\]\n\nSample splitting / cross-fitting will make this easier"
  },
  {
    "objectID": "doubleml.html#cross-fitting",
    "href": "doubleml.html#cross-fitting",
    "title": "Double / Debiased Machine Learning",
    "section": "Cross-fitting",
    "text": "Cross-fitting\n\nRandomly partition into \\(K\\) subsets \\((I_k)_{k=1}^K\\)\n\\(I^c_k = \\{1, ..., n\\} \\setminus I_k\\)\n\\(\\hat{\\eta}_k =\\) estimate of \\(\\eta\\) using \\(I^c_k\\)\nEstimator: \\[\n\\begin{align*}\n0 = & \\frac{1}{K} \\sum_{k=1}^K \\frac{K}{n} \\sum_{i \\in I_k}\n\\psi(w_i;\\hat{\\theta}^{DML},\\hat{\\eta}_k) \\\\\n0 = & \\frac{1}{K} \\sum_{k=1}^K \\En_k[\n\\psi(w_i;\\hat{\\theta}^{DML},\\hat{\\eta}_k)]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "doubleml.html#assumptions",
    "href": "doubleml.html#assumptions",
    "title": "Double / Debiased Machine Learning",
    "section": "Assumptions",
    "text": "Assumptions\n\nNeyman Orthogonality \\[\n\\partial \\eta \\Er[\\psi(W;\\theta_0,\\eta_0)](\\eta-\\eta_0) \\approx 0\n\\]\nFast enough convergence of \\(\\hat{\\eta}\\) \\[\n\\sqrt{\\Er[(\\hat{\\eta}_k(x_i) - \\eta(x_i))^2|I_k^c]} \\lesssim_P n^{-1/4}\n\\]\nVarious moments exist and other regularity conditions\nMoment condition linear in \\(\\theta\\) (to simplify notation only) \\[\n\\psi(w;\\theta,\\eta) = \\psi^a(w;\\eta) \\theta + \\psi^b(w;\\eta)\n\\]\n\n1\nThese are stated loosely, see Chernozhukov et al. (2018) for precise conditions."
  },
  {
    "objectID": "doubleml.html#asymptotic-normality",
    "href": "doubleml.html#asymptotic-normality",
    "title": "Double / Debiased Machine Learning",
    "section": "Asymptotic Normality",
    "text": "Asymptotic Normality\n\\[\n\\sqrt{n} \\sigma^{-1} (\\hat{\\theta} - \\theta_0) = \\frac{1}{\\sqrt{n}}\n\\sum_{i=1}^n \\bar{\\psi}(w_i) + o_p(1) \\indist N(0,1)\n\\] - Variance \\[\\sigma^2 = \\Er[\\psi^a(w_i;\\eta_0)]^{-1}  \\Er\\left[ \\psi(w;\\theta_0,\\eta_0)\n  \\psi(w;\\theta_0,\\eta_0)'\\right]  \\Er[\\psi^a(w_i;\\eta_0)]^{-1}\n\\] - Influence function \\[\\bar{\\psi}(w) = -\\sigma^{-1} \\Er[\\psi^a(w_i;\\eta_0)]^{-1} \\psi(w;\\theta_0,\\eta_0)\\]"
  },
  {
    "objectID": "doubleml.html#creating-orthogonal-moments-1",
    "href": "doubleml.html#creating-orthogonal-moments-1",
    "title": "Double / Debiased Machine Learning",
    "section": "Creating orthogonal moments",
    "text": "Creating orthogonal moments\n\nNeed \\[\n\\partial \\eta\\Er\\left[\\psi(W;\\theta_0,\\eta_0)[\\eta-\\eta_0] \\right]  \\approx 0\n\\]\nGiven an some model, how do we find a suitable \\(\\psi\\)?\nRelated to finding the efficient influence function, see Oliver Hines and Vansteelandt (2022)"
  },
  {
    "objectID": "doubleml.html#orthogonal-scores-via-concentrating-out",
    "href": "doubleml.html#orthogonal-scores-via-concentrating-out",
    "title": "Double / Debiased Machine Learning",
    "section": "Orthogonal scores via concentrating-out",
    "text": "Orthogonal scores via concentrating-out\n\nOriginal model: \\[\n(\\theta_0, \\beta_0) = \\argmax_{\\theta, \\beta} \\Er[\\ell(W;\\theta,\\beta)]\n\\]\nDefine \\[\n\\eta(\\theta) = \\beta(\\theta) = \\argmax_\\beta \\Er[\\ell(W;\\theta,\\beta)]\n\\]\nFirst order condition from \\(\\max_\\theta\n\\Er[\\ell(W;\\theta,\\beta(\\theta))]\\) is \\[\n0 = \\Er\\left[ \\underbrace{\\frac{\\partial \\ell}{\\partial \\theta} + \\frac{\\partial \\ell}{\\partial \\beta} \\frac{d \\beta}{d \\theta}}_{\\psi(W;\\theta,\\beta(\\theta))} \\right]\n\\]"
  },
  {
    "objectID": "doubleml.html#example-average-derivative",
    "href": "doubleml.html#example-average-derivative",
    "title": "Double / Debiased Machine Learning",
    "section": "Example: average derivative",
    "text": "Example: average derivative\n\n\\(x,y \\in \\R^1\\), \\(\\Er[y|x] = f_0(x)\\), \\(p(x) =\\) density of \\(x\\)\n\\(\\theta_0 = \\Er[f_0'(x)]\\)\nJoint objective \\[\n\\min_{\\theta,f} \\Er\\left[ (y - f(x))^2 + (\\theta - f'(x)^2) \\right]\n\\]\nSolve for minimizing \\(f\\) given \\(\\theta\\) \\[\nf_\\theta(x) = \\Er[y|x] - \\theta \\partial_x \\log p(x) + f''(x) + f'(x) \\partial_x \\log p(x)\n\\]"
  },
  {
    "objectID": "doubleml.html#example-average-derivative-1",
    "href": "doubleml.html#example-average-derivative-1",
    "title": "Double / Debiased Machine Learning",
    "section": "Example: average derivative",
    "text": "Example: average derivative\n\nConcentrated objective: \\[\n\\min_\\theta \\Er\\left[ (y - f_\\theta(x))^2 + (\\theta - f_\\theta'(x)^2)\n\\right]\n\\]\nFirst order condition at \\(f_\\theta = f_0\\) gives \\[\n0 = \\Er\\left[ (y - f_0(x))\\partial_x \\log p(x) + (\\theta - f_0'(x)) \\right]\n\\]"
  },
  {
    "objectID": "doubleml.html#orthogonal-scores-via-two-other-methods",
    "href": "doubleml.html#orthogonal-scores-via-two-other-methods",
    "title": "Double / Debiased Machine Learning",
    "section": "Orthogonal scores via Two Other Methods",
    "text": "Orthogonal scores via Two Other Methods\n\n“Orthogonal” suggests ideas from linear algebra will useful, and they are\nProjection: take orthogonal to \\(\\eta_0\\) projection of moments\nRiesz representer\n\n\nChernozhukov et al. (2018) show how to construct orthogonal scores in a few examples via concentrating out and projection. Chernozhukov, Hansen, and Spindler (2015) also discusses creating orthogonal scores."
  },
  {
    "objectID": "doubleml.html#data",
    "href": "doubleml.html#data",
    "title": "Double / Debiased Machine Learning",
    "section": "Data",
    "text": "Data\n\n\nimports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nimport statsmodels as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.iolib.summary2 import summary_col\n\n\n\ncpsall = pd.read_stata(\"https://www.nber.org/morg/annual/morg20.dta\")\n# take subset of data just to reduce computation time\ncps = cpsall.sample(30000, replace=False, random_state=0)\ncps.describe()\n\n\n\n\n\n\n\n\nhurespli\nhhnum\ncounty\ncentcity\nsmsastat\nicntcity\nsmsa04\nrelref95\nage\nspouse\n...\nrecnum\nyear\nym_file\nym\nch02\nch35\nch613\nch1417\nch05\nihigrdc\n\n\n\n\ncount\n29998.000000\n30000.000000\n30000.000000\n24785.000000\n29699.000000\n3775.000000\n30000.000000\n30000.000000\n30000.000000\n15421.000000\n...\n30000.000000\n30000.0\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n30000.000000\n20929.000000\n\n\nmean\n1.248017\n1.050267\n25.579267\n1.926811\n1.186942\n1.399735\n3.691300\n42.792200\n48.781800\n1.574347\n...\n200364.281250\n2020.0\n725.461367\n716.238567\n0.053967\n0.064967\n0.136967\n0.081267\n0.099833\n12.443547\n\n\nstd\n0.617033\n0.238765\n61.435104\n0.718238\n0.389872\n0.987978\n2.592906\n3.830515\n18.922986\n0.675013\n...\n116372.054688\n0.0\n3.498569\n6.903731\n0.225956\n0.246471\n0.343818\n0.273249\n0.299783\n2.441900\n\n\nmin\n0.000000\n1.000000\n0.000000\n1.000000\n1.000000\n1.000000\n0.000000\n40.000000\n16.000000\n1.000000\n...\n23.000000\n2020.0\n720.000000\n705.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n1.000000\n1.000000\n0.000000\n1.000000\n1.000000\n1.000000\n0.000000\n40.000000\n33.000000\n1.000000\n...\n98957.250000\n2020.0\n722.000000\n710.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n12.000000\n\n\n50%\n1.000000\n1.000000\n0.000000\n2.000000\n1.000000\n1.000000\n4.000000\n41.000000\n49.000000\n2.000000\n...\n200032.500000\n2020.0\n725.000000\n716.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n12.000000\n\n\n75%\n1.000000\n1.000000\n27.000000\n2.000000\n1.000000\n1.000000\n6.000000\n42.000000\n64.000000\n2.000000\n...\n302111.750000\n2020.0\n729.000000\n722.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n14.000000\n\n\nmax\n13.000000\n4.000000\n810.000000\n3.000000\n2.000000\n7.000000\n7.000000\n59.000000\n85.000000\n9.000000\n...\n401132.000000\n2020.0\n731.000000\n728.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n18.000000\n\n\n\n\n8 rows × 55 columns"
  },
  {
    "objectID": "doubleml.html#partial-linear-model",
    "href": "doubleml.html#partial-linear-model",
    "title": "Double / Debiased Machine Learning",
    "section": "Partial Linear Model",
    "text": "Partial Linear Model\n\ndef partial_linear(y, d, X, yestimator, destimator, folds=3):\n    \"\"\"Estimate the partially linear model y = d*C + f(x) + e\n\n    Parameters\n    ----------\n    y : array_like\n        vector of outcomes\n    d : array_like\n        vector or matrix of regressors of interest\n    X : array_like\n        matrix of controls\n    mlestimate : Estimator object for partialling out X. Must have ‘fit’\n        and ‘predict’ methods.\n    folds : int\n        Number of folds for cross-fitting\n\n    Returns\n    -------\n    ols : statsmodels regression results containing estimate of coefficient on d.\n    yhat : cross-fitted predictions of y\n    dhat : cross-fitted predictions of d\n    \"\"\"\n\n    # we want predicted probabilities if y or d is discrete\n    ymethod = \"predict\" if False==getattr(yestimator, \"predict_proba\",False) else \"predict_proba\"\n    dmethod = \"predict\" if False==getattr(destimator, \"predict_proba\",False) else \"predict_proba\"\n    # get the predictions\n    yhat = cross_val_predict(yestimator,X,y,cv=folds,method=ymethod)\n    dhat = cross_val_predict(destimator,X,d,cv=folds,method=dmethod)\n    ey = np.array(y - yhat)\n    ed = np.array(d - dhat)\n    ols = sm.regression.linear_model.OLS(ey,ed).fit(cov_type='HC0')\n\n    return(ols, yhat, dhat)"
  },
  {
    "objectID": "doubleml.html#unconditional-gap",
    "href": "doubleml.html#unconditional-gap",
    "title": "Double / Debiased Machine Learning",
    "section": "Unconditional Gap",
    "text": "Unconditional Gap\n\n\nCode\ncps[\"female\"] = (cps.sex==2)\ncps[\"log_earn\"] = np.log(cps.earnwke)\ncps.loc[np.isinf(cps.log_earn), \"log_earn\"] = np.nan\ncps[\"log_uhours\"] = np.log(cps.uhourse)\ncps.loc[np.isinf(cps.log_uhours), \"log_uhours\"] = np.nan\ncps[\"log_hourslw\"] = np.log(cps.hourslw)\ncps.loc[np.isinf(cps.log_hourslw),\"log_hourslw\"] = np.nan\ncps[\"log_wageu\"] = cps.log_earn - cps.log_uhours\ncps[\"log_wagelw\"] = cps.log_earn - cps.log_hourslw\n\nlm = list()\nlm.append(smf.ols(formula=\"log_earn ~ female\", data=cps,\n                  missing=\"drop\").fit(cov_type='HC0'))\nlm.append( smf.ols(formula=\"log_wageu ~ female\", data=cps,\n                   missing=\"drop\").fit(cov_type='HC0'))\nlm.append(smf.ols(formula=\"log_wagelw ~ female\", data=cps,\n                  missing=\"drop\").fit(cov_type='HC0'))\nlm.append(smf.ols(formula=\"log_earn ~ female + log_hourslw + log_uhours\", data=cps,\n                  missing=\"drop\").fit(cov_type='HC0'))\nsummary_col(lm, stars=True)\n\n\n\n\n\n\nlog_earn I\nlog_wageu I\nlog_wagelw I\nlog_earn II\n\n\nIntercept\n6.8607***\n3.2022***\n3.2366***\n2.0104***\n\n\n\n(0.0091)\n(0.0080)\n(0.0088)\n(0.0945)\n\n\nfemale[T.True]\n-0.2965***\n-0.1729***\n-0.1746***\n-0.1363***\n\n\n\n(0.0133)\n(0.0112)\n(0.0124)\n(0.0115)\n\n\nlog_hourslw\n\n\n\n0.0227\n\n\n\n\n\n\n(0.0223)\n\n\nlog_uhours\n\n\n\n1.3025***\n\n\n\n\n\n\n(0.0372)\n\n\nR-squared\n0.0323\n0.0166\n0.0136\n0.3367\n\n\nR-squared Adj.\n0.0323\n0.0165\n0.0135\n0.3366\n\n\n\n\nStandard errors in parentheses.\n* p&lt;.1, ** p&lt;.05, ***p&lt;.01"
  },
  {
    "objectID": "doubleml.html#adding-controls",
    "href": "doubleml.html#adding-controls",
    "title": "Double / Debiased Machine Learning",
    "section": "Adding Controls",
    "text": "Adding Controls\n\nfrom patsy import dmatrices\nfmla  = \"log_earn + female ~ log_uhours + log_hourslw + I(age**2) + age + C(grade92) + C(race) + C(smsastat) + C(unionmme) + C(unioncov)\" #C(ind02) + C(occ2012)\"\nyd, X = dmatrices(fmla,cps)\nfemale = yd[:,1]\nlogearn = yd[:,2];"
  },
  {
    "objectID": "doubleml.html#estimating-eta",
    "href": "doubleml.html#estimating-eta",
    "title": "Double / Debiased Machine Learning",
    "section": "Estimating \\(\\eta\\)",
    "text": "Estimating \\(\\eta\\)\n\nalphas = np.exp(np.linspace(-2, -12, 20))\nlassoy = linear_model.LassoCV(cv=4, alphas=alphas, max_iter=5000).fit(X,logearn)\nlassod = linear_model.LassoCV(cv=4, alphas=alphas, max_iter=5000).fit(X,female);"
  },
  {
    "objectID": "doubleml.html#estimating-eta-1",
    "href": "doubleml.html#estimating-eta-1",
    "title": "Double / Debiased Machine Learning",
    "section": "Estimating \\(\\eta\\)",
    "text": "Estimating \\(\\eta\\)\n\n\nCode\nfig, ax = plt.subplots(1,2)\n\ndef plotlassocv(l, ax) :\n    alphas = l.alphas_\n    mse = l.mse_path_.mean(axis=1)\n    std_error = l.mse_path_.std(axis=1)\n    ax.plot(alphas,mse)\n    ax.fill_between(alphas, mse + std_error, mse - std_error, alpha=0.2)\n\n    ax.set_ylabel('MSE +/- std error')\n    ax.set_xlabel('alpha')\n    ax.set_xlim([alphas[0], alphas[-1]])\n    ax.set_xscale(\"log\")\n    return(ax)\n\nax[0] = plotlassocv(lassoy,ax[0])\nax[0].set_title(\"MSE for log(earn)\")\nax[1] = plotlassocv(lassod,ax[1])\nax[1].set_title(\"MSE for female\")\nfig.tight_layout()\nfig.show()"
  },
  {
    "objectID": "doubleml.html#estimating-eta-2",
    "href": "doubleml.html#estimating-eta-2",
    "title": "Double / Debiased Machine Learning",
    "section": "Estimating \\(\\eta\\)",
    "text": "Estimating \\(\\eta\\)\n\ndef pickalpha(lassocv) :\n    #imin = np.argmin(lassocv.mse_path_.mean(axis=1))\n    #msemin = lassocv.mse_path_.mean(axis=1)[imin]\n    #se = lassocv.mse_path_.std(axis=1)[imin]\n    #alpha= min([alpha for (alpha, mse) in zip(lassocv.alphas_, lassocv.mse_path_.mean(axis=1)) if mse&lt;msemin+se])\n    alpha = lassocv.alpha_\n    return(alpha)\n\nalphay = pickalpha(lassoy)\nalphad = pickalpha(lassod)"
  },
  {
    "objectID": "doubleml.html#estimate-theta",
    "href": "doubleml.html#estimate-theta",
    "title": "Double / Debiased Machine Learning",
    "section": "Estimate \\(\\theta\\)",
    "text": "Estimate \\(\\theta\\)\n\npl_lasso = partial_linear(logearn, female, X,\n                          linear_model.Lasso(alpha=alphay),\n                          linear_model.Lasso(alpha=alphad))\npl_lasso[0].summary(slim=True)\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nF-statistic:\n\n\nModel:\nOLS\nProb (F-statistic):\n\n\nNo. Observations:\n12038\n\n\n\nCovariance Type:\nHC0\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nx1\n-0.1868\n0.011\n-16.643\n0.000\n-0.209\n-0.165\n\n\n\nNotes:[1] R² is computed without centering (uncentered) since the model does not contain a constant.[2] Standard Errors are heteroscedasticity robust (HC0)"
  },
  {
    "objectID": "doubleml.html#examining-predictions",
    "href": "doubleml.html#examining-predictions",
    "title": "Double / Debiased Machine Learning",
    "section": "Examining Predictions",
    "text": "Examining Predictions\n\n\nCode\nimport seaborn as sns\n# Visualize predictions\ndef preddf(pl):\n    df = pd.DataFrame({\"logearn\":logearn,\n                       \"predicted\":pl[1],\n                       \"female\":female,\n                       \"P(female|x)\":pl[2]})\n    return(df)\n\ndef plotpredictions(df) :\n    fig, ax = plt.subplots(2,1)\n    plt.figure()\n    sns.scatterplot(x = df.predicted, y = df.logearn-df.predicted, hue=df.female, ax=ax[0])\n    ax[0].set_title(\"Prediction Errors for Log Earnings\")\n\n    sns.histplot(df[\"P(female|x)\"][df.female==0], kde = False,\n                 label = \"Male\", ax=ax[1])\n    sns.histplot(df[\"P(female|x)\"][df.female==1], kde = False,\n                 label = \"Female\", ax=ax[1])\n    ax[1].set_title('P(female|x)')\n    return(fig)\n\nfig=plotpredictions(preddf(pl_lasso))\nfig.show()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 960x480 with 0 Axes&gt;"
  },
  {
    "objectID": "doubleml.html#software",
    "href": "doubleml.html#software",
    "title": "Double / Debiased Machine Learning",
    "section": "Software",
    "text": "Software\n\ndoubleml\neconml"
  },
  {
    "objectID": "doubleml.html#using-doubleml",
    "href": "doubleml.html#using-doubleml",
    "title": "Double / Debiased Machine Learning",
    "section": "Using doubleml",
    "text": "Using doubleml\n\n\nCode\nimport doubleml\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LassoCV, LogisticRegressionCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\ndml_data = doubleml.DoubleMLData.from_arrays(X,logearn,female)\n# Initialize learners\nCs = 0.0001*np.logspace(0, 4, 10)\nlasso = make_pipeline(StandardScaler(), LassoCV(cv=5, max_iter=10000))\nlasso_class = make_pipeline(StandardScaler(),\n                            LogisticRegressionCV(cv=5, penalty='l1', solver='liblinear',\n                                                 Cs = Cs, max_iter=1000))\n\nnp.random.seed(123)\ndml_plr_lasso = doubleml.DoubleMLPLR(dml_data,\n                                     ml_l = lasso,\n                                     ml_m = lasso_class,\n                                     n_folds = 3)\ndml_plr_lasso.fit(store_predictions=True)\nprint(dml_plr_lasso.summary)\n\n\n       coef   std err          t         P&gt;|t|     2.5 %   97.5 %\nd -0.181669  0.011178 -16.252139  2.157063e-59 -0.203578 -0.15976"
  },
  {
    "objectID": "doubleml.html#visualizing-predictions-lasso-logistic-lasso",
    "href": "doubleml.html#visualizing-predictions-lasso-logistic-lasso",
    "title": "Double / Debiased Machine Learning",
    "section": "Visualizing Predictions: Lasso & Logistic Lasso",
    "text": "Visualizing Predictions: Lasso & Logistic Lasso\n\n\nCode\ndef dmlpreddf(dml_model):\n    df=pd.DataFrame({\"logearn\":logearn,\n                     \"predicted\":dml_model.predictions['ml_l'].flatten(),\n                     \"female\":female,\n                     \"P(female|x)\":dml_model.predictions['ml_m'].flatten()})\n    return(df)\n\nplotpredictions(dmlpreddf(dml_plr_lasso)).show()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 960x480 with 0 Axes&gt;"
  },
  {
    "objectID": "doubleml.html#with-gradient-boosted-trees",
    "href": "doubleml.html#with-gradient-boosted-trees",
    "title": "Double / Debiased Machine Learning",
    "section": "With Gradient Boosted Trees",
    "text": "With Gradient Boosted Trees\n\n\nCode\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nylearner = LGBMRegressor(force_col_wise=True)\ndlearner = LGBMClassifier(force_col_wise=True)\ndml_plr_gbt = doubleml.DoubleMLPLR(dml_data, ylearner, dlearner)\ndml_plr_gbt.fit(store_predictions=True)\nprint(dml_plr_gbt.summary)\n\n\n[LightGBM] [Info] Total Bins 292\n[LightGBM] [Info] Number of data points in the train set: 9630, number of used features: 28\n[LightGBM] [Info] Start training from score 6.719951\n[LightGBM] [Info] Total Bins 290\n[LightGBM] [Info] Number of data points in the train set: 9630, number of used features: 28\n[LightGBM] [Info] Start training from score 6.715158\n[LightGBM] [Info] Total Bins 292\n[LightGBM] [Info] Number of data points in the train set: 9630, number of used features: 28\n[LightGBM] [Info] Start training from score 6.719371\n[LightGBM] [Info] Total Bins 289\n[LightGBM] [Info] Number of data points in the train set: 9631, number of used features: 28\n[LightGBM] [Info] Start training from score 6.719395\n[LightGBM] [Info] Total Bins 289\n[LightGBM] [Info] Number of data points in the train set: 9631, number of used features: 28\n[LightGBM] [Info] Start training from score 6.714159\n[LightGBM] [Info] Number of positive: 4711, number of negative: 4919\n[LightGBM] [Info] Total Bins 292\n[LightGBM] [Info] Number of data points in the train set: 9630, number of used features: 28\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.489200 -&gt; initscore=-0.043205\n[LightGBM] [Info] Start training from score -0.043205\n[LightGBM] [Info] Number of positive: 4647, number of negative: 4983\n[LightGBM] [Info] Total Bins 290\n[LightGBM] [Info] Number of data points in the train set: 9630, number of used features: 28\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482555 -&gt; initscore=-0.069810\n[LightGBM] [Info] Start training from score -0.069810\n[LightGBM] [Info] Number of positive: 4701, number of negative: 4929\n[LightGBM] [Info] Total Bins 292\n[LightGBM] [Info] Number of data points in the train set: 9630, number of used features: 28\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488162 -&gt; initscore=-0.047361\n[LightGBM] [Info] Start training from score -0.047361\n[LightGBM] [Info] Number of positive: 4678, number of negative: 4953\n[LightGBM] [Info] Total Bins 289\n[LightGBM] [Info] Number of data points in the train set: 9631, number of used features: 28\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485723 -&gt; initscore=-0.057123\n[LightGBM] [Info] Start training from score -0.057123\n[LightGBM] [Info] Number of positive: 4703, number of negative: 4928\n[LightGBM] [Info] Total Bins 289\n[LightGBM] [Info] Number of data points in the train set: 9631, number of used features: 28\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488319 -&gt; initscore=-0.046733\n[LightGBM] [Info] Start training from score -0.046733\n       coef   std err          t         P&gt;|t|     2.5 %    97.5 %\nd -0.163842  0.011051 -14.826095  9.934713e-50 -0.185501 -0.142182"
  },
  {
    "objectID": "doubleml.html#visualizing-predictions-trees",
    "href": "doubleml.html#visualizing-predictions-trees",
    "title": "Double / Debiased Machine Learning",
    "section": "Visualizing Predictions: Trees",
    "text": "Visualizing Predictions: Trees\n\n\nCode\nplotpredictions(dmlpreddf(dml_plr_gbt)).show()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 960x480 with 0 Axes&gt;"
  },
  {
    "objectID": "doubleml.html#interactive-regression-model",
    "href": "doubleml.html#interactive-regression-model",
    "title": "Double / Debiased Machine Learning",
    "section": "Interactive Regression Model",
    "text": "Interactive Regression Model\n\nSimilar to matching, the partially linear regression model can suffer from misspecification bias if the effect of \\(D\\) varies with \\(X\\)\nInteractive regression model: \\[\n\\begin{align*}\nY & = g_0(D,X) + U \\\\\nD & = m_0(X) + V\n\\end{align*}\n\\]\nMechanics same as matching heterogeneous effects\nOrthogonal moment condition is same as doubly robust matching"
  },
  {
    "objectID": "doubleml.html#interactive-regression-model---lasso",
    "href": "doubleml.html#interactive-regression-model---lasso",
    "title": "Double / Debiased Machine Learning",
    "section": "Interactive Regression Model - Lasso",
    "text": "Interactive Regression Model - Lasso\n\n\nCode\nnp.random.seed(123)\ndml_irm_lasso = doubleml.DoubleMLIRM(dml_data,\n                                     ml_g = lasso,\n                                     ml_m = lasso_class,\n                                     trimming_threshold = 0.01,\n                                     n_folds = 3)\ndml_irm_lasso.fit()\ndml_irm_lasso.summary\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n2.5 %\n97.5 %\n\n\n\n\nd\n-0.229206\n0.03706\n-6.184647\n6.224143e-10\n-0.301843\n-0.156569"
  },
  {
    "objectID": "doubleml.html#interactive-regression-model---trees",
    "href": "doubleml.html#interactive-regression-model---trees",
    "title": "Double / Debiased Machine Learning",
    "section": "Interactive Regression Model - Trees",
    "text": "Interactive Regression Model - Trees\n\n\nCode\nnp.random.seed(123)\ndml_irm_gbt = doubleml.DoubleMLIRM(dml_data,\n                                     ml_g = ylearner,\n                                     ml_m = dlearner,\n                                     trimming_threshold = 0.01,\n                                     n_folds = 3)\ndml_irm_gbt.fit()\ndml_irm_gbt.summary\n\n\n[LightGBM] [Info] Total Bins 257\n[LightGBM] [Info] Number of data points in the train set: 4119, number of used features: 25\n[LightGBM] [Info] Start training from score 6.868347\n[LightGBM] [Info] Total Bins 252\n[LightGBM] [Info] Number of data points in the train set: 4118, number of used features: 23\n[LightGBM] [Info] Start training from score 6.859271\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 4119, number of used features: 24\n[LightGBM] [Info] Start training from score 6.857057\n[LightGBM] [Info] Total Bins 254\n[LightGBM] [Info] Number of data points in the train set: 3906, number of used features: 22\n[LightGBM] [Info] Start training from score 6.576155\n[LightGBM] [Info] Total Bins 263\n[LightGBM] [Info] Number of data points in the train set: 3907, number of used features: 24\n[LightGBM] [Info] Start training from score 6.567823\n[LightGBM] [Info] Total Bins 253\n[LightGBM] [Info] Number of data points in the train set: 3907, number of used features: 21\n[LightGBM] [Info] Start training from score 6.553555\n[LightGBM] [Info] Number of positive: 3906, number of negative: 4119\n[LightGBM] [Info] Total Bins 286\n[LightGBM] [Info] Number of data points in the train set: 8025, number of used features: 28\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486729 -&gt; initscore=-0.053097\n[LightGBM] [Info] Start training from score -0.053097\n[LightGBM] [Info] Number of positive: 3907, number of negative: 4118\n[LightGBM] [Info] Total Bins 286\n[LightGBM] [Info] Number of data points in the train set: 8025, number of used features: 28\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486854 -&gt; initscore=-0.052598\n[LightGBM] [Info] Start training from score -0.052598\n[LightGBM] [Info] Number of positive: 3907, number of negative: 4119\n[LightGBM] [Info] Total Bins 288\n[LightGBM] [Info] Number of data points in the train set: 8026, number of used features: 28\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486793 -&gt; initscore=-0.052841\n[LightGBM] [Info] Start training from score -0.052841\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n2.5 %\n97.5 %\n\n\n\n\nd\n-0.159562\n0.013846\n-11.524304\n9.951303e-31\n-0.1867\n-0.132425"
  },
  {
    "objectID": "doubleml.html#sources-and-further-reading",
    "href": "doubleml.html#sources-and-further-reading",
    "title": "Double / Debiased Machine Learning",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nThese slides borrow heavily from my notes on machine learning in economics\nThe example is from my chapter on machine learning in QuantEcon: DataScience\nChernozhukov et al. (2017) : short introduction to main idea\nChernozhukov et al. (2018) : underlying theory\nKnaus (2022) : approachable review of DML for doubly robust matching"
  },
  {
    "objectID": "doubleml.html#references",
    "href": "doubleml.html#references",
    "title": "Double / Debiased Machine Learning",
    "section": "References",
    "text": "References\n\n\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014. “Inference on Treatment Effects After Selection Among High-Dimensional Controls†.” The Review of Economic Studies 81 (2): 608–50. https://doi.org/10.1093/restud/rdt044.\n\n\nChernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, and Whitney Newey. 2017. “Double/Debiased/Neyman Machine Learning of Treatment Effects.” American Economic Review 107 (5): 261–65. https://doi.org/10.1257/aer.p20171038.\n\n\nChernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. “Double/Debiased Machine Learning for Treatment and Structural Parameters.” The Econometrics Journal 21 (1): C1–68. https://doi.org/10.1111/ectj.12097.\n\n\nChernozhukov, Victor, Christian Hansen, and Martin Spindler. 2015. “Valid Post-Selection and Post-Regularization Inference: An Elementary, General Approach.” Annual Review of Economics 7 (1): 649–88. https://doi.org/10.1146/annurev-economics-012315-015826.\n\n\nKnaus, Michael C. 2022. “Double machine learning-based programme evaluation under unconfoundedness.” The Econometrics Journal 25 (3): 602–27. https://doi.org/10.1093/ectj/utac015.\n\n\nOliver Hines, Karla Diaz-Ordaz, Oliver Dukes, and Stijn Vansteelandt. 2022. “Demystifying Statistical Learning Based on Efficient Influence Functions.” The American Statistician 76 (3): 292–304. https://doi.org/10.1080/00031305.2021.2021984."
  },
  {
    "objectID": "causality_intro.html#summary",
    "href": "causality_intro.html#summary",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "Summary",
    "text": "Summary\n\nPotential outcomes, treatment effects\nRandomized experiments"
  },
  {
    "objectID": "causality_intro.html#treatment",
    "href": "causality_intro.html#treatment",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "Treatment",
    "text": "Treatment\n\n\n\\(T_i \\in \\{0,1\\}\\)\nObserved outcome \\(Y_i\\)\nWe want the causal effect of \\(T\\) on \\(Y\\), but what does that mean?\n\nPotential outcomes give a rigorous definition"
  },
  {
    "objectID": "causality_intro.html#potential-outcomes",
    "href": "causality_intro.html#potential-outcomes",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "Potential Outcomes",
    "text": "Potential Outcomes\n\nPotential outcomes \\(Y_i(0), Y_i(1)\\) if \\(T_i = 0\\) or \\(1\\)\nObserve \\(Y_i = Y_i(T_i)\\)\n\n\n\nAssume: No Interference: \\((Y_i(0), Y_i(1))\\) is unaffected by \\(T_j\\) for \\(j \\neq i\\)\n\naka Stable Unit-Treatment Value Assumption (SUTVA)\n\nTreatment effect on \\(i\\) = \\(Y_i(1) - Y_i(0)\\)"
  },
  {
    "objectID": "causality_intro.html#fundamental-problem-of-causal-inference",
    "href": "causality_intro.html#fundamental-problem-of-causal-inference",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\n\n\nOnly observe \\(Y_i(1)\\) or \\(Y_i(0)\\), never both\nIndividual effects, \\(Y_i(1) - Y_i(0)\\), generally impossible to recover\nSummaries of individual effects, e.g. \\(\\Er[Y_i(1) - Y_i(0)]\\), possible to estimate, but require assumptions"
  },
  {
    "objectID": "causality_intro.html#average-treatment-effect",
    "href": "causality_intro.html#average-treatment-effect",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "Average Treatment Effect",
    "text": "Average Treatment Effect\n\n\nWant the average treatment effect \\[\nATE = \\Er[Y_i(1) - Y_i(0)]\n\\]\nCan’t estimate \\(\\Er[Y_i(d)]\\), because \\(Y_i(d)\\) not always observed"
  },
  {
    "objectID": "causality_intro.html#average-population-effect-and-selection-bias",
    "href": "causality_intro.html#average-population-effect-and-selection-bias",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "Average Population Effect and Selection Bias",
    "text": "Average Population Effect and Selection Bias\n\n\nCan estimate \\(\\Er[Y_i(d)|T_i=d]\\)\nAverage population effect \\[\nAPE = \\Er[Y_i(1)|T_i=1] - \\Er[Y_i(0)|T_i=0]\n\\]\nHow does it compare to the ATE? \\[ %\n\\begin{align*}\nATE = & \\Er[Y_i(1) - Y_i(0)] \\\\\n= & \\overbrace{\\Er[Y_i(1) - Y_i(0) | T_i=1]}^{\\text{avg treatment effect  on treated}} P(T_i=1) + \\overbrace{\\Er[Y_i(1) - Y_i(0) | T_i=0]}^{\\text{avg treatment effect on untreated}} P(T_i=0) \\\\\n= & \\left(\\Er[Y_i(1) | T_i=1] - \\Er[Y_i(0)|T_i=0] + \\overbrace{\\Er[Y_i(0)|T_i=0] - \\Er[Y_i(0)|T_i=1]}^{\\text{selection bias}}\\right)\n  P(T_i=1) + \\\\\n& +  \\left(\\Er[Y_i(1) | T_i=1] - \\Er[Y_i(0)|T_i=0] + \\underbrace{\\Er[Y_i(1)|T_i=0] - \\Er[Y_i(1)|T_i=1]}_{\\text{selection bias}}\\right)\nP(T_i=0)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "causality_intro.html#sources-and-further-reading",
    "href": "causality_intro.html#sources-and-further-reading",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nChapter 2 of Chernozhukov et al. (2024) is the basis for much of these slides, inlcuding the Pfizer/BioNTech Covid Vaccine RCT example\nChapter 1 of Facure (2022)"
  },
  {
    "objectID": "causality_intro.html#average-population-effect",
    "href": "causality_intro.html#average-population-effect",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "Average Population Effect",
    "text": "Average Population Effect\n\nCan estimate \\(\\Er[Y_i(d)|T_i=d]\\)\nAverage population effect \\[\nAPE = \\Er[Y_i(1)|T_i=1] - \\Er[Y_i(0)|T_i=0]\n\\]\nHow does it compare to the ATE?"
  },
  {
    "objectID": "causality_intro.html#selection-bias",
    "href": "causality_intro.html#selection-bias",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "Selection Bias",
    "text": "Selection Bias\n\nComparing ATE and APE \\[ %\n\\begin{align*}\nATE = & \\Er[Y_i(1) - Y_i(0)] \\\\\n= & \\overbrace{\\Er[Y_i(1) - Y_i(0) | T_i=1]}^{\\text{avg treatment effect  on treated}} P(T_i=1) + \\overbrace{\\Er[Y_i(1) - Y_i(0) | T_i=0]}^{\\text{avg treatment effect on untreated}} P(T_i=0) \\\\\n= & \\left(APE + \\overbrace{\\Er[Y_i(0)|T_i=0] - \\Er[Y_i(0)|T_i=1]}^{\\text{selection bias}}\\right)\n  P(T_i=1) + \\\\\n& +  \\left(APE + \\underbrace{\\Er[Y_i(1)|T_i=0] - \\Er[Y_i(1)|T_i=1]}_{\\text{selection bias}}\\right)\nP(T_i=0)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "causality_intro.html#selection-bias-1",
    "href": "causality_intro.html#selection-bias-1",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "Selection Bias",
    "text": "Selection Bias\n\nOr, \\[ %\nAPE = ATE + \\underbrace{\\begin{pmatrix} (\\Er[Y_i(0) | T_i=1] - \\Er[Y_i(0)|T_i=0])P(T_i=1) + \\\\\n  + (\\Er[Y_i(1) | T_i=1] - \\Er[Y_i(1)|T_i=0])P(T_i=0)\n\\end{pmatrix}}_{\\text{selection bias}}\n\\]\nSelection bias is nonzero if the treated and untreated groups would be different even if everyone had been treated or untreated\nSelection bias usually nonzero if people select their own treatment"
  },
  {
    "objectID": "causality_intro.html#references",
    "href": "causality_intro.html#references",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "References",
    "text": "References\n\n\n\n\nChernozhukov, V., C. Hansen, N. Kallus, M. Spindler, and V. Syrgkanis. 2024. Applied Causal Inference Powered by ML and AI. https://causalml-book.org/.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html."
  },
  {
    "objectID": "causality_intro.html#selection-bias-example",
    "href": "causality_intro.html#selection-bias-example",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "Selection Bias Example",
    "text": "Selection Bias Example\n\nPeople have some (possibly noisy) information about \\(Y_i(0), Y_i(1)\\) and choose \\(T_i\\) they prefer\n\ne.g. \\(T_i = \\arg\\max_{d\\in \\{0,1\\}} \\Er[U(Y_i(d)) | \\mathcal{I}_i]\\)\n\nSimulation\n\n\\(i\\) observes signal \\(S_i(0) = Y_i(0) + \\epsilon_i(0)\\) and \\(S_i(1) = Y_i(1) + \\epsilon_i(1)\\)\n\\(\\epsilon_i(d) \\sim N(0,\\sigma^2)\\), independent\nChooses \\(\\max_d \\Er[Y_i(d)|S_i(0), S_i(1)] = \\max_d S_i(d)\\)\n\n\n\nCode\nimport numpy as np\nnp.random.seed(0)\nclass selectiondata:\n    def __init__(self, n=1000, noisesd=1.0, ate=0.5):\n        self.Y0 = np.random.normal(size=n)\n        self.Y1 = np.random.normal(size=n) + ate\n        self.S0 = self.Y0 + np.random.normal(size=n)*noisesd\n        self.S1 = self.Y1 + np.random.normal(size=n)*noisesd\n        self.T = (self.S1 &gt; self.S0).astype(int)\n        self.Y = self.Y0 * (1 - self.T) + self.Y1 * self.T\n\n    def APE(self):\n        return np.mean(self.Y[self.T==1]) - np.mean(self.Y[self.T==0])\n\n    def ATE(self):\n        return np.mean(self.Y1) - np.mean(self.Y0)\n\n    def selectionbias(self):\n        return (self.APE() - self.ATE())\n\n    def selectionbias0(self):\n        return np.mean( self.Y0[self.T==1]) - np.mean( self.Y0[self.T==0] )\n\n    def selectionbias1(self):\n        return np.mean( self.Y1[self.T==1]) - np.mean(self.Y1[self.T==0] )\n\n\ns = 0.5\neate = 0.5\ndata = selectiondata(n=10_000,noisesd=s, ate=eate)\n\nprint(\"|APE|ATE|Selection Bias|\\n\" +\n      \"|---|---|---|\\n\" +\n      f\"|{data.APE():.2}|{data.ATE():.2}|{data.selectionbias():.2}|\\n\"\n      f\"|σ={s:.2}|\\n\\n\")\n\n\n\n\nAPE\nATE\nSelection Bias\n\n\n\n\n0.27\n0.53\n-0.26\n\n\nσ=0.5"
  },
  {
    "objectID": "causality_intro.html#random-experiment",
    "href": "causality_intro.html#random-experiment",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "Random Experiment",
    "text": "Random Experiment\n\nAssign treatment randomly \\[\nT_i \\indep (Y_i(0),Y_i(1))\n\\]\nImplies \\[\n\\Er[Y_i(1) | T_i=1] = \\Er[Y_i(1)] \\text{ and } \\Er[Y_i(0) | T_i=0] = \\Er[Y_i(0)]\n\\]\nSo \\[\n\\begin{align*}\nAPE = & \\Er[Y_i(1)|T_i=1] - \\Er[Y_i(0)|T_i=0] \\\\\n  = & \\Er[Y_i(1)] - \\Er[Y_i(0)] \\\\\n  = & ATE\n\\end{align*}\n\\]"
  },
  {
    "objectID": "causality_intro.html#example-pfizer-covid-vaccine-rct",
    "href": "causality_intro.html#example-pfizer-covid-vaccine-rct",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "Example: Pfizer Covid Vaccine RCT",
    "text": "Example: Pfizer Covid Vaccine RCT\n\nNumber of participants and number infected by treatment status\n\n\n\n\nGroup\nTreated\nPlacebo\n\n\n\n\nAll\n19965\n20172\n\n\nInfected\n9\n169\n\n\n65+\n4044\n4067\n\n\n65+ Infected\n1\n19"
  },
  {
    "objectID": "causality_intro.html#example-pfizer-covid-vaccine-rct-1",
    "href": "causality_intro.html#example-pfizer-covid-vaccine-rct-1",
    "title": "Introduction to Causality and Potential Outcomes",
    "section": "Example Pfizer Covid Vaccine RCT",
    "text": "Example Pfizer Covid Vaccine RCT\n\nimport statsmodels.api as sm\n\nclass binarybinaryrct :\n    def __init__(self, NT, NU, NYT, NYU):\n        self.NT=NT\n        self.NU=NU\n        self.NYT=NYT\n        self.NYU=NYU\n\n    def ATE(self):\n        return (self.NYT/self.NT - self.NYU/self.NU)\n\n    def table(self):\n        return(\"|  | Infection Rate per 1000|\\n\"+\n               \"|---|---|\\n\"\n               f\"|Treated| {self.NYT/self.NT*1000:.2}|\\n\" +\n               f\"|Control| {self.NYU/self.NU*1000:.2}|\\n\" +\n               f\"|Difference| {self.ATE()*1000:.2}|\\n\")\n    def VE(self):\n        tb = sm.stats.Table2x2([[self.NYT, self.NT - self.NYT], [self.NYU, self.NU - self.NYU]])\n        ve=1-tb.riskratio\n        ci = tb.riskratio_confint()\n        ci = [1-ci[1],1-ci[0]]\n        return(ve,ci)\n\npfizerall = binarybinaryrct(19965, 20172, 9, 169)\npfizer65 = binarybinaryrct(4044, 4067, 1, 19)\n\nprint(\"\\n- All\\n\\n\" + pfizerall.table() + \"\\n - 65+\\n\\n\" + pfizer65.table())\n\n\n- All\n\n|  | Infection Rate per 1000|\n|---|---|\n|Treated| 0.45|\n|Control| 8.4|\n|Difference| -7.9|\n\n - 65+\n\n|  | Infection Rate per 1000|\n|---|---|\n|Treated| 0.25|\n|Control| 4.7|\n|Difference| -4.4|"
  },
  {
    "objectID": "uncertainty.html#summary",
    "href": "uncertainty.html#summary",
    "title": "Uncertainty Quantification",
    "section": "Summary",
    "text": "Summary\n\nStandard errors\nConfidence intervals\nStandard errors for functions of estimates\n\nDelta method\nSimulation\n\nBootstrap"
  },
  {
    "objectID": "uncertainty.html#example-pfizer-covid-vaccine-rct",
    "href": "uncertainty.html#example-pfizer-covid-vaccine-rct",
    "title": "Uncertainty Quantification",
    "section": "Example: Pfizer Covid Vaccine RCT",
    "text": "Example: Pfizer Covid Vaccine RCT\n\nNumber of participants and number infected by treatment status\n\n\n\n\nGroup\nTreated\nPlacebo\n\n\n\n\nAll\n19965\n20172\n\n\nInfected\n9\n169\n\n\n65+\n4044\n4067\n\n\n65+ Infected\n1\n19"
  },
  {
    "objectID": "uncertainty.html#example-pfizer-covid-vaccine-rct-1",
    "href": "uncertainty.html#example-pfizer-covid-vaccine-rct-1",
    "title": "Uncertainty Quantification",
    "section": "Example Pfizer Covid Vaccine RCT",
    "text": "Example Pfizer Covid Vaccine RCT\n\n\n\nCode\nimport statsmodels.api as sm\n\nclass binarybinaryrct :\n    def __init__(self, NT, NU, NYT, NYU):\n        self.NT=NT\n        self.NU=NU\n        self.NYT=NYT\n        self.NYU=NYU\n\n    def ATE(self):\n        return (self.NYT/self.NT - self.NYU/self.NU)\n\n    def table(self):\n        return(\"|  | Infection Rate per 1000|\\n\"+\n               \"|---|---|\\n\"\n               f\"|Treated| {self.NYT/self.NT*1000:.2}|\\n\" +\n               f\"|Control| {self.NYU/self.NU*1000:.2}|\\n\" +\n               f\"|Difference| {self.ATE()*1000:.2}|\\n\")\n    def VE(self):\n        tb = sm.stats.Table2x2([[self.NYT, self.NT - self.NYT], [self.NYU, self.NU - self.NYU]])\n        ve=1-tb.riskratio\n        ci = tb.riskratio_confint()\n        ci = [1-ci[1],1-ci[0]]\n        return(ve,ci)\n\npfizerall = binarybinaryrct(19965, 20172, 9, 169)\npfizer65 = binarybinaryrct(4044, 4067, 1, 19)\n\nprint(\"\\n- All\\n\\n\" + pfizerall.table() + \"\\n - 65+\\n\\n\" + pfizer65.table())\n\n\nAll\n\n\n\n\n\nInfection Rate per 1000\n\n\n\n\nTreated\n0.45\n\n\nControl\n8.4\n\n\nDifference\n-7.9\n\n\n\n\n65+\n\n\n\n\n\nInfection Rate per 1000\n\n\n\n\nTreated\n0.25\n\n\nControl\n4.7\n\n\nDifference\n-4.4\n\n\n\n\n\nWe see the sample ATE of the Vaccine on infections per 1000 people\nSample ATE is random, how confident can we be that it is near the population ATE?"
  },
  {
    "objectID": "uncertainty.html#standard-errors",
    "href": "uncertainty.html#standard-errors",
    "title": "Uncertainty Quantification",
    "section": "Standard Errors",
    "text": "Standard Errors\n\n\n\nStandard Error = standard deviation of an estimator\nIn this example, ATE is difference of two proportions, so \\[\n\\begin{align*}\n  \\var(ATE) & = \\var(\\hat{p}_1 - \\hat{p}_0) \\\\\n  & = \\var(\\hat{p}_1) + \\var(\\hat{p}_0) \\\\\n  & = \\var\\left(\\frac{1}{N_1} \\sum_{i=1}^{N_1} Y_i \\right) + \\var\\left(\\frac{1}{N_0} \\sum_{i=1}^{N_0} Y_i \\right) \\\\\n  & = \\frac{1}{N_1} \\hat{p}_1(1-\\hat{p}_1) + \\frac{1}{N_0} \\hat{p}_0(1-\\hat{p}_0)\n\\end{align*}\n\\]\n\n\n\nCode\ndef tablewithse(self, scale=1000):\n    p1 = self.NYT/self.NT\n    p0 = self.NYU/self.NU\n    se = (p1*(1-p1)/self.NT + p0*(1-p0)/self.NU)**0.5\n    return(f\"|  | Infection Rate per {scale}|\\n\"+\n           \"|---|---|\\n\"\n           f\"|Treated| {p1*scale:.2} ({scale*(p1*(1-p1)/self.NT)**0.5:.2})|\\n\" +\n           f\"|Control| {p0*scale:.2} ({scale*(p0*(1-p0)/self.NU)**0.5:.2})|\\n\" +\n           f\"|Difference| {self.ATE()*scale:.2} ({se*scale:.2})|\\n\")\n\nbinarybinaryrct.table = tablewithse\n\nprint(pfizerall.table())\n\n\n\n\n\nInfection Rate per 1000\n\n\n\n\nTreated\n0.45 (0.15)\n\n\nControl\n8.4 (0.64)\n\n\nDifference\n-7.9 (0.66)"
  },
  {
    "objectID": "uncertainty.html#confidence-intervals",
    "href": "uncertainty.html#confidence-intervals",
    "title": "Uncertainty Quantification",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nFunction of data constructed such that \\[\nP(ATE \\in \\widehat{CI}_\\alpha) \\approx \\alpha\n\\]\nCan create if we know the (approximate) distribution of the data or the estimate"
  },
  {
    "objectID": "uncertainty.html#exact-confidence-interval",
    "href": "uncertainty.html#exact-confidence-interval",
    "title": "Uncertainty Quantification",
    "section": "Exact Confidence Interval",
    "text": "Exact Confidence Interval\n\nKnowing distribution of \\(\\hat{p}\\) given \\(p\\) can compute \\(P(|\\hat{p}-p|\\geq |\\hat{p}_{obs} - p|)\\) \\[\n\\widehat{CI}_\\alpha(\\hat{p}_{obs}) = \\{p: P(|\\hat{p}-p| \\leq |\\hat{p}_{obs} - p|) \\leq \\alpha \\}\n\\]\n\n\ndef proportioncisim(phat, n, level=0.95, S=100_000):\n    se = (phat*(1-phat)/n)**0.5\n    dp = se/20\n    p = phat - dp\n    while (abs(np.random.binomial(n,p, S)/n-p) &lt;= abs((phat-p))).mean() &lt; level :\n        p = p - dp\n        if p &lt; 0:\n            break\n    plo = p\n    p = phat + dp\n    while (abs(np.random.binomial(n,p,S)/n-p) &lt;= abs((phat-p))).mean() &lt; level :\n        p = p + dp\n        if p &lt; 0:\n            break\n    phi = p\n\n    return(plo,phi)\n\nphat = 0.05\nn = 100\nproportioncisim(phat, 100, level=0.95)\n\n(0.024936331074641174, 0.10557596153014372)"
  },
  {
    "objectID": "uncertainty.html#references",
    "href": "uncertainty.html#references",
    "title": "Uncertainty Quantification",
    "section": "References",
    "text": "References\n\n\n\n\nChernozhukov, V., C. Hansen, N. Kallus, M. Spindler, and V. Syrgkanis. 2024. Applied Causal Inference Powered by ML and AI. https://causalml-book.org/.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nRaič, Martin. 2019. “A multivariate Berry–Esseen theorem with explicit constants.” Bernoulli 25 (4A): 2824–53. https://doi.org/10.3150/18-BEJ1072."
  },
  {
    "objectID": "uncertainty.html#sources-and-further-reading",
    "href": "uncertainty.html#sources-and-further-reading",
    "title": "Uncertainty Quantification",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nChapter 3 of Facure (2022)\nChapter 1 of Chernozhukov et al. (2024)"
  },
  {
    "objectID": "uncertainty.html#exact-estimator-distribution",
    "href": "uncertainty.html#exact-estimator-distribution",
    "title": "Uncertainty Quantification",
    "section": "Exact Estimator Distribution",
    "text": "Exact Estimator Distribution\n\nConfidence interval for \\(P(Y)\\) with \\(Y_i \\in \\{0,1\\}\\), i.i.d.\nGiven \\(P(Y)=p\\) can calculate or simulate distribution of \\(\\hat{p} = \\frac{1}{n} \\sum Y_i\\)\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nps = [0.1, 0.05, 0.01]\nN = 400\nS = 100_000\nfor p in ps:\n    Y = np.random.binomial(1, p, (S, N))\n    Ybar = Y.mean(axis=1)\n    plt.hist(Ybar, bins=20, alpha=0.5,\n             label=f\"p₀={p}\")\n\nplt.title(\"Distribution of Sample Proportion\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "uncertainty.html#exact-confidence-interval-1",
    "href": "uncertainty.html#exact-confidence-interval-1",
    "title": "Uncertainty Quantification",
    "section": "Exact Confidence Interval",
    "text": "Exact Confidence Interval\n\nimport scipy\n#| echo: true\ndef proportionci(phat, n, level=0.95):\n    se = (phat*(1-phat)/n)**0.5\n    dp = se/200\n    k = round(phat*n)\n    p = phat - dp\n    while (scipy.stats.binom.cdf(k,n,p) &lt; (1-(1-level)/2) ) :\n        p = p - dp\n        if (p &lt; 0):\n            break\n    plo = p\n    p = phat + dp\n    while (scipy.stats.binom.cdf(k,n,p) &gt; (1-level)/2) :\n        p = p + dp\n        if p &lt; 0 :\n            break\n    phi = p\n    return(plo,phi)\n\nproportionci(phat, n)\n\n(0.022320991708517042, 0.11287711726057285)"
  },
  {
    "objectID": "uncertainty.html#standard-errors-1",
    "href": "uncertainty.html#standard-errors-1",
    "title": "Uncertainty Quantification",
    "section": "Standard Errors",
    "text": "Standard Errors\n\n\n\nStandard Error = standard deviation of an estimator\nIn this example, ATE is difference of two proportions, so \\[\n\\begin{align*}\n  \\var(ATE) & = \\var(\\hat{p}_1 - \\hat{p}_0) \\\\\n  & = \\var(\\hat{p}_1) + \\var(\\hat{p}_0) \\\\\n  & = \\var\\left(\\frac{1}{N_1} \\sum_{i=1}^{N_1} Y_i \\right) + \\var\\left(\\frac{1}{N_0} \\sum_{i=1}^{N_0} Y_i \\right) \\\\\n  & = \\frac{1}{N_1} \\hat{p}_1(1-\\hat{p}_1) + \\frac{1}{N_0} \\hat{p}_0(1-\\hat{p}_0)\n\\end{align*}\n\\]\n\n\n\nCode\ndef tablewithse(self, scale=1000):\n    p1 = self.NYT/self.NT\n    p0 = self.NYU/self.NU\n    se = (p1*(1-p1)/self.NT + p0*(1-p0)/self.NU)**0.5\n    return(f\"|  | Infection Rate per {scale}|\\n\"+\n           \"|---|---|\\n\"\n           f\"|Treated| {p1*scale:.2} ({scale*(p1*(1-p1)/self.NT)**0.5:.2})|\\n\" +\n           f\"|Control| {p0*scale:.2} ({scale*(p0*(1-p0)/self.NU)**0.5:.2})|\\n\" +\n           f\"|Difference| {self.ATE()*scale:.2} ({se*scale:.2})|\\n\")\n\nbinarybinaryrct.table = tablewithse\n\nprint(pfizerall.table())\n\n\n\n\n\nInfection Rate per 1000\n\n\n\n\nTreated\n0.45 (0.15)\n\n\nControl\n8.4 (0.64)\n\n\nDifference\n-7.9 (0.66)"
  },
  {
    "objectID": "uncertainty.html#confidence-intervals-1",
    "href": "uncertainty.html#confidence-intervals-1",
    "title": "Uncertainty Quantification",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nFunction of data constructed such that \\[\nP(ATE \\in \\widehat{CI}_\\alpha) \\approx \\alpha\n\\]\nCan create if we know the (approximate) distribution of the data or the estimate"
  },
  {
    "objectID": "uncertainty.html#approximate-confidence-interval",
    "href": "uncertainty.html#approximate-confidence-interval",
    "title": "Uncertainty Quantification",
    "section": "Approximate Confidence Interval",
    "text": "Approximate Confidence Interval\n\nUsually, distribution of data and/or estimator unknown\nBut a Central Limit Theorem can give an approximate distribution"
  },
  {
    "objectID": "uncertainty.html#central-limit-theorem",
    "href": "uncertainty.html#central-limit-theorem",
    "title": "Uncertainty Quantification",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\n\n\nBerry-Esseen Central Limit Theorem\n\n\nIf \\(X_i\\) are i.i.d. with \\(\\Er[X] = 0\\) and \\(\\var(X)=1\\), then \\[\n\\sup_{z \\in \\R} \\left\\vert\nP\\left(\\left[\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n X_i\\right] \\leq z \\right) - \\Phi(z) \\right\\vert \\leq 0.5 \\Er[|X|^3]/\\sqrt{n}\n\\] where \\(\\Phi\\) is the normal CDF.\n\n\n\n\n\n\n\n\n\nMultivariate Central Limit Theorem\n\n\nIf \\(X_i \\in \\R^d\\) are i.i.d. with \\(\\Er[X] = 0\\) and \\(\\var(X)=I_d\\), then \\[\n\\sup_{A \\subset \\R^d, \\text{convex}} \\left\\vert\nP\\left(\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n X_i \\in A \\right) - P(N(0,I_d) \\in A) \\right\\vert \\leq\n(42 d^{1/4} + 16) \\Er[\\Vert X \\Vert ^3]/\\sqrt{n}\n\\]\n1\n\n\n\n\n\nSee Raič (2019) for details."
  },
  {
    "objectID": "uncertainty.html#approximate-confidence-interval-1",
    "href": "uncertainty.html#approximate-confidence-interval-1",
    "title": "Uncertainty Quantification",
    "section": "Approximate Confidence Interval",
    "text": "Approximate Confidence Interval\n\ndef normalci(estimate, se, level=0.95):\n    return (estimate + se*scipy.stats.norm.ppf((1-level)/2),\n            estimate + se*scipy.stats.norm.ppf(1-(1-level)/2))\n\nphat = pfizerall.NYT/pfizerall.NT\nse = (phat*(1-phat)/pfizerall.NT)**0.5\nlo,hi=normalci(phat, se)\nlo*1000, hi*1000\n\n(np.float64(0.1563452787741701), np.float64(0.7452324823077232))\n\n\n\ndef differenceci(e1,se1, e0, se0, level=0.95) :\n    diff = e1 - e0\n    se = (se1*se1 + se0*s0)**0.5\n    return(normalci(diff, se, level=level))\n\np0 = pfizerall.NYU/pfizerall.NU\ns0 = (p0*(1-p0)/pfizerall.NU)**0.5\nlo,hi=differenceci(phat,se, p0, s0)\nlo*1000, hi*1000\n\n(np.float64(-9.218976095611334), np.float64(-6.63534540961651))"
  },
  {
    "objectID": "uncertainty.html#functions-of-estimates-1",
    "href": "uncertainty.html#functions-of-estimates-1",
    "title": "Uncertainty Quantification",
    "section": "Functions of Estimates",
    "text": "Functions of Estimates\n\nOften report functions of estimates\n\nE.g. \\[\n\\text{vaccine efficacy} = 1 - \\frac{\\Er[Y|T=1]} {\\Er[Y|T=0]}\n\\]"
  },
  {
    "objectID": "uncertainty.html#delta-method",
    "href": "uncertainty.html#delta-method",
    "title": "Uncertainty Quantification",
    "section": "Delta Method",
    "text": "Delta Method\n\n\n\n\nDelta Method\n\n\nIf 1. \\(\\sqrt{n} (\\hat{\\theta} - \\theta_0) \\leadsto N(0,\\Omega)\\)\n\n\\(g: \\R^k \\to \\R^m\\) is continuously differentiable\n\nThen \\(\\sqrt{n}(g(\\hat{\\theta}) - g(\\theta_0)) \\leadsto N\\left(0, D(g)(\\theta_0) \\Omega D(g)(\\theta_0) \\right)'\\)\n\n\n\n\n\nE.g. vaccine efficacy\n\n\\(\\theta_0 = (\\Er[Y|T=0], \\Er[Y|t=1])\\)\n\\(g(\\theta) = 1 - \\theta_2/\\theta_1\\)\n\\(D(g)(\\theta_0) = \\begin{pmatrix} \\frac{\\Er[Y|T=1]}{\\Er[Y|T=0]^2} & -\\frac{1}{\\Er[Y|T=0]} \\end{pmatrix}\\)\n\\(\\Omega = \\begin{pmatrix} p_0(1-p_0)/n_0 & 0 \\\\ 0 & p_1(1-p_1)/n_1 \\end{pmatrix}\\)"
  },
  {
    "objectID": "uncertainty.html#delta-method-2",
    "href": "uncertainty.html#delta-method-2",
    "title": "Uncertainty Quantification",
    "section": "Delta Method",
    "text": "Delta Method\n\nimport torch\n\ntheta = [p0, phat]\nOmega = np.matrix([[s0**2, 0], [0, se**2]])\ndef ve(theta) :\n    return(1 - theta[1]/theta[0])\n\ndef deltamethod(g, theta, Omega) :\n    x = torch.tensor(theta, requires_grad=True)\n    val = g(x)\n    val.backward()\n    Dg = x.grad.numpy()\n    return(Dg @ Omega @ Dg.transpose())\n\nprint(f\"Efficacy among everyone is {ve(theta):.3} ({deltamethod(ve,theta,Omega)[0,0]**0.5:.2})\")\n\nEfficacy among everyone is 0.946 (0.018)"
  },
  {
    "objectID": "uncertainty.html#simulation",
    "href": "uncertainty.html#simulation",
    "title": "Uncertainty Quantification",
    "section": "Simulation",
    "text": "Simulation\n\nSuppose \\(\\sqrt{n} (\\hat{\\theta} - \\theta_0) \\leadsto N(0,\\Omega)\\)\nCreate \\(\\tilde{\\theta}_s \\sim N(\\hat{\\theta}, \\Omega/n)\\)\n\nso \\(\\sqrt{n}(\\tilde{\\theta}_s - \\hat{\\theta}) \\sim N(0,\\Omega)\\)\n\nand \\(\\sqrt{n}(g(\\theta_{s}) - g(\\hat{\\theta})) \\approx \\sqrt{n}(g(\\hat{\\theta}) - g(\\theta_0))\\)\n\n\n\ndef simulateg(g,theta,Omega,S=10_000):\n    thetas = np.random.multivariate_normal(theta, Omega, S)\n    gs = np.apply_along_axis(g,1,thetas)\n    return(gs, ((gs-g(theta))**2).mean())\n\ngs, gv=simulateg(ve, theta, Omega)\nprint(f\"simulation se = {gv**0.5:.2}\")\n\nplt.hist(gs, bins=50, alpha=0.5)\nplt.title(\"Distribution of Vaccine Efficacy\")\nplt.show()\n\n\nsimulation se = 0.019"
  },
  {
    "objectID": "uncertainty.html#exact-confidence-interval-2",
    "href": "uncertainty.html#exact-confidence-interval-2",
    "title": "Uncertainty Quantification",
    "section": "Exact Confidence Interval",
    "text": "Exact Confidence Interval\n\nphat = pfizerall.NYT/pfizerall.NT\nplo, phi=proportionci(phat,pfizerall.NT)\n1000*plo, 1000*phi\n\n(0.23971701200382992, 0.8556562725036012)"
  },
  {
    "objectID": "uncertainty.html#bootstrap-1",
    "href": "uncertainty.html#bootstrap-1",
    "title": "Uncertainty Quantification",
    "section": "Bootstrap",
    "text": "Bootstrap\n\nData \\(\\{x_i\\}_{i=1}^n\\)\nEstimator \\(\\hat{\\theta}(\\{x_i\\}_{i=1}^n)\\)\n\ne.g. mean \\(\\hat{\\theta}(\\{x_i\\}_{i=1}^n) = \\frac{1}{n} \\sum_{i=1}^n x_i\\)\n\nIf we knew the distribution of \\(x\\), we could calculate the distribution of \\(\\hat{\\theta}\\)\n\n\n\nBootstrap: use empirical distribution as estimate of unknown distribution of \\(x\\)"
  },
  {
    "objectID": "uncertainty.html#bootstrap-2",
    "href": "uncertainty.html#bootstrap-2",
    "title": "Uncertainty Quantification",
    "section": "Bootstrap",
    "text": "Bootstrap\n\nnp.random.seed(6987)\ndef dgp(n):\n    x = np.exp(abs(np.random.normal(0,1,n)))\n    return x\n\ndef estimator(x):\n    return np.mean(x)\n\n\ndef bootstrap(data, estimator, B=999):\n    n = len(data)\n    est = estimator(data)\n    bestimates = [estimator(np.random.choice(data,size=n, replace=True))-est for _ in range(B)]\n    return bestimates\n\ndef cltdistmean(data, B=999):\n    return np.random.normal(0,np.sqrt(np.var(data)/len(data)),B)\n\ncmap = sns.color_palette(\"tab10\")\nn = 20\nB = 999\ntrue = estimator(dgp(n*10_000))\n# simulate true distribution of estimator\nestsims = [estimator(dgp(n)) - true for _ in range(S)]\nsns.ecdfplot(estsims, label=\"True distribution\", linewidth=3, color=\"black\")\nfor b in range(3):\n    data = dgp(n)\n    sns.ecdfplot(bootstrap(data, estimator, B=B), alpha=0.5, label=f\"Bootstrap {b}\", color=cmap[b])\n    sns.ecdfplot(cltdistmean(data,B=10*B), alpha=0.5,\n                 label=f\"CLT {b}\", color=cmap[b], linestyle='dashed')\n\nsd=np.std(estsims)\nplt.xlim(-2.5*sd,2.5*sd)\nplt.ylim(0,1)\nplt.legend()\nplt.title(f\"True CDF and Approximate CDFs for three data realizations with n={n}\")\nplt.show()"
  },
  {
    "objectID": "uncertainty.html#bootstrap-2-output",
    "href": "uncertainty.html#bootstrap-2-output",
    "title": "Uncertainty Quantification",
    "section": "Bootstrap",
    "text": "Bootstrap"
  },
  {
    "objectID": "uncertainty.html#bootstrap-confidence-interval",
    "href": "uncertainty.html#bootstrap-confidence-interval",
    "title": "Uncertainty Quantification",
    "section": "Bootstrap Confidence Interval",
    "text": "Bootstrap Confidence Interval\n\ndef bootstrapci(data, estimator, B=999, level=0.95):\n    estimates = bootstrap(data, estimator, B)\n    lo = np.percentile(estimates, 100*(1-level)/2) + estimator(data)\n    hi = np.percentile(estimates, 100*(1-(1-level)/2)) + estimator(data)\n    return lo, hi\n\nbootstrapci(dgp(n), np.mean)\n\n(np.float64(2.1750298043936604), np.float64(3.7622303748107635))"
  },
  {
    "objectID": "uncertainty.html#multiple-testing",
    "href": "uncertainty.html#multiple-testing",
    "title": "Uncertainty Quantification",
    "section": "Multiple Testing",
    "text": "Multiple Testing\n\nIf construct many confidence intervals, there is high change at least one will not contain the true parameter\n\n65% chance with twenty 95% confidence intervals\n\nTrying many things and only focusing on “statistically significant” results only leads to spurious conclusions"
  },
  {
    "objectID": "uncertainty.html#dependence",
    "href": "uncertainty.html#dependence",
    "title": "Uncertainty Quantification",
    "section": "Dependence",
    "text": "Dependence\n\nStandard errors require assumptions limiting dependence\n\nIndependent observations\nNot-too-dependent observations\n\nClustered (for panel or grouped data)\nStationary (for time series)\n\n\nBootstrap needs to be modified for dependence"
  },
  {
    "objectID": "uncertainty.html#pathological-parameter-values",
    "href": "uncertainty.html#pathological-parameter-values",
    "title": "Uncertainty Quantification",
    "section": "Pathological Parameter Values",
    "text": "Pathological Parameter Values\n\nUsual asymptotics (CLT, bootstrap, etc) don’t work if\n\nTrue parameter on boundary (e.g. variance = 0)\nTrue parameter near value where distribution of estimator discontinuous\n\nWeak instruments\nUnit root"
  },
  {
    "objectID": "uncertainty.html#high-dimensions",
    "href": "uncertainty.html#high-dimensions",
    "title": "Uncertainty Quantification",
    "section": "High Dimensions",
    "text": "High Dimensions\n\nNumber of parameters (\\(p\\)) large compared to sample size (\\(n\\))\nUsual asymptotics break down\nRisk of overfitting\nInference difficult, but possible\n\nChernozhukov et al. (2024) chapter 4\n\nMore later …"
  },
  {
    "objectID": "uncertainty.html#simulating-distribution-of-gtheta",
    "href": "uncertainty.html#simulating-distribution-of-gtheta",
    "title": "Uncertainty Quantification",
    "section": "Simulating Distribution of \\(g(\\theta)\\)",
    "text": "Simulating Distribution of \\(g(\\theta)\\)\n\nSuppose \\(\\sqrt{n} (\\hat{\\theta} - \\theta_0) \\leadsto N(0,\\Omega)\\)\nCreate \\(\\tilde{\\theta}_s \\sim N(\\hat{\\theta}, \\Omega/n)\\)\n\nso \\(\\sqrt{n}(\\tilde{\\theta}_s - \\hat{\\theta}) \\sim N(0,\\Omega)\\)\n\nand \\(\\sqrt{n}(g(\\theta_{s}) - g(\\hat{\\theta})) \\approx \\sqrt{n}(g(\\hat{\\theta}) - g(\\theta_0))\\)"
  },
  {
    "objectID": "uncertainty.html#simulated-distribution-of-vaccine-efficacy",
    "href": "uncertainty.html#simulated-distribution-of-vaccine-efficacy",
    "title": "Uncertainty Quantification",
    "section": "Simulated Distribution of Vaccine Efficacy",
    "text": "Simulated Distribution of Vaccine Efficacy\n\n\ndef simulateg(g,theta,Omega,S=10_000):\n    thetas = np.random.multivariate_normal(theta, Omega, S)\n    gs = np.apply_along_axis(g,1,thetas)\n    return(gs, ((gs-g(theta))**2).mean())\n\ngs, gv=simulateg(ve, theta, Omega)\nprint(f\"simulation se = {gv**0.5:.2}\")\n\nplt.hist(gs, bins=50, alpha=0.5, density=True)\ndse = deltamethod(ve,theta,Omega)[0,0]**0.5\nz = np.linspace(-3,3,num=100)*dse + ve(theta)\nfdeltamethod = scipy.stats.norm(loc=ve(theta), scale=dse).pdf\nplt.plot(z, fdeltamethod(z))\nplt.title(\"Distribution of Vaccine Efficacy\")\nplt.show()\n\n\nsimulation se = 0.019"
  },
  {
    "objectID": "regression.html#regression-for-rct",
    "href": "regression.html#regression-for-rct",
    "title": "Linear Regression",
    "section": "Regression for RCT",
    "text": "Regression for RCT\n\nCreate a dataframe to represent the Pfizer Covid vaccine trial\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom statsmodels.iolib.summary2 import summary_col\nimport os\nimport requests\n\n\n\n\nCode\ndef pfizerrctdata():\n    n1 = 19965\n    n0 = 20172\n    y1 = 9\n    y0 = 169\n    n1o = 4044\n    n0o = 4067\n    y1o = 1\n    y0o = 19\n    over65 = np.zeros(n1 + n0)\n    over65[0:(n1o+n0o)] = 1\n    treat = np.concatenate([np.ones(n1o), np.zeros(n0o), np.ones(n1-n1o), np.zeros(n0-n0o)])\n    infected = np.concatenate([np.ones(y1o), np.zeros(n1o-y1o), np.ones(y0o), np.zeros(n0o-y0o),\n                               np.ones(y1-y1o), np.zeros(n1-n1o-(y1-y1o)),\n                               np.ones(y0-y0o), np.zeros(n0-n0o-y0+y0o)])\n    data = pd.DataFrame({'treat': treat, 'infected': infected, 'over65': over65})\n    return data\n\ndata = pfizerrctdata()\n\ndata.groupby(['over65', 'treat']).mean()\n\n\n\n\n\n\n\n\n\n\ninfected\n\n\nover65\ntreat\n\n\n\n\n\n0.0\n0.0\n0.009314\n\n\n1.0\n0.000502\n\n\n1.0\n0.0\n0.004672\n\n\n1.0\n0.000247"
  },
  {
    "objectID": "regression.html#heteroskedasticity-robust-standard-errors",
    "href": "regression.html#heteroskedasticity-robust-standard-errors",
    "title": "Linear Regression",
    "section": "Heteroskedasticity Robust Standard Errors",
    "text": "Heteroskedasticity Robust Standard Errors\n\nolsse = np.sqrt(np.diag(smf.ols('infected ~ treat',data=data.loc[data['over65']==1]).fit().cov_params())[1])\nprint(f\"OLS SE: {olsse}, manual SE: {ate[1]}\")\n\nOLS SE: 0.0011004170544555998, manual SE: 0.0010976149240857532\n\n\n\nBut standard error very slightly different\nDefault of smf.ols assumes homoskedasticiy \\(\\Er[\\epsilon^2|X] = \\sigma^2\\)\nWith \\(y\\) and \\(T\\), binary, \\(\\Er[\\epsilon^2|T] = P(y=1|T)(1-P(y=1|T))\\)\n\n\nolshcse = np.sqrt(np.diag(smf.ols('infected ~ treat',data=data.loc[data['over65']==1]).fit(cov_type=\"HC3\").cov_params())[1])\nprint(f\"OLS HC SE: {olshcse}, manual SE: {ate[1]}\")\n\nOLS HC SE: 0.001097749929536793, manual SE: 0.0010976149240857532\n\n\n\nalways use heteroskedasticity robust standard errors"
  },
  {
    "objectID": "regression.html#multiple-regression",
    "href": "regression.html#multiple-regression",
    "title": "Linear Regression",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\n\\(y \\in \\R^n\\), \\(X \\in \\R^{n \\times k}\\) \\[ %\n\\begin{align*}\n\\hat{\\beta} & \\in \\argmin_{\\beta} \\Vert y - X \\beta \\Vert_2^2 \\\\\n\\hat{\\beta} & \\in \\argmin_{\\beta} \\sum_{i=1}^n (y_i - x_i' \\beta)^2\n\\end{align*}\n\\]\nPopulation regression \\[\n\\begin{align*}\n\\beta_0 & \\in \\argmin_{\\beta} \\Er[(y - x'\\beta)^2] \\\\\n\\beta_0 & \\in \\argmin_{\\beta} \\Er[(\\Er[y|x] - x'\\beta)^2]\n\\end{align*}\n\\]\n\nbest linear approximation to conditional expectation"
  },
  {
    "objectID": "regression.html#large-sample-behavior",
    "href": "regression.html#large-sample-behavior",
    "title": "Linear Regression",
    "section": "Large Sample Behavior",
    "text": "Large Sample Behavior\n\nWith appropriate assumptions,\n\nconsistent \\(\\hat{\\beta} \\inprob \\beta_0\\)\nasymptotically normal \\[\n\\sqrt{n}(\\hat{\\beta} - \\beta_0) \\indist N\\left(0, \\Er[xx']^{-1} \\Er[xx'\\epsilon^2] \\Er[xx']^{-1} \\right)\n\\]"
  },
  {
    "objectID": "regression.html#ceteris-paribus",
    "href": "regression.html#ceteris-paribus",
    "title": "Linear Regression",
    "section": "Ceteris Paribus",
    "text": "Ceteris Paribus\n\nRegression estimates \\(\\beta_0 \\in \\argmin_{\\beta} \\Er[(\\Er[y|x] - x'\\beta)^2]\\)\n\n\\(x'\\beta_0\\) is the best linear approximation to \\(\\Er[y|x]\\)\n\\(\\frac{\\partial}{\\partial x_1}\\Er[y|x] \\approx \\beta_{0,1}\\) is the change in \\(x_1\\) holding the rest of \\(x\\) constant"
  },
  {
    "objectID": "regression.html#example-gender-earnings-gap",
    "href": "regression.html#example-gender-earnings-gap",
    "title": "Linear Regression",
    "section": "Example: Gender Earnings Gap",
    "text": "Example: Gender Earnings Gap\n\nimport os\nimport requests\nurl = 'https://www.nber.org/morg/annual/morg23.dta'\nlocal_filename = 'data/morg23.dta'\n\nif not os.path.exists(local_filename):\n    response = requests.get(url)\n    with open(local_filename, 'wb') as file:\n        file.write(response.content)\n\ncps=pd.read_stata(local_filename)\ncps[\"female\"] = (cps.sex==2)\ncps[\"log_earn\"] = np.log(cps[\"earnwke\"])\ncps[\"log_uhours\"] = np.log(cps.uhourse)\ncps[\"log_hourslw\"] = np.log(cps.hourslw)\ncps.replace(-np.inf, np.nan, inplace=True)\ncps[\"nevermarried\"] = cps.marital==7\ncps[\"wasmarried\"] = (cps.marital &gt;= 4) & (cps.marital &lt;= 6)\ncps[\"married\"] = cps.marital &lt;= 3\n\nlm = list()\nlm.append(smf.ols(formula=\"log_earn ~ female\", data=cps,\n                  missing=\"drop\").fit(cov_type='HC3'))\nlm.append(smf.ols(formula=\"log_earn ~ female + log_hourslw + log_uhours\", data=cps,\n                  missing=\"drop\").fit(cov_type='HC3'))\nlm.append(smf.ols(formula=\"log_earn ~ female + log_hourslw + log_uhours + wasmarried + married\", data=cps,\n                  missing=\"drop\").fit(cov_type='HC3'))\nlm.append(smf.ols(formula=\"log_earn ~ female*(wasmarried+married) + log_hourslw + log_uhours\", data=cps,\n                  missing=\"drop\").fit(cov_type='HC3'))\n\nsummary_col(lm, stars=True, model_names=[f\"{i+1}\" for i in range(len(lm))])"
  },
  {
    "objectID": "regression.html#example-gender-earnings-gap-output",
    "href": "regression.html#example-gender-earnings-gap-output",
    "title": "Linear Regression",
    "section": "Example: Gender Earnings Gap",
    "text": "Example: Gender Earnings Gap\n\n\n\n\n\n1\n2\n3\n4\n\n\nIntercept\n6.9948***\n2.2378***\n2.2410***\n2.2166***\n\n\n\n(0.0031)\n(0.0295)\n(0.0280)\n(0.0280)\n\n\nfemale[T.True]\n-0.2772***\n-0.1318***\n-0.1340***\n-0.0715***\n\n\n\n(0.0046)\n(0.0038)\n(0.0038)\n(0.0061)\n\n\nlog_hourslw\n\n0.0424***\n0.0443***\n0.0434***\n\n\n\n\n(0.0077)\n(0.0076)\n(0.0075)\n\n\nlog_uhours\n\n1.2623***\n1.2113***\n1.2108***\n\n\n\n\n(0.0119)\n(0.0115)\n(0.0115)\n\n\nwasmarried[T.True]\n\n\n0.1474***\n0.1831***\n\n\n\n\n\n(0.0058)\n(0.0089)\n\n\nmarried[T.True]\n\n\n0.2896***\n0.3361***\n\n\n\n\n\n(0.0041)\n(0.0055)\n\n\nfemale[T.True]:wasmarried[T.True]\n\n\n\n-0.0727***\n\n\n\n\n\n\n(0.0118)\n\n\nfemale[T.True]:married[T.True]\n\n\n\n-0.0988***\n\n\n\n\n\n\n(0.0080)\n\n\nR-squared\n0.0287\n0.3632\n0.3898\n0.3906\n\n\nR-squared Adj.\n0.0287\n0.3631\n0.3898\n0.3906\n\n\n\n\nStandard errors in parentheses.\n* p&lt;.1, ** p&lt;.05, ***p&lt;.01"
  },
  {
    "objectID": "regression.html#partialling-out",
    "href": "regression.html#partialling-out",
    "title": "Linear Regression",
    "section": "Partialling Out",
    "text": "Partialling Out\n\\[y_i = x_i \\beta + w_i'\\gamma + u_i\\] - Can equivalently calculate \\(\\beta\\) by\n\n\n\nMultiple regression of \\(y\\) on \\(x\\) and \\(w\\), or\n\n\nsmf.ols(formula=\"log_earn ~ female + log_hourslw + log_uhours\", data=cps, missing=\"drop\").fit(cov_type=\"HC3\").params[1]\n\nnp.float64(-0.13177744005735112)\n\n\n\n\nBivariate regression of residuals from regressing \\(y\\) on \\(w\\), on the residuals from regression \\(x\\) on \\(w\\)\n\n\ney=smf.ols(formula=\"log_earn ~ log_hourslw + log_uhours\", data=cps, missing=\"drop\").fit().resid\nex=smf.ols(formula=\"I(1*female) ~ log_hourslw + log_uhours\", data=cps, missing=\"drop\").fit().resid\nedf = pd.concat([ex,ey],axis=1)\nedf.columns=['ex','ey']\nsmf.ols('ey ~ ex', data=edf).fit(cov_type=\"HC3\").params[1]\n\nnp.float64(-0.131764318408401)"
  },
  {
    "objectID": "regression.html#omitted-variables",
    "href": "regression.html#omitted-variables",
    "title": "Linear Regression",
    "section": "Omitted Variables",
    "text": "Omitted Variables\n\nIf we want \\[\ny_i = \\beta_0 + x_i \\beta + w_i'\\gamma + u_i\n\\]\nBut only regression \\(y\\) on \\(x\\), then \\[\n\\hat{\\beta}^s = \\hat{\\beta} + \\frac{ \\sum (x_i - \\bar{x})w_i'}{\\sum (x_i - \\bar{x})^2} \\hat{\\gamma}\n\\] and \\[\n\\hat{\\beta}^s \\inprob \\beta + \\frac{ \\Er[(x_i - \\Er[x])w_i']}{\\var(x_i)} \\gamma\n\\]\nUseful for:\n\nUnderstanding mechanically why coefficients change when we add/remove variables\nSpeculating about direction of bias when we some variables are unobserved"
  },
  {
    "objectID": "regression.html#more-conditioning",
    "href": "regression.html#more-conditioning",
    "title": "Linear Regression",
    "section": "More Conditioning",
    "text": "More Conditioning\n\nimport pyfixest as pf\n\ncontrols=\"age + I(age**2) | race + grade92 + unionmme + unioncov +  ind17 + occ18\"\nallcon=pf.feols(\"log_earn ~ female*(wasmarried + married) + log_hourslw + log_uhours + \" + controls, data=cps,vcov='hetero')\nallcon.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: log_earn, Fixed effects: race+grade92+unionmme+unioncov+ind17+occ18\nInference:  hetero\nObservations:  104607\n\n| Coefficient       |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:------------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| female            |     -0.084 |        0.006 |   -14.742 |      0.000 | -0.095 |  -0.073 |\n| wasmarried        |      0.082 |        0.009 |     9.570 |      0.000 |  0.065 |   0.099 |\n| married           |      0.126 |        0.005 |    23.207 |      0.000 |  0.115 |   0.137 |\n| log_hourslw       |      0.044 |        0.007 |     6.649 |      0.000 |  0.031 |   0.058 |\n| log_uhours        |      0.988 |        0.011 |    93.671 |      0.000 |  0.967 |   1.009 |\n| age               |      0.003 |        0.000 |    19.767 |      0.000 |  0.003 |   0.003 |\n| I(age ** 2)       |     -0.000 |        0.000 |    -1.174 |      0.240 | -0.000 |   0.000 |\n| female:wasmarried |     -0.059 |        0.011 |    -5.507 |      0.000 | -0.080 |  -0.038 |\n| female:married    |     -0.069 |        0.007 |    -9.734 |      0.000 | -0.083 |  -0.055 |\n---\nRMSE: 0.524 R2: 0.582 R2 Within: 0.314"
  },
  {
    "objectID": "regression.html#regression-for-rcts-1",
    "href": "regression.html#regression-for-rcts-1",
    "title": "Linear Regression",
    "section": "Regression for RCTs",
    "text": "Regression for RCTs\n\nRCT with outcome \\(Y\\), treatment \\(T\\), other variables \\(X\\)\nShould we estimate ATE in a regression that includes \\(X\\)?"
  },
  {
    "objectID": "regression.html#simulated-rct",
    "href": "regression.html#simulated-rct",
    "title": "Linear Regression",
    "section": "Simulated RCT",
    "text": "Simulated RCT\n\nfrom Chernozhukov et al. (2024) chapter 2 (who got the setup from Roth)\n\n\nnp.random.seed(54)\nn = 1000             # sample size\nZ = np.random.normal(size=n)         # generate Z\nY0 = -Z + np.random.normal(size=n)   # conditional average baseline response is -Z\nY1 = Z + np.random.normal(size=n)    # conditional average treatment effect is +Z\nD = np.random.binomial(1, .2, size=n)    # treatment indicator; only 20% get treated\nY = Y1 * D + Y0 * (1 - D)  # observed Y\nZ = Z - Z.mean()       # demean Z\ndata = pd.DataFrame({\"Y\": Y, \"D\": D, \"Z\": Z})\nprint(f\"Unobservable sample ATE = {np.mean(Y1-Y0):.3}\")\n\nUnobservable sample ATE = 0.072\n\n\n\nPopulation ATE is \\(0\\)"
  },
  {
    "objectID": "regression.html#simulated-rct-1",
    "href": "regression.html#simulated-rct-1",
    "title": "Linear Regression",
    "section": "Simulated RCT",
    "text": "Simulated RCT\n\nhc = 'HC0'\nm1=smf.ols('Y ~ D',data=data).fit(cov_type=hc)\nmadd=smf.ols('Y ~ D + Z',data=data).fit(cov_type=hc)\nsummary_col([m1, madd], model_names=['simple','additive'])\n\n\n\n\n\nsimple\nadditive\n\n\nIntercept\n-0.0924\n-0.0789\n\n\n\n(0.0501)\n(0.0386)\n\n\nD\n-0.0370\n-0.1046\n\n\n\n(0.1166)\n(0.1440)\n\n\nZ\n\n-0.5630\n\n\n\n\n(0.0561)\n\n\nR-squared\n0.0001\n0.1551\n\n\nR-squared Adj.\n-0.0009\n0.1534\n\n\n\n\nStandard errors in parentheses."
  },
  {
    "objectID": "regression.html#simulated-rct-2",
    "href": "regression.html#simulated-rct-2",
    "title": "Linear Regression",
    "section": "Simulated RCT",
    "text": "Simulated RCT\n\nminteract=smf.ols('Y ~ D + Z*D',data=data).fit(cov_type=hc)\nsummary_col([m1, madd, minteract],model_names=['simple','additive','interactive'])\n\n\n\n\n\nsimple\nadditive\ninteractive\n\n\nIntercept\n-0.0924\n-0.0789\n-0.0681\n\n\n\n(0.0501)\n(0.0386)\n(0.0352)\n\n\nD\n-0.0370\n-0.1046\n0.0463\n\n\n\n(0.1166)\n(0.1440)\n(0.0766)\n\n\nZ\n\n-0.5630\n-1.0111\n\n\n\n\n(0.0561)\n(0.0341)\n\n\nZ:D\n\n\n2.1349\n\n\n\n\n\n(0.0748)\n\n\nR-squared\n0.0001\n0.1551\n0.5245\n\n\nR-squared Adj.\n-0.0009\n0.1534\n0.5231\n\n\n\n\nStandard errors in parentheses."
  },
  {
    "objectID": "regression.html#if-t-indep-x-interactive-model-reduces-variance",
    "href": "regression.html#if-t-indep-x-interactive-model-reduces-variance",
    "title": "Linear Regression",
    "section": "If \\(T \\indep X\\), Interactive Model Reduces Variance",
    "text": "If \\(T \\indep X\\), Interactive Model Reduces Variance\n\nAssume \\(T \\indep (X, Y(0), Y(1))\\), \\(T \\in \\{0,1\\}\\), \\(\\Er[X] = 0\\)\nConsider \\[\n\\begin{align*}\nY & = \\beta_0^s + \\beta_1^s T + \\epsilon^s \\\\\nY & = \\beta_0^a + \\beta_1^a T + X'\\gamma^a_0 + \\epsilon^a \\\\\nY & = \\beta_0^i + \\beta_1^i T + X'\\gamma^i_0 + TX'\\gamma^i_1 + \\epsilon^s\n\\end{align*}\n\\]\nAll are consistent \\[\n\\plim \\hat{\\beta}_1^s = \\plim \\hat{\\beta}_1^a = \\plim \\hat{\\beta}_1^i = ATE\n\\]\nInteractive has smaller asymptotic variance \\[\n\\var(\\hat{\\beta}_1^i) \\leq \\var(\\hat{\\beta}_1^s) \\text{ and } \\var(\\hat{\\beta}_1^i) \\leq \\var(\\hat{\\beta}_1^a)\n\\]"
  },
  {
    "objectID": "regression.html#collections-and-payment-reminders",
    "href": "regression.html#collections-and-payment-reminders",
    "title": "Linear Regression",
    "section": "Collections and Payment Reminders",
    "text": "Collections and Payment Reminders\n\nData from credit firm\nTreatment = email reminder to repay\nOutcome = payments\nOther variables\n\ncredit limit\nrisk score\nwhether email openned\nwhether agreed to repay after opening email\n\n\n\n\nFrom chapter 7 of Facure (2022)"
  },
  {
    "objectID": "regression.html#collections-and-payment-reminders-1",
    "href": "regression.html#collections-and-payment-reminders-1",
    "title": "Linear Regression",
    "section": "Collections and Payment Reminders",
    "text": "Collections and Payment Reminders\n\n\nCode\nfilename = 'data/collections_email.csv'\nurl = 'https://raw.githubusercontent.com/matheusfacure/python-causality-handbook/refs/heads/master/causal-inference-for-the-brave-and-true/data/collections_email.csv'\nif not os.path.exists(filename):\n    response = requests.get(url)\n    with open(filename, 'wb') as file:\n        file.write(response.content)\n\ndata = pd.read_csv(filename)\ndata.describe()\n\n\n\n\n\n\n\n\n\npayments\nemail\nopened\nagreement\ncredit_limit\nrisk_score\n\n\n\n\ncount\n5000.000000\n5000.000000\n5000.000000\n5000.000000\n5000.000000\n5000.000000\n\n\nmean\n669.672000\n0.490800\n0.273400\n0.160800\n1194.845188\n0.480812\n\n\nstd\n103.970065\n0.499965\n0.445749\n0.367383\n480.978996\n0.100376\n\n\nmin\n330.000000\n0.000000\n0.000000\n0.000000\n193.695573\n0.131784\n\n\n25%\n600.000000\n0.000000\n0.000000\n0.000000\n843.049867\n0.414027\n\n\n50%\n670.000000\n0.000000\n0.000000\n0.000000\n1127.640297\n0.486389\n\n\n75%\n730.000000\n1.000000\n1.000000\n0.000000\n1469.096523\n0.552727\n\n\nmax\n1140.000000\n1.000000\n1.000000\n1.000000\n3882.178408\n0.773459"
  },
  {
    "objectID": "regression.html#collections-and-payment-reminders-2",
    "href": "regression.html#collections-and-payment-reminders-2",
    "title": "Linear Regression",
    "section": "Collections and Payment Reminders",
    "text": "Collections and Payment Reminders\n\n\nCode\nlm = list()\nlm.append(smf.ols(formula=\"payments ~ email\", data=data).fit(cov_type='HC3'))\nlm.append(smf.ols(formula=\"payments ~ email + credit_limit + risk_score\",data=data).fit(cov_type='HC3'))\nlm.append(smf.ols(formula=\"payments ~ email + credit_limit + risk_score + opened + agreement\",data=data).fit(cov_type='HC3'))\nsummary_col(lm, stars=True, model_names=[f\"{i+1}\" for i in range(len(lm))])\n\n\n\n\n\n\n1\n2\n3\n\n\nIntercept\n669.9764***\n490.8653***\n488.4416***\n\n\n\n(2.0973)\n(10.2265)\n(10.2052)\n\n\nemail\n-0.6203\n4.4304**\n-1.6095\n\n\n\n(2.9401)\n(2.1273)\n(2.7095)\n\n\ncredit_limit\n\n0.1511***\n0.1507***\n\n\n\n\n(0.0085)\n(0.0085)\n\n\nrisk_score\n\n-8.0516\n-2.0929\n\n\n\n\n(40.6389)\n(40.4986)\n\n\nopened\n\n\n3.9808\n\n\n\n\n\n(3.9791)\n\n\nagreement\n\n\n11.7093***\n\n\n\n\n\n(4.2158)\n\n\nR-squared\n0.0000\n0.4774\n0.4796\n\n\nR-squared Adj.\n-0.0002\n0.4771\n0.4791\n\n\n\n\nStandard errors in parentheses.\n* p&lt;.1, ** p&lt;.05, ***p&lt;.01"
  },
  {
    "objectID": "regression.html#collections-and-payment-reminders-3",
    "href": "regression.html#collections-and-payment-reminders-3",
    "title": "Linear Regression",
    "section": "Collections and Payment Reminders",
    "text": "Collections and Payment Reminders\n\nWhich specification make sense?\nAny red-flags in the results?\nWhat conclusions can we draw?"
  },
  {
    "objectID": "regression.html#bad-controls-or-mediators-or-colliders",
    "href": "regression.html#bad-controls-or-mediators-or-colliders",
    "title": "Linear Regression",
    "section": "“Bad controls” or Mediators or Colliders",
    "text": "“Bad controls” or Mediators or Colliders\n\n\nFacure (2022) calls controlling for opened and agreement “selection bias,” but in economics, we would not call it that. We would refer to opened and agreement as bad controls because they mediate the outcome of interest. We would not want hold them constant because part of how email affects payments is by changing opened and agreement.\nSince at least Heckman (1976) and Heckman (1979) selection bias refers to when the expectation of an outcome conditional on observing it is not equal to the expectation in the population. As far as I know, calling conditioning on mediators “selection bias” was popularized by Hernán, Hernández-Dı́az, and Robins (2004) in epidemiology and spread to other field.s If you want to avoid confusion, one coould call Heckman (1976) selection bias “self-selection bias” or “Heckman selection bias,” and call Hernán, Hernández-Dı́az, and Robins (2004) selection bias “collider bias” or “mediator bias.”"
  },
  {
    "objectID": "regression.html#drug-trial-at-two-hospitals",
    "href": "regression.html#drug-trial-at-two-hospitals",
    "title": "Linear Regression",
    "section": "Drug Trial at Two Hospitals",
    "text": "Drug Trial at Two Hospitals\n\nCode\nfilename = 'data/hospital_treatment.csv'\nurl = 'https://raw.githubusercontent.com/matheusfacure/python-causality-handbook/refs/heads/master/causal-inference-for-the-brave-and-true/data/hospital_treatment.csv'\nif not os.path.exists(filename):\n    response = requests.get(url)\n    with open(filename, 'wb') as file:\n        file.write(response.content)\n\ndrug = pd.read_csv(filename)\nprint(drug.apply([np.mean, np.std]).to_markdown() + f\"\\n| N    | {len(drug)}     |\\n\")\n\n\n\n\n\nhospital\ntreatment\nseverity\ndays\n\n\n\n\nmean\n0.6375\n0.625\n15.4758\n42.1125\n\n\nstd\n0.480722\n0.484123\n7.14637\n15.9429\n\n\nN\n80\n\n\n\n\n\n\n\nhospital \\(\\in \\{0,1\\}\\)\ntreatment \\(\\in \\{0,1\\}\\)\nseverity prior to treatment assignment\ndays in hospital\n\n\n\nFrom chapter 7 of Facure (2022)"
  },
  {
    "objectID": "regression.html#drug-trial-at-two-hospitals-1",
    "href": "regression.html#drug-trial-at-two-hospitals-1",
    "title": "Linear Regression",
    "section": "Drug Trial at Two Hospitals",
    "text": "Drug Trial at Two Hospitals\nprint(drug.groupby('hospital').mean().to_markdown())\n\n\n\nhospital\ntreatment\nseverity\ndays\n\n\n\n\n0\n0.0689655\n7.94499\n29.6207\n\n\n1\n0.941176\n19.758\n49.2157\n\n\n\n\nTreatment randomly assigned within each hospital, but with very different \\(P(T=1|\\text{hospital})\\)"
  },
  {
    "objectID": "regression.html#drug-trial-at-two-hospitals-2",
    "href": "regression.html#drug-trial-at-two-hospitals-2",
    "title": "Linear Regression",
    "section": "Drug Trial at Two Hospitals",
    "text": "Drug Trial at Two Hospitals\n\n\nCode\nmodels = [\n    smf.ols(\"days ~ treatment\", data=drug).fit(cov_type='HC0'),\n    smf.ols(\"days ~ treatment\", data=drug.query(\"hospital==0\")).fit(cov_type='HC0'),\n    smf.ols(\"days ~ treatment\", data=drug.query(\"hospital==1\")).fit(cov_type='HC0'),\n    smf.ols(\"days ~ treatment + severity \", data=drug).fit(cov_type='HC0'),\n    smf.ols(\"days ~ treatment + severity + hospital\", data=drug).fit(cov_type='HC0'),\n    ]\nsummary_col(models, model_names=['all','hosp 0', 'hosp 1', 'all', 'all'])\n\n\n\n\n\n\nall I\nhosp 0 I\nhosp 1 I\nall II\nall III\n\n\nIntercept\n33.2667\n30.4074\n59.0000\n11.6641\n11.0111\n\n\n\n(3.0510)\n(2.8561)\n(4.9889)\n(2.1262)\n(2.1632)\n\n\ntreatment\n14.1533\n-11.4074\n-10.3958\n-7.5912\n-5.0945\n\n\n\n(3.5481)\n(4.5450)\n(5.2627)\n(2.5753)\n(4.0915)\n\n\nseverity\n\n\n\n2.2741\n2.3865\n\n\n\n\n\n\n(0.1910)\n(0.1961)\n\n\nhospital\n\n\n\n\n-4.1535\n\n\n\n\n\n\n\n(4.3465)\n\n\nR-squared\n0.1847\n0.0388\n0.0436\n0.7878\n0.7902\n\n\nR-squared Adj.\n0.1743\n0.0032\n0.0241\n0.7823\n0.7820\n\n\n\n\nStandard errors in parentheses."
  },
  {
    "objectID": "regression.html#drug-trial-at-two-hospitals-3",
    "href": "regression.html#drug-trial-at-two-hospitals-3",
    "title": "Linear Regression",
    "section": "Drug Trial at Two Hospitals",
    "text": "Drug Trial at Two Hospitals\n\nBivariate regression with all observations on treatment has wrong sign\n\nHospital 1 has higher severity which increases days, but also higher P(treatment)\nIgnoring interaction of severity and days leads to sign reversal\n\nComparing “all II” and “all III”, more controls does not always decrease SE"
  },
  {
    "objectID": "regression.html#drug-trial-at-two-hospitals-4",
    "href": "regression.html#drug-trial-at-two-hospitals-4",
    "title": "Linear Regression",
    "section": "Drug Trial at Two Hospitals",
    "text": "Drug Trial at Two Hospitals\n\ndrug['severity_c'] = drug['severity'] - drug['severity'].mean()\ndrug['hospital_c'] = drug['hospital'] - drug['hospital'].mean()\ndrug['hs_c'] = drug['severity']*drug['hospital'] - np.mean(drug['severity']*drug['hospital'])\nmodels = [\n    smf.ols(\"days ~ treatment*severity_c\", data=drug).fit(cov_type='HC0'),\n    smf.ols(\"days ~ treatment*hospital_c\", data=drug).fit(cov_type='HC0'),\n    smf.ols(\"days ~ treatment*(hospital_c + severity_c + hs_c)\", data=drug).fit(cov_type='HC0')\n]\nsummary_col(models, model_names=['I','II','III'])"
  },
  {
    "objectID": "regression.html#drug-trial-at-two-hospitals-5",
    "href": "regression.html#drug-trial-at-two-hospitals-5",
    "title": "Linear Regression",
    "section": "Drug Trial at Two Hospitals",
    "text": "Drug Trial at Two Hospitals\n\nWhat is the best estimator here?\n\nWe will explore more in matching"
  },
  {
    "objectID": "regression.html#sources-and-futher-reading",
    "href": "regression.html#sources-and-futher-reading",
    "title": "Linear Regression",
    "section": "Sources and Futher Reading",
    "text": "Sources and Futher Reading\n\nChapters 5, 6 , and 7 of Facure (2022)\nChernozhukov et al. (2024) chapters 1 and 2\nThe Effect: Chapter 13 - Regression Huntington-Klein (2021)"
  },
  {
    "objectID": "regression.html#references",
    "href": "regression.html#references",
    "title": "Linear Regression",
    "section": "References",
    "text": "References\n\n\n\n\nChernozhukov, V., C. Hansen, N. Kallus, M. Spindler, and V. Syrgkanis. 2024. Applied Causal Inference Powered by ML and AI. https://causalml-book.org/.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nHeckman, James J. 1976. “The Common Structure of Statistical Models of Truncation, Sample Selection and Limited Dependent Variables and a Simple Estimator for Such Models.” Annals of Economic and Social Measurement 5: 475–92. https://api.semanticscholar.org/CorpusID:117935755.\n\n\n———. 1979. “Sample Selection Bias as a Specification Error.” Econometrica.\n\n\nHernán, Miguel A, Sonia Hernández-Dı́az, and James M Robins. 2004. “A Structural Approach to Selection Bias.” Epidemiology 15 (5): 615–25.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. https://theeffectbook.net/."
  },
  {
    "objectID": "regression.html#ate",
    "href": "regression.html#ate",
    "title": "Linear Regression",
    "section": "ATE",
    "text": "ATE\n\n\nCode\ndef ATE(data, treatment='treat', y='infected'):\n    means=data.groupby(treatment)[y].mean()\n    ATE = means[1] - means[0]\n    se = sum(data.groupby(treatment)[y].var()/data.groupby(treatment)[y].count())**0.5\n    return ATE, se\n\nate=ATE(data.loc[data['over65']==1,:])\nprint(f\"difference in means = {ate[0]}, SE = {ate[1]}\")\n\n\ndifference in means = -0.0044244682964888074, SE = 0.0010976149240857532\n\n\n\nRegression estimate \\[\ny_i = \\beta_0 + \\beta_1 T_i + \\epsilon_i\n\\]\n\n\\(\\Er[Y_i\\mid T_i] = \\beta_0 + \\beta_1 T_i + \\epsilon_i\\) \\[\n\\begin{align*}\nATE = & \\Er[Y_i\\mid T_i=1] - E[Y_i\\mid T_i=0] \\\\\n= & (\\beta_0 + \\beta_1*1)-(\\beta_0 + \\beta_1 * 0) \\\\\n= & \\beta_1\n\\end{align*}\n\\]\n\n\n\nreg=smf.ols('infected ~ treat', data=data.loc[data['over65']==1,:]).fit()\nprint(f\"regression estimate={reg.params[1]:3}, se={reg.bse[1]:5}\")\n\nregression estimate=-0.004424468296488705, se=0.0011004170544555998"
  },
  {
    "objectID": "iv.html#sources-and-further-reading",
    "href": "iv.html#sources-and-further-reading",
    "title": "Instrumental Variables",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nChapter 8 and 9 of Facure (2022)\nChapters 6, 12, and 13 of Chernozhukov et al. (2024)\nThe Effect: Chapter 19 - Instrumental Variables Huntington-Klein (2021)\n\n\n\n\n\nAngrist, Joshua D., and Alan B. Krueger. 1991. “Does Compulsory School Attendance Affect Schooling and Earnings?” The Quarterly Journal of Economics 106 (4): pp. 979–1014. http://www.jstor.org/stable/2937954.\n\n\nChernozhukov, V., C. Hansen, N. Kallus, M. Spindler, and V. Syrgkanis. 2024. Applied Causal Inference Powered by ML and AI. https://causalml-book.org/.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. https://theeffectbook.net/.\n\n\nLondschien, Malte, and Peter Bühlmann. 2024. “Weak-Instrument-Robust Subvector Inference in Instrumental Variables Regression: A Subvector Lagrange Multiplier Test and Properties of Subvector Anderson-Rubin Confidence Sets.” https://arxiv.org/abs/2407.15256."
  },
  {
    "objectID": "uncertainty.html#simulated-illustration-of-berry-esseen-clt",
    "href": "uncertainty.html#simulated-illustration-of-berry-esseen-clt",
    "title": "Uncertainty Quantification",
    "section": "Simulated Illustration of Berry-Esseen CLT",
    "text": "Simulated Illustration of Berry-Esseen CLT\n\nimport seaborn as sns\ndef dgp(n, xhi=2):\n    p = 1/(1+xhi**2)\n    xlo = -p*xhi/(1-p)\n    hi = np.random.uniform(0,1,n)&lt;p # so var(x)=1\n    x = np.where(hi, xhi, xlo)\n    return x\n\nprint(f\"var(x) is about {np.var(dgp(100_000)):.2}\")\n\ndef Ex3(xhi):\n    p = 1/(1+xhi**2)\n    xlo = -p*xhi/(1-p)\n    return p*xhi**3 + (1-p)*-xlo**3\n\ne3 = np.mean(np.pow(np.abs(dgp(100_000)),3))\nprint(f\"E[|x|^3] is about {e3:.2}, exact is {Ex3(5):.2}\")\n\ndef plotcdfwithbounds(dgp,  e3, n=[10,100,1000], S=9999):\n    cmap = sns.color_palette(\"tab10\")\n    x = np.linspace(-4.5, 4.5, num=200)\n    cdfx= scipy.stats.norm().cdf(x)\n    plt.plot(x, cdfx, label=\"Normal\", color=\"black\", linestyle=\"dashed\")\n    for i in range(len(n)):\n        truedist = [dgp(n[i]).mean()*np.sqrt(n[i]) for _ in range(S)]\n        sns.ecdfplot(truedist, label=f\"n={n[i]}\", color=cmap[i])\n        plt.fill_between(x, cdfx - 0.5*e3/np.sqrt(n[i]), cdfx + 0.5*e3/np.sqrt(n[i]), alpha=0.2, color=cmap[i])\n    plt.legend()\n    plt.xlim(-2.5,2.5)\n    plt.ylim(0,1)\n    plt.title(\"Distribution of Scaled Sample Mean\")\n    return(plt.gcf())\n\nfig=plotcdfwithbounds(dgp, e3)\nfig.show()\n\n\n\nThis particular DGP of a skewed distribution with two points of support is the worst in terms of getting close to the bound. Esseen showed the best error is greater than \\(0.4 \\Er[|X|^3]/\\sqrt{n}\\) for a similar distribution."
  },
  {
    "objectID": "uncertainty.html#bootstrap-3",
    "href": "uncertainty.html#bootstrap-3",
    "title": "Uncertainty Quantification",
    "section": "Bootstrap",
    "text": "Bootstrap\n\nError of the bootstrap is the same size as error of CLT except in special circumstances\nBootstrap has smaller error if used for a statistic whose asymptotic distribution is known\n\nE.g. \\[\nz = \\sqrt{n}(\\bar{x} - \\Er[x]) \\leadsto N(0, \\var(x))\n\\] is not pivotal because limiting distribution depends on \\(\\var(x)\\)\nE.g. \\[\nt = \\sqrt{n}(\\bar{x} - \\Er[x])/\\hat{\\sigma}_x \\leadsto N(0, 1)\n\\] is pivotal"
  },
  {
    "objectID": "uncertainty.html#bootstrap-for-pivotal-statistic",
    "href": "uncertainty.html#bootstrap-for-pivotal-statistic",
    "title": "Uncertainty Quantification",
    "section": "Bootstrap for Pivotal Statistic",
    "text": "Bootstrap for Pivotal Statistic\n\nnp.random.seed(6987)\ndef estimator(data) :\n    return np.mean(data), np.std(data)/np.sqrt(len(data))\n\ndef tbootstrap(data, estimator, B=999):\n    n = len(data)\n    bestimates = np.zeros(B)\n    est = estimator(data)\n    for b in range(B):\n        eb = estimator(np.random.choice(data,size=n, replace=True))\n        bestimates[b] = (eb[0]-est[0])/eb[1]\n    return bestimates\n\ncmap = sns.color_palette(\"tab10\")\ntrue = estimator(dgp(n*10_000))\n# simulate true distribution of estimator\nestsims = np.zeros(S)\nfor s in range(S):\n    m, sd = estimator(dgp(n))\n    estsims[s] = (m-true[0])/sd\n\nsns.ecdfplot(estsims, label=\"True distribution\", linewidth=3, color=\"black\")\nfor b in range(3):\n    data = dgp(n)\n    sns.ecdfplot(tbootstrap(data, estimator, B=B), alpha=0.5, label=f\"Pivotal Bootstrap {b}\", color=cmap[b])\n\nx = np.linspace(-4.5, 4.5, num=200)\ncdfx= scipy.stats.norm().cdf(x)\nplt.plot(x, cdfx, label=\"Normal\", color=\"black\", linestyle=\"dashed\")\n\nsd=np.std(estsims)\nplt.ylim(0,1)\nplt.xlim(-2.5*sd,2.5*sd)\nplt.legend()\nplt.title(f\"True CDF and Approximate CDFs for t-stat for three data realizations with n={n}\")\nplt.show()"
  },
  {
    "objectID": "uncertainty.html#simulated-illustration-of-berry-esseen-clt-output",
    "href": "uncertainty.html#simulated-illustration-of-berry-esseen-clt-output",
    "title": "Uncertainty Quantification",
    "section": "Simulated Illustration of Berry-Esseen CLT",
    "text": "Simulated Illustration of Berry-Esseen CLT\n\nvar(x) is about 1.0\nE[|x|^3] is about 1.7, exact is 4.8"
  },
  {
    "objectID": "uncertainty.html#bootstrap-for-pivotal-statistic-output",
    "href": "uncertainty.html#bootstrap-for-pivotal-statistic-output",
    "title": "Uncertainty Quantification",
    "section": "Bootstrap for Pivotal Statistic",
    "text": "Bootstrap for Pivotal Statistic"
  },
  {
    "objectID": "regression.html#drug-trial-at-two-hospitals-4-output",
    "href": "regression.html#drug-trial-at-two-hospitals-4-output",
    "title": "Linear Regression",
    "section": "Drug Trial at Two Hospitals",
    "text": "Drug Trial at Two Hospitals\n\n\n\n\n\nI\nII\nIII\n\n\nIntercept\n46.6189\n48.6352\n46.8291\n\n\n\n(2.4925)\n(3.3447)\n(3.2836)\n\n\ntreatment\n-7.5209\n-10.7625\n-4.5251\n\n\n\n(2.6555)\n(3.7377)\n(3.3474)\n\n\nseverity_c\n2.2342\n\n2.7264\n\n\n\n(0.2919)\n\n(0.2508)\n\n\ntreatment:severity_c\n0.0867\n\n6.0732\n\n\n\n(0.3601)\n\n(0.2508)\n\n\nhospital_c\n\n28.5926\n29.3104\n\n\n\n\n(5.7486)\n(14.2210)\n\n\ntreatment:hospital_c\n\n1.0116\n13.1092\n\n\n\n\n(6.9536)\n(15.0690)\n\n\nhs_c\n\n\n-1.7888\n\n\n\n\n\n(0.5150)\n\n\ntreatment:hs_c\n\n\n-4.6814\n\n\n\n\n\n(0.5822)\n\n\nR-squared\n0.7880\n0.3760\n0.8075\n\n\nR-squared Adj.\n0.7796\n0.3514\n0.7887\n\n\n\n\nStandard errors in parentheses."
  },
  {
    "objectID": "regression.html#gender-wage-gap-with-more-conditioning",
    "href": "regression.html#gender-wage-gap-with-more-conditioning",
    "title": "Linear Regression",
    "section": "Gender Wage Gap with More Conditioning",
    "text": "Gender Wage Gap with More Conditioning\n\nimport pyfixest as pf\n\ncontrols=\"age + I(age**2) | race + grade92 + unionmme + unioncov +  ind17 + occ18\"\nallcon=pf.feols(\"log_earn ~ female*(wasmarried + married) + log_hourslw + log_uhours + \" + controls, data=cps,vcov='hetero')\nallcon.summary()\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n###\n\nEstimation:  OLS\nDep. var.: log_earn, Fixed effects: race+grade92+unionmme+unioncov+ind17+occ18\nInference:  hetero\nObservations:  104607\n\n| Coefficient       |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:------------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| female            |     -0.084 |        0.006 |   -14.742 |      0.000 | -0.095 |  -0.073 |\n| wasmarried        |      0.082 |        0.009 |     9.570 |      0.000 |  0.065 |   0.099 |\n| married           |      0.126 |        0.005 |    23.207 |      0.000 |  0.115 |   0.137 |\n| log_hourslw       |      0.044 |        0.007 |     6.649 |      0.000 |  0.031 |   0.058 |\n| log_uhours        |      0.988 |        0.011 |    93.671 |      0.000 |  0.967 |   1.009 |\n| age               |      0.003 |        0.000 |    19.767 |      0.000 |  0.003 |   0.003 |\n| I(age ** 2)       |     -0.000 |        0.000 |    -1.174 |      0.240 | -0.000 |   0.000 |\n| female:wasmarried |     -0.059 |        0.011 |    -5.507 |      0.000 | -0.080 |  -0.038 |\n| female:married    |     -0.069 |        0.007 |    -9.734 |      0.000 | -0.083 |  -0.055 |\n---\nRMSE: 0.524 R2: 0.582 R2 Within: 0.314"
  },
  {
    "objectID": "matching.html#example-choosing-flexible-estimators",
    "href": "matching.html#example-choosing-flexible-estimators",
    "title": "Matching",
    "section": "Example: choosing flexible estimators",
    "text": "Example: choosing flexible estimators\n\n\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.linear_model import LassoCV, LogisticRegressionCV, ElasticNetCV\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.kernel_approximation import RBFSampler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\n\ngamma = 0.5\nnc = 100\nmodels = [\n    Pipeline([('rbf',RBFSampler(gamma=gamma, random_state=1, n_components=nc)),\n              ('lasso',LassoCV(max_iter=10_000, tol=1e-2, n_jobs=-1, selection='random',\n                               alphas=10**np.linspace(-3,3,20)))]),\n    Pipeline([('poly',PolynomialFeatures(degree=10)),\n              ('scale', StandardScaler()),\n              ('lasso',LassoCV(max_iter=10_000, n_jobs=-1, selection='random', tol=1e-2,\n                               alphas=10**np.linspace(-3,3,20)))]),\n    GradientBoostingRegressor(learning_rate=0.1, criterion=\"squared_error\")]\n\nfor m in models :\n    print(m)\n    print(f\"cross val R-squared {cross_val_score(m, X, y, cv=5).mean():.4} \\n\\n\")\n\n\nPipeline(steps=[('rbf', RBFSampler(gamma=0.5, random_state=1)),\n                ('lasso',\n                 LassoCV(alphas=array([1.00000000e-03, 2.06913808e-03, 4.28133240e-03, 8.85866790e-03,\n       1.83298071e-02, 3.79269019e-02, 7.84759970e-02, 1.62377674e-01,\n       3.35981829e-01, 6.95192796e-01, 1.43844989e+00, 2.97635144e+00,\n       6.15848211e+00, 1.27427499e+01, 2.63665090e+01, 5.45559478e+01,\n       1.12883789e+02, 2.33572147e+02, 4.83293024e+02, 1.00000000e+03]),\n                         max_iter=10000, n_jobs=-1, selection='random',\n                         tol=0.01))])\ncross val R-squared 0.4365 \n\n\nPipeline(steps=[('poly', PolynomialFeatures(degree=10)),\n                ('scale', StandardScaler()),\n                ('lasso',\n                 LassoCV(alphas=array([1.00000000e-03, 2.06913808e-03, 4.28133240e-03, 8.85866790e-03,\n       1.83298071e-02, 3.79269019e-02, 7.84759970e-02, 1.62377674e-01,\n       3.35981829e-01, 6.95192796e-01, 1.43844989e+00, 2.97635144e+00,\n       6.15848211e+00, 1.27427499e+01, 2.63665090e+01, 5.45559478e+01,\n       1.12883789e+02, 2.33572147e+02, 4.83293024e+02, 1.00000000e+03]),\n                         max_iter=10000, n_jobs=-1, selection='random',\n                         tol=0.01))])\ncross val R-squared 0.5392 \n\n\nGradientBoostingRegressor(criterion='squared_error')\ncross val R-squared 0.5006"
  },
  {
    "objectID": "matching.html#example-choosing-flexible-estimators-1",
    "href": "matching.html#example-choosing-flexible-estimators-1",
    "title": "Matching",
    "section": "Example: choosing flexible estimators",
    "text": "Example: choosing flexible estimators\n\n\ngamma = 0.5\nnc = 100\nmodels = [\n    Pipeline([('rbf',RBFSampler(gamma=gamma, random_state=1, n_components=nc)),\n              ('logistic',LogisticRegressionCV(n_jobs=-1, scoring='neg_log_loss'))]),\n    Pipeline([('poly',PolynomialFeatures(degree=10)),\n              ('scale', StandardScaler()),\n              ('logistic',LogisticRegressionCV(n_jobs=-1, scoring='neg_log_loss'))]),\n    GradientBoostingClassifier(learning_rate=0.1)]\nfor m in models :\n    print(m)\n    print(f\"cross val log likelihood {cross_val_score(m, X, T, scoring='neg_log_loss', cv=5).mean():.4} \\n\\n\")\n\n\nPipeline(steps=[('rbf', RBFSampler(gamma=0.5, random_state=1)),\n                ('logistic',\n                 LogisticRegressionCV(n_jobs=-1, scoring='neg_log_loss'))])\ncross val log likelihood -0.5077 \n\n\nPipeline(steps=[('poly', PolynomialFeatures(degree=10)),\n                ('scale', StandardScaler()),\n                ('logistic',\n                 LogisticRegressionCV(n_jobs=-1, scoring='neg_log_loss'))])\ncross val log likelihood -0.5098 \n\n\nGradientBoostingClassifier()\ncross val log likelihood -0.5184"
  },
  {
    "objectID": "matching.html#example-checking-overlap",
    "href": "matching.html#example-checking-overlap",
    "title": "Matching",
    "section": "Example: Checking Overlap",
    "text": "Example: Checking Overlap\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport doubleml as dml\ng = Pipeline([('poly',PolynomialFeatures(degree=10)),\n              ('scale', StandardScaler()),\n              ('lasso',LassoCV(max_iter=10_000, n_jobs=-1, selection='random', tol=1e-2,\n                               alphas=10**np.linspace(-3,3,20)))])\nm = Pipeline([('rbf',RBFSampler(gamma=gamma, random_state=1, n_components=nc)),\n              ('logistic',LogisticRegressionCV(n_jobs=-1, scoring='neg_log_loss'))])\n\nm.fit(X,T)\n\ndef plotp(model,X,d):\n    fig,ax=plt.subplots()\n    p = model.predict_proba(X)\n    sns.histplot(p[d==0,1], kde = False,\n                 label = \"Untreated\", ax=ax)\n    sns.histplot(p[d==1,1], kde = False,\n                 label = \"Treated\", ax=ax)\n    ax.set_title('P(T|x)')\n    return(fig)\n\nfig=plotp(m, X, T)"
  },
  {
    "objectID": "matching.html#example-checking-overlap-output",
    "href": "matching.html#example-checking-overlap-output",
    "title": "Matching",
    "section": "Example: Checking Overlap",
    "text": "Example: Checking Overlap"
  },
  {
    "objectID": "matching.html#example-results",
    "href": "matching.html#example-results",
    "title": "Matching",
    "section": "Example: Results",
    "text": "Example: Results\n\ndmldata = dml.DoubleMLData.from_arrays(X, y, T)\ndmlATE = dml.DoubleMLAPOS(dmldata, g, m, treatment_levels=[0, 1])\ndmlATE.fit()\ndmlATE.summary\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n2.5 %\n97.5 %\n\n\n\n\n0\n-0.021252\n0.030485\n-0.697120\n0.485728\n-0.081001\n0.038497\n\n\n1\n0.131497\n0.038623\n3.404609\n0.000663\n0.055797\n0.207197\n\n\n\n\n\n\n\n\ndmlATE.causal_contrast(0).summary\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n2.5 %\n97.5 %\n\n\n\n\n1 vs 0\n0.152749\n0.049476\n3.087356\n0.002019\n0.055778\n0.249719\n\n\n\n\n\n\n\n\naipw = Ey1x - Ey0x + T*(y - Ey1x)/P - (1-T)*(y - Ey0x)/(1-P)\nse = np.sqrt(np.var(aipw)/len(aipw))\nprint(f\"Infeasible estimator with true E[y1-y0|x] = and p(x) {np.mean(aipw):.2} with 95% CI = [{np.mean(aipw)-1.96*se:.2},{np.mean(aipw)+1.96*se:.2}]\\n\")\n\nInfeasible estimator with true E[y1-y0|x] = and p(x) 0.15 with 95% CI = [0.057,0.24]"
  },
  {
    "objectID": "matching.html#visualizing-fit",
    "href": "matching.html#visualizing-fit",
    "title": "Matching",
    "section": "Visualizing Fit",
    "text": "Visualizing Fit\n\nfrom sklearn.model_selection import train_test_split\nnames = [\"lasso-rbf\", \"lasso-poly\", \"gbforest\"]\nX_train,  X_test,y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=5498)\nfig, ax = plt.subplots(1,3, figsize=(12,5))\nax = ax.flatten()\ndef r2(y,yhat) :\n    return(1 - np.mean((y-yhat)**2)/np.var(y))\notr = np.argsort(X_train[:,1])\note = np.argsort(X_test[:,1])\n\nfor i, m in enumerate(models):\n    m.fit(X_train,y=y_train)\n    yhat = m.predict(X_train)\n    yhat_test = m.predict(X_test)\n    ax[i].scatter(X_train[:,1], y_train, alpha=0.3, label=\"Train y\", color=\"C0\", marker=\".\")\n    ax[i].plot(X_train[otr,1], yhat[otr], alpha=0.5, label=\"Train yhat\", color=\"C0\")\n    ax[i].scatter(X_test[:,1], y_test, alpha=0.3, label=\"Test y\", color=\"C1\", marker=\".\")\n    ax[i].plot(X_test[ote,1], yhat_test[ote], alpha=0.5, label=\"Test yhat\", color=\"C1\")\n    ax[i].set_xlabel(\"x\")\n    ax[i].set_ylabel(\"y\")\n    ax[i].set_title(names[i])\n    ax[i].annotate(f\"Train R-squared = {r2(y_train, yhat):.2}\", (0.05, 0.95), xycoords='axes fraction')\n    ax[i].annotate(f\"Test R-squared = {r2(y_test, yhat_test):.2}\", (0.05, 0.90), xycoords='axes fraction')\n    ax[i].legend()\n\nplt.show()"
  },
  {
    "objectID": "matching.html#visualizing-fit-output",
    "href": "matching.html#visualizing-fit-output",
    "title": "Matching",
    "section": "Visualizing Fit",
    "text": "Visualizing Fit"
  },
  {
    "objectID": "moredid.html#using-a-package",
    "href": "moredid.html#using-a-package",
    "title": "Difference in Differences II",
    "section": "Using a package",
    "text": "Using a package\n\n# check that it matches fixed effect estimate from a package\nimport pyfixest as pf\n\ndf = pd.DataFrame({\n    'id': np.repeat(np.arange(1, n + 1), T),\n    't': np.tile(np.arange(1, T + 1), n),\n    'y': y.flatten(),\n    'D': D.flatten()\n})\nresult=pf.feols('y ~ D | id + t', df, vcov={\"CRV1\": \"id\"})\nresult.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: y, Fixed effects: id+t\nInference:  CRV1\nObservations:  900\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| D             |      0.081 |        0.133 |     0.610 |      0.543 | -0.183 |   0.346 |\n---\nRMSE: 0.953 R2: 0.109 R2 Within: 0.0"
  },
  {
    "objectID": "fe.html#what-fixed-effects-cannot-fix",
    "href": "fe.html#what-fixed-effects-cannot-fix",
    "title": "Fixed Effects",
    "section": "What Fixed Effects Cannot Fix",
    "text": "What Fixed Effects Cannot Fix\n\nFixed effects corrects for time-invariant unobserved confounders, but\n\n\n\nCannot fix:\n\nTime-varying confounders\nReverse causality\nCan make some biases worse if “between” \\(i\\) variation in \\(X_{it}\\) is useful (e.g. measurement error)"
  },
  {
    "objectID": "moredid.html#what-to-do-3",
    "href": "moredid.html#what-to-do-3",
    "title": "Difference in Differences II",
    "section": "What to Do?",
    "text": "What to Do?\n\nCohorts = unique sequences of \\((D_{i1}, ..., D_{iT})\\)\n\nIn current simulated example, three cohorts\n\n\\((0, 0, 0, 0, 0, 0, 0, 0, 0)\\)\n\\((0, 0, 0, 0, 0, 0, 0, 1, 1)\\)\n\\((0, 1, 1, 1, 1, 1, 1, 1, 1)\\)"
  },
  {
    "objectID": "moredid.html#regression-with-cohort-interactions",
    "href": "moredid.html#regression-with-cohort-interactions",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort Interactions",
    "text": "Regression with Cohort Interactions\n\ndef definecohort(df):\n    # convert dummies into categorical\n    n = len(df.id.unique())\n    T = len(df.t.unique())\n    df = df.set_index(['id','t'])\n    dmat=np.array(df.sort_index().D)\n    dmat=np.array(df.D).reshape(n,T)\n    cohort=dmat.dot(1 &lt;&lt; np.arange(dmat.shape[-1] - 1, -1, -1))\n    cdf = pd.DataFrame({\"id\":np.array(df.index.levels[0]), \"cohort\":pd.Categorical(cohort)})\n    cdf =cdf.set_index('id')\n    df = df.reset_index().set_index('id')\n    df=pd.merge(df, cdf, left_index=True, right_index=True)\n    df=df.reset_index()\n    return(df)\n\ndfc = definecohort(df)\n\ndef defineinteractions(df):\n    df['dct'] = 'untreated'\n    df['dct'] = df.apply(lambda x: f\"t{x['t']},c{x['cohort']}\" if x['D'] else f\"untreated\", axis=1)\n    return(df)\n\ndfc = defineinteractions(dfc)\n\nmodc = pf.feols(\"y ~  C(dct, Treatment('untreated')) | id + t\", dfc, vcov={\"CRV1\": \"id\"})\npf.etable([modc], type=\"md\")"
  },
  {
    "objectID": "moredid.html#regression-with-cohort-interactions-output",
    "href": "moredid.html#regression-with-cohort-interactions-output",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort Interactions",
    "text": "Regression with Cohort Interactions\n\nindex                                           est1\n-----------------------------------------  ---------\ndepvar                                             y\n----------------------------------------------------\nC(dct, Treatment('untreated'))[T.t2,c255]     0.524\n                                             (0.277)\nC(dct, Treatment('untreated'))[T.t3,c255]    0.761*\n                                             (0.320)\nC(dct, Treatment('untreated'))[T.t4,c255]   0.820**\n                                             (0.304)\nC(dct, Treatment('untreated'))[T.t5,c255]   0.880**\n                                             (0.329)\nC(dct, Treatment('untreated'))[T.t6,c255]     0.458\n                                             (0.285)\nC(dct, Treatment('untreated'))[T.t7,c255]    0.624*\n                                             (0.307)\nC(dct, Treatment('untreated'))[T.t8,c255]  5.242***\n                                             (0.408)\nC(dct, Treatment('untreated'))[T.t8,c3]    1.121***\n                                             (0.311)\nC(dct, Treatment('untreated'))[T.t9,c255]  5.693***\n                                             (0.354)\nC(dct, Treatment('untreated'))[T.t9,c3]    1.007***\n                                             (0.286)\n----------------------------------------------------\nt                                                  x\nid                                                 x\n----------------------------------------------------\nObservations                                     900\nS.E. type                                     by: id\nR2                                             0.724\n----------------------------------------------------"
  },
  {
    "objectID": "moredid.html#regression-with-cohort-interactions-1",
    "href": "moredid.html#regression-with-cohort-interactions-1",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort Interactions",
    "text": "Regression with Cohort Interactions\n\nimport re\ndef plotcohortatt(modc):\n    coef = modc.coef()\n    ci = modc.confint()\n    tcregex = re.compile(r\".+t(\\d+),c(\\d+)]\")\n    catt = pd.DataFrame(index=coef.index, columns=['t','c','att','yerr'])\n    for i in range(len(coef)):\n        m = tcregex.match(coef.index[i])\n        t,c = m.groups()\n        t=int(t)\n        c=int(c)\n        catt.loc[coef.index[i]] = [t,c,coef.iloc[i],np.abs(ci.iloc[i][0]-coef.iloc[i])]\n\n    catt.sort_values(['c','t'],inplace=True)\n    fig, ax = plt.subplots()\n    ax.set(xlabel='time', ylabel='ATT | cohort')\n    for g in catt.groupby('c') :\n        c = g[0]\n        g = g[1]\n        ax.errorbar(g['t'], g['att'], yerr=g['yerr'], fmt='o', label=f'cohort {c}')\n\n    ax.legend()\n    return(fig)\n\nfig=plotcohortatt(modc)"
  },
  {
    "objectID": "moredid.html#regression-with-cohort-interactions-1-output",
    "href": "moredid.html#regression-with-cohort-interactions-1-output",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort Interactions",
    "text": "Regression with Cohort Interactions"
  },
  {
    "objectID": "moredid.html#pre-trends-2",
    "href": "moredid.html#pre-trends-2",
    "title": "Difference in Differences II",
    "section": "Pre-trends",
    "text": "Pre-trends\n\ndef eventstudyplot(modct) :\n    evdf = pd.DataFrame(index=modct.coef().index, columns=['c', 't','delta','did','se','vindex'])\n    pattern=r\".+T\\.(\\d+).+\\[T\\.(\\d+)\\]\"\n    for i in range(len(modct.coef())):\n        m = re.match(pattern, modct.coef().index[i])\n        c,t = m.groups()\n        t = int(t)\n        c = int(c)\n        evdf.loc[modct.coef().index[i]] = [c,t,modct.coef().iloc[i],0, 0, i]\n\n    evdf.sort_values(['c','t'],inplace=True)\n    T = evdf['t'].max()\n    V = modct._vcov\n    evdf.reset_index(inplace=True)\n    for t in range(T) :\n        if not (t+1) in evdf.t.unique() :\n            ndf = pd.DataFrame(columns=evdf.columns)\n            ndf.c=evdf.c.unique()\n            ndf.t=t+1\n            ndf.delta=0\n            ndf.vindex=V.shape[0]\n            evdf=pd.concat([evdf,ndf])\n    V = np.vstack([V,np.zeros(V.shape[1])])\n    V = np.hstack([V,np.zeros((V.shape[0],1))])\n\n    def did(g) :\n        c = g.c.unique()[0]\n        timestreated = bin(c).count('1')\n        t0 = T - timestreated\n        g=g.set_index('t')\n        g['did'] = g['delta'] - g['delta'][t0]\n        vi = g['vindex'].to_numpy().astype(int)\n        g['se'] = np.sqrt(V[vi,vi] - 2*V[g['vindex'][t0],vi] + V[g['vindex'][t0],g['vindex'][t0]])\n        return(g)\n\n    evdf=evdf.groupby('c').apply(did).drop(columns='c').reset_index()\n\n    fig, ax = plt.subplots(len(evdf.c.unique()),1)\n    for (i,g) in enumerate(evdf.groupby('c')) :\n        c = g[0]\n        g = g[1]\n        timestreated = bin(c).count('1')\n        t0 = T - timestreated + 1\n        ax[i].axvline(t0, color='black', ls=\":\")\n        ax[i].errorbar(g['t'], g['did'], yerr=1.96*g['se'], fmt='o', label=f'cohort {c}')\n        ax[i].set_title(f'cohort {c}')\n        ax[i].set(xlabel='time', ylabel='ATT | cohort')\n\n    return(fig)\n\nfig=eventstudyplot(modct)"
  },
  {
    "objectID": "moredid.html#regression-with-cohort-time-interactions-3",
    "href": "moredid.html#regression-with-cohort-time-interactions-3",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort-Time Interactions",
    "text": "Regression with Cohort-Time Interactions\n\nmodct = pf.feols(\"y ~ C(cohort)*C(t) | id + t\",dfc, vcov={\"CRV1\":\"id\"})\npf.etable(modct, type='md')\n\nindex                            est1\n--------------------------  ---------\ndepvar                              y\n-------------------------------------\nC(cohort)[T.3]:C(t)[T.2]       0.288\n                              (0.355)\nC(cohort)[T.255]:C(t)[T.2]    0.696*\n                              (0.338)\nC(cohort)[T.3]:C(t)[T.3]       0.248\n                              (0.404)\nC(cohort)[T.255]:C(t)[T.3]    0.909*\n                              (0.404)\nC(cohort)[T.3]:C(t)[T.4]       0.386\n                              (0.343)\nC(cohort)[T.255]:C(t)[T.4]   1.051**\n                              (0.350)\nC(cohort)[T.3]:C(t)[T.5]      -0.119\n                              (0.375)\nC(cohort)[T.255]:C(t)[T.5]    0.809*\n                              (0.405)\nC(cohort)[T.3]:C(t)[T.6]       0.480\n                              (0.291)\nC(cohort)[T.255]:C(t)[T.6]    0.745*\n                              (0.338)\nC(cohort)[T.3]:C(t)[T.7]       0.309\n                              (0.360)\nC(cohort)[T.255]:C(t)[T.7]    0.808*\n                              (0.382)\nC(cohort)[T.3]:C(t)[T.8]     1.349**\n                              (0.398)\nC(cohort)[T.255]:C(t)[T.8]  5.378***\n                              (0.438)\nC(cohort)[T.3]:C(t)[T.9]     1.235**\n                              (0.383)\nC(cohort)[T.255]:C(t)[T.9]  5.829***\n                              (0.383)\n-------------------------------------\nt                                   x\nid                                  x\n-------------------------------------\nObservations                      900\nS.E. type                      by: id\nR2                              0.725\n-------------------------------------"
  },
  {
    "objectID": "moredid.html#pre-trends-2-output",
    "href": "moredid.html#pre-trends-2-output",
    "title": "Difference in Differences II",
    "section": "Pre-trends",
    "text": "Pre-trends"
  },
  {
    "objectID": "iv.html#endogeneity",
    "href": "iv.html#endogeneity",
    "title": "Instrumental Variables",
    "section": "Endogeneity",
    "text": "Endogeneity\n\\[\nY_i = X_i' \\beta_0 + \\epsilon_i\n\\]\n\nBelieve \\(\\Er[X_i \\epsilon_i] \\neq 0\\), so OLS biased and inconsistent"
  },
  {
    "objectID": "iv.html#sources-of-endogeneity",
    "href": "iv.html#sources-of-endogeneity",
    "title": "Instrumental Variables",
    "section": "Sources of Endogeneity",
    "text": "Sources of Endogeneity\n\nOmitted variables\nSimultaneity\nMeasurement error\nAnd more …"
  },
  {
    "objectID": "iv.html#omitted-variables",
    "href": "iv.html#omitted-variables",
    "title": "Instrumental Variables",
    "section": "Omitted Variables",
    "text": "Omitted Variables\n\nDesired model \\[\nY_i = X_i'\\beta_0 + W_i'\\gamma_0 + \\epsilon_i\n\\] Assume \\(\\Er[\\epsilon] = \\Er[X\\epsilon] = \\Er[W\\epsilon] = 0\\)\nEstimated model \\[\nY_i = X_i'\\beta + \\underbrace{u_i}_{\\equiv W_i'\\gamma_0 + \\epsilon_i}\n\\]"
  },
  {
    "objectID": "iv.html#omitted-variables-bias",
    "href": "iv.html#omitted-variables-bias",
    "title": "Instrumental Variables",
    "section": "Omitted Variables Bias",
    "text": "Omitted Variables Bias\n\n\nOLS inconsistent and biased\n\n\\[\n\\plim \\hat{\\beta}^{OLS} \\inprob \\beta_0 + \\Er[X_i X_i']^{-1} \\Er[X_i W_i'] \\gamma_0\n\\]\n\nOr if \\(X_i = (1, x_i)'\\) and \\(W_i\\) is a scalar, \\(y_i = \\beta_0 + \\beta_1 x_i + \\gamma_0 w_i + \\epsilon\\) \\[\n\\plim \\hat{\\beta}_1 \\inprob \\beta_1 + \\frac{\\cov(x_i,w_i)}{\\var(x_i)} \\gamma_0\n\\]"
  },
  {
    "objectID": "iv.html#simultaneity",
    "href": "iv.html#simultaneity",
    "title": "Instrumental Variables",
    "section": "Simultaneity",
    "text": "Simultaneity\n\nEquilibrium conditions lead to variables that are simultaneously determined\nDemand and supply: \\[\n\\begin{align*}\nQ_i^D & = P_i \\beta_D + X_D'\\gamma_D + u_{D,i} \\\\\nQ_i^S & = P_i \\beta_S + X_S'\\gamma_S + u_{S,i} \\\\\nQ_i^S & = Q_i^D\n\\end{align*}\n\\]\nAssume \\(X_D\\) and \\(X_S\\) exogenous\n\n\\(0 = \\Er[X_D u_{D,i}] = \\Er[X_D u_{S,i}] = \\Er[X_S u_{D,i}] = \\Er[X_S u_{S,i}]\\)"
  },
  {
    "objectID": "iv.html#simultaneity-1",
    "href": "iv.html#simultaneity-1",
    "title": "Instrumental Variables",
    "section": "Simultaneity",
    "text": "Simultaneity\n\nStructural equations: (demand and inverse supply): \\[\n\\begin{align*}\nQ_i & = P_i \\beta_D + X_D'\\gamma_D + u_{D,i} \\\\\nP_i & = Q_i \\frac{1}{\\beta_S}  - X_S'\\gamma_D\\frac{1}{\\beta_S} - u_{S,i}\\frac{1}{\\beta_S} \\\\\n\\end{align*}\n\\]\nReduced form: \\[\n\\begin{align*}\nQ_i = & \\frac{\\beta_D}{\\beta_D - \\beta_S} \\left( -X_{D,i}' \\gamma_D + X_{S,i}'\\gamma_S - u_{D,i} + u_{S,i} \\right) + X_{D,i}'\\gamma_D + u_{D,i} \\\\\nP_i = & \\frac{1}{\\beta_D - \\beta_S}\\left(-X_{D,i}' \\gamma_D + X_{S,i}'\\gamma_S - u_{D,i} + u_{S,i} \\right)\n\\end{align*}\n\\]\n\\(\\Er[P_i u_{D,i}] \\neq 0\\), so OLS estimate of \\(Q_i = P_i \\beta_D + X_D'\\gamma_D + u_{D,i}\\) is biased and inconsistent"
  },
  {
    "objectID": "iv.html#measurment-error",
    "href": "iv.html#measurment-error",
    "title": "Instrumental Variables",
    "section": "Measurment Error",
    "text": "Measurment Error\n\nTrue model \\[\nY_i = \\beta_0 + \\beta_1 x_i^* + \\epsilon_i\n\\] with \\(\\Er[x_i^* \\epsilon_i]=0\\), but \\(x_i^*\\) unobserved\nObserve \\(x_i = x_i^* + u_i\\)\nEstimated model \\[\nY_i = \\beta_0 + \\beta_1 x_i + \\underbrace{\\tilde{\\epsilon}_i}_{\\equiv -\\beta_1 u_i + \\epsilon_i}\n\\]\n\\(\\Er[x_i\\tilde{\\epsilon}_i] = -\\beta_1\\Er[u_i^2] \\neq 0\\)"
  },
  {
    "objectID": "iv.html#instrumental-variables-1",
    "href": "iv.html#instrumental-variables-1",
    "title": "Instrumental Variables",
    "section": "Instrumental Variables",
    "text": "Instrumental Variables\n\n\n\\[\nY_i = \\underbrace{X_i}_{\\in \\R^k}' \\beta_0 + \\epsilon_i\n\\]\n\n\\(\\Er[\\epsilon_i] = 0\\), but \\(\\Er[X_i \\epsilon_i] \\neq 0\\)\nInstrument \\(Z_i \\in \\R^d\\) s.t.\n\nRelevant \\(rank(\\Er[Z_i X_i']) = k\\)\nExogenous \\(\\Er[Z_i \\epsilon_i] = 0\\)\n\n\n\n\n\nCode\nimport graphviz as gr\n\ndot = gr.Digraph()\ndot.edge('X', 'Y', style = 'dashed')\ndot.edge('ϵ','Y')\ndot.edge('ϵ','X')\ndot.edge('Z','X')\ndot"
  },
  {
    "objectID": "iv.html#example-returns-to-education-1",
    "href": "iv.html#example-returns-to-education-1",
    "title": "Instrumental Variables",
    "section": "Example: Returns to Education",
    "text": "Example: Returns to Education\n\nFrom Angrist and Krueger (1991)\n\n\n\nCode\nimport graphviz as gr\n\ng = gr.Digraph()\ng.edge('Education', 'Earnings', style = 'dashed')\ng.edge('Ability', 'Education')\ng.edge('Ability', 'Earnings')\ng.edge('?', 'Education')\ng"
  },
  {
    "objectID": "iv.html#example-returns-to-education-2",
    "href": "iv.html#example-returns-to-education-2",
    "title": "Instrumental Variables",
    "section": "Example: Returns to Education",
    "text": "Example: Returns to Education\n\n\n\n\nCode\ng = gr.Digraph()\ng.edge('Education', 'Earnings', style = 'dashed')\ng.edge('Ability', 'Education')\ng.edge('Ability', 'Earnings')\ng.edge('Quarter of Birth', 'Education')\ng\n\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv('data/ak91.csv')\ndf.head()\n\n\n\n\n\n\n\n\nlog_wage\nyears_of_schooling\nyear_of_birth\nquarter_of_birth\nstate_of_birth\n\n\n\n\n0\n5.790019\n12.0\n30.0\n1.0\n45.0\n\n\n1\n5.952494\n11.0\n30.0\n1.0\n45.0\n\n\n2\n5.315949\n12.0\n30.0\n1.0\n45.0\n\n\n3\n5.595926\n12.0\n30.0\n1.0\n45.0\n\n\n4\n6.068915\n12.0\n30.0\n1.0\n37.0"
  },
  {
    "objectID": "iv.html#example-returns-to-education-3",
    "href": "iv.html#example-returns-to-education-3",
    "title": "Instrumental Variables",
    "section": "Example: Returns to Education",
    "text": "Example: Returns to Education\n\nWe must assume that\n\nRelevance: \\(\\cov(QOB, Education) \\neq 0\\)\nExogeneity: \\(Earnings \\perp QOB \\mid Education\\)"
  },
  {
    "objectID": "iv.html#example-returns-to-education-relevance",
    "href": "iv.html#example-returns-to-education-relevance",
    "title": "Instrumental Variables",
    "section": "Example: Returns to Education : Relevance",
    "text": "Example: Returns to Education : Relevance\n\nThe relevance assumption can be checked\n\n\n\nimport matplotlib.pyplot as plt\ngroup_data = (df\n    .groupby([\"year_of_birth\", \"quarter_of_birth\"])\n    [[\"log_wage\", \"years_of_schooling\"]]\n    .mean()\n    .reset_index()\n    .assign(time_of_birth = lambda d: d[\"year_of_birth\"] + (d[\"quarter_of_birth\"])/4))\n\nplt.figure(figsize=(6,6))\nplt.plot(group_data[\"time_of_birth\"], group_data[\"years_of_schooling\"], zorder=-1)\nfor q in range(1, 5):\n    x = group_data.query(f\"quarter_of_birth=={q}\")[\"time_of_birth\"]\n    y = group_data.query(f\"quarter_of_birth=={q}\")[\"years_of_schooling\"]\n    plt.scatter(x, y, marker=\"s\", s=200, c=f\"C{q}\")\n    plt.scatter(x, y, marker=f\"${q}$\", s=100, c=f\"white\")\n\nplt.title(\"Years of Education by Quarter of Birth (first stage)\")\nplt.xlabel(\"Quarter of Birth\")\nplt.ylabel(\"Years of Schooling\");"
  },
  {
    "objectID": "iv.html#example-returns-to-education-relevance-1",
    "href": "iv.html#example-returns-to-education-relevance-1",
    "title": "Instrumental Variables",
    "section": "Example: Returns to Education : Relevance",
    "text": "Example: Returns to Education : Relevance\n\n\n# Convert the quarter of birth to dummy variables\nfactor_data = df.assign(**{f\"q{int(q)}\": (df[\"quarter_of_birth\"] == q).astype(int)\n                             for q in df[\"quarter_of_birth\"].unique()})\n\n# Run the first stage regression\nimport statsmodels.formula.api as smf\n\nfirst_stage = smf.ols(\"years_of_schooling ~ C(year_of_birth) + C(state_of_birth) + q4\", data=factor_data).fit()\n\nprint(\"q4 parameter estimate:, \", first_stage.params[\"q4\"])\nprint(\"q4 p-value:, \", first_stage.pvalues[\"q4\"])\n\n\nq4 parameter estimate:,  0.10085809272786722\nq4 p-value:,  5.4648294166122615e-15"
  },
  {
    "objectID": "iv.html#example-returns-to-education-exogeneity",
    "href": "iv.html#example-returns-to-education-exogeneity",
    "title": "Instrumental Variables",
    "section": "Example: Returns to Education : Exogeneity",
    "text": "Example: Returns to Education : Exogeneity\n\nNo way to test the exogeneity assumption\n\nSeems okay here\nHard to come up with other reason why quarter of birth would affect earnings"
  },
  {
    "objectID": "iv.html#example-returns-to-education-reduced-form",
    "href": "iv.html#example-returns-to-education-reduced-form",
    "title": "Instrumental Variables",
    "section": "Example: Returns to Education : Reduced Form",
    "text": "Example: Returns to Education : Reduced Form\n\n\n# Plot the reduced form\nplt.figure(figsize=(6,6))\nplt.plot(group_data[\"time_of_birth\"], group_data[\"log_wage\"], zorder=-1)\nfor q in range(1, 5):\n    x = group_data.query(f\"quarter_of_birth=={q}\")[\"time_of_birth\"]\n    y = group_data.query(f\"quarter_of_birth=={q}\")[\"log_wage\"]\n    plt.scatter(x, y, marker=\"s\", s=200, c=f\"C{q}\")\n    plt.scatter(x, y, marker=f\"${q}$\", s=100, c=f\"white\")\n\nplt.title(\"Average Weekly Wage by Quarter of Birth (reduced form)\")\nplt.xlabel(\"Year of Birth\")\nplt.ylabel(\"Log Weekly Earnings\");"
  },
  {
    "objectID": "iv.html#example-returns-to-education-reduced-form-1",
    "href": "iv.html#example-returns-to-education-reduced-form-1",
    "title": "Instrumental Variables",
    "section": "Example: Returns to Education : Reduced Form",
    "text": "Example: Returns to Education : Reduced Form\n\n\n# Run the reduced form\nreduced_form = smf.ols(\"log_wage ~ C(year_of_birth) + C(state_of_birth) + q4\", data=factor_data).fit()\n\nprint(\"q4 parameter estimate:, \", reduced_form.params[\"q4\"])\nprint(\"q4 p-value:, \", reduced_form.pvalues[\"q4\"])\n\n\nq4 parameter estimate:,  0.008603484260139821\nq4 p-value:,  0.001494912718366322"
  },
  {
    "objectID": "iv.html#example-returns-to-education-2sls",
    "href": "iv.html#example-returns-to-education-2sls",
    "title": "Instrumental Variables",
    "section": "Example: Returns to Education : 2SLS",
    "text": "Example: Returns to Education : 2SLS\n\n\nate_iv = reduced_form.params[\"q4\"] / first_stage.params[\"q4\"]\nprint(\"ATE (IV):\", ate_iv)\n\n\nATE (IV): 0.08530286492085"
  },
  {
    "objectID": "iv.html#example-returns-to-education-4",
    "href": "iv.html#example-returns-to-education-4",
    "title": "Instrumental Variables",
    "section": "Example: Returns to Education",
    "text": "Example: Returns to Education\n\nUse a package for IV\n\nivmodels (best support for weak instrument robust inference)\npyfixest.feols\nstatsmodels.sandbox.regression.gmm.IV2SLS\nlinearmodels.iv.IV2SLS"
  },
  {
    "objectID": "iv.html#example-returns-to-education-linearmodels",
    "href": "iv.html#example-returns-to-education-linearmodels",
    "title": "Instrumental Variables",
    "section": "Example: Returns to Education : linearmodels",
    "text": "Example: Returns to Education : linearmodels\n\nfrom linearmodels.iv import IV2SLS\n\nformula = 'log_wage ~ 1 + C(year_of_birth) + C(state_of_birth) + [years_of_schooling ~ q4]'\niv = IV2SLS.from_formula(formula, data=factor_data).fit()\nprint(iv.summary)\n\n                          IV-2SLS Estimation Summary                          \n==============================================================================\nDep. Variable:               log_wage   R-squared:                      0.1217\nEstimator:                    IV-2SLS   Adj. R-squared:                 0.1215\nNo. Observations:              329509   F-statistic:                 1.028e+04\nDate:                Wed, Nov 20 2024   P-value (F-stat)                0.0000\nTime:                        09:41:21   Distribution:                 chi2(60)\nCov. Estimator:                robust                                         \n                                                                              \n                                     Parameter Estimates                                     \n=============================================================================================\n                           Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n---------------------------------------------------------------------------------------------\nIntercept                     4.7468     0.2904     16.348     0.0000      4.1777      5.3158\nC(year_of_birth)[T.31.0]     -0.0078     0.0063    -1.2384     0.2156     -0.0201      0.0045\nC(year_of_birth)[T.32.0]     -0.0145     0.0073    -1.9698     0.0489     -0.0288   -7.25e-05\nC(year_of_birth)[T.33.0]     -0.0174     0.0086    -2.0367     0.0417     -0.0342     -0.0007\nC(year_of_birth)[T.34.0]     -0.0217     0.0094    -2.3012     0.0214     -0.0401     -0.0032\nC(year_of_birth)[T.35.0]     -0.0344     0.0108    -3.1821     0.0015     -0.0556     -0.0132\nC(year_of_birth)[T.36.0]     -0.0347     0.0118    -2.9309     0.0034     -0.0579     -0.0115\nC(year_of_birth)[T.37.0]     -0.0410     0.0132    -3.0976     0.0020     -0.0670     -0.0151\nC(year_of_birth)[T.38.0]     -0.0433     0.0152    -2.8520     0.0043     -0.0730     -0.0135\nC(year_of_birth)[T.39.0]     -0.0547     0.0161    -3.3887     0.0007     -0.0864     -0.0231\nC(state_of_birth)[T.2.0]      0.2272     0.1060     2.1433     0.0321      0.0194      0.4350\nC(state_of_birth)[T.4.0]      0.1215     0.0404     3.0083     0.0026      0.0424      0.2007\nC(state_of_birth)[T.5.0]      0.0431     0.0123     3.5103     0.0004      0.0190      0.0671\nC(state_of_birth)[T.6.0]      0.1351     0.0557     2.4268     0.0152      0.0260      0.2442\nC(state_of_birth)[T.8.0]      0.0920     0.0435     2.1143     0.0345      0.0067      0.1774\nC(state_of_birth)[T.9.0]      0.1051     0.0429     2.4511     0.0142      0.0211      0.1891\nC(state_of_birth)[T.10.0]     0.0796     0.0280     2.8408     0.0045      0.0247      0.1346\nC(state_of_birth)[T.11.0]     0.1162     0.0572     2.0317     0.0422      0.0041      0.2282\nC(state_of_birth)[T.12.0]    -0.0157     0.0278    -0.5651     0.5720     -0.0701      0.0387\nC(state_of_birth)[T.13.0]    -0.0155     0.0119    -1.3003     0.1935     -0.0389      0.0079\nC(state_of_birth)[T.15.0]     0.1256     0.0575     2.1857     0.0288      0.0130      0.2382\nC(state_of_birth)[T.16.0]     0.0718     0.0497     1.4450     0.1485     -0.0256      0.1693\nC(state_of_birth)[T.17.0]     0.1762     0.0429     4.1080     0.0000      0.0921      0.2603\nC(state_of_birth)[T.18.0]     0.1341     0.0288     4.6494     0.0000      0.0775      0.1906\nC(state_of_birth)[T.19.0]     0.0735     0.0382     1.9251     0.0542     -0.0013      0.1483\nC(state_of_birth)[T.20.0]     0.0420     0.0462     0.9081     0.3638     -0.0486      0.1326\nC(state_of_birth)[T.21.0]     0.1194     0.0153     7.7839     0.0000      0.0893      0.1495\nC(state_of_birth)[T.22.0]     0.0883     0.0150     5.8957     0.0000      0.0589      0.1176\nC(state_of_birth)[T.23.0]    -0.0236     0.0219    -1.0779     0.2811     -0.0666      0.0193\nC(state_of_birth)[T.24.0]     0.1048     0.0225     4.6641     0.0000      0.0608      0.1489\nC(state_of_birth)[T.25.0]     0.0783     0.0463     1.6916     0.0907     -0.0124      0.1691\nC(state_of_birth)[T.26.0]     0.2035     0.0342     5.9550     0.0000      0.1365      0.2705\nC(state_of_birth)[T.27.0]     0.1236     0.0393     3.1445     0.0017      0.0466      0.2006\nC(state_of_birth)[T.28.0]    -0.0170     0.0133    -1.2800     0.2005     -0.0431      0.0090\nC(state_of_birth)[T.29.0]     0.0955     0.0273     3.4927     0.0005      0.0419      0.1491\nC(state_of_birth)[T.30.0]     0.0479     0.0469     1.0222     0.3067     -0.0440      0.1398\nC(state_of_birth)[T.31.0]     0.0610     0.0442     1.3793     0.1678     -0.0257      0.1476\nC(state_of_birth)[T.32.0]     0.1220     0.0609     2.0047     0.0450      0.0027      0.2413\nC(state_of_birth)[T.33.0]     0.0036     0.0294     0.1226     0.9024     -0.0540      0.0611\nC(state_of_birth)[T.34.0]     0.1390     0.0453     3.0702     0.0021      0.0503      0.2277\nC(state_of_birth)[T.35.0]     0.0552     0.0285     1.9373     0.0527     -0.0006      0.1110\nC(state_of_birth)[T.36.0]     0.1254     0.0517     2.4242     0.0153      0.0240      0.2268\nC(state_of_birth)[T.37.0]    -0.0562     0.0100    -5.6174     0.0000     -0.0758     -0.0366\nC(state_of_birth)[T.38.0]     0.1076     0.0360     2.9885     0.0028      0.0370      0.1781\nC(state_of_birth)[T.39.0]     0.1429     0.0328     4.3559     0.0000      0.0786      0.2071\nC(state_of_birth)[T.40.0]     0.0679     0.0349     1.9475     0.0515     -0.0004      0.1362\nC(state_of_birth)[T.41.0]     0.1069     0.0515     2.0749     0.0380      0.0059      0.2079\nC(state_of_birth)[T.42.0]     0.1103     0.0304     3.6256     0.0003      0.0507      0.1699\nC(state_of_birth)[T.44.0]     0.0236     0.0352     0.6715     0.5019     -0.0454      0.0927\nC(state_of_birth)[T.45.0]    -0.0723     0.0163    -4.4362     0.0000     -0.1043     -0.0404\nC(state_of_birth)[T.46.0]     0.0500     0.0419     1.1946     0.2323     -0.0320      0.1321\nC(state_of_birth)[T.47.0]     0.0513     0.0115     4.4653     0.0000      0.0288      0.0738\nC(state_of_birth)[T.48.0]     0.0708     0.0262     2.7014     0.0069      0.0194      0.1221\nC(state_of_birth)[T.49.0]     0.1001     0.0590     1.6968     0.0897     -0.0155      0.2158\nC(state_of_birth)[T.50.0]    -0.0508     0.0276    -1.8360     0.0664     -0.1049      0.0034\nC(state_of_birth)[T.51.0]     0.0362     0.0126     2.8697     0.0041      0.0115      0.0610\nC(state_of_birth)[T.53.0]     0.1533     0.0511     2.9998     0.0027      0.0532      0.2535\nC(state_of_birth)[T.54.0]     0.1256     0.0112     11.224     0.0000      0.1037      0.1476\nC(state_of_birth)[T.55.0]     0.1051     0.0337     3.1212     0.0018      0.0391      0.1711\nC(state_of_birth)[T.56.0]     0.1100     0.0548     2.0086     0.0446      0.0027      0.2174\nyears_of_schooling            0.0853     0.0255     3.3399     0.0008      0.0352      0.1354\n=============================================================================================\n\nEndogenous: years_of_schooling\nInstruments: q4\nRobust Covariance (Heteroskedastic)\nDebiased: False"
  },
  {
    "objectID": "iv.html#example-returns-to-education-pyfixest",
    "href": "iv.html#example-returns-to-education-pyfixest",
    "title": "Instrumental Variables",
    "section": "Example: Returns to Education : pyfixest",
    "text": "Example: Returns to Education : pyfixest\n\nimport pyfixest as pf\nivpf = pf.feols('log_wage ~ 1 | state_of_birth + year_of_birth | years_of_schooling ~ q4', data=factor_data, vcov=\"HC1\")\npf.etable(ivpf, type='md')\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\nindex                    est1\n------------------  ---------\ndepvar               log_wage\n-----------------------------\nyears_of_schooling  0.085***\n                      (0.026)\n-----------------------------\nyear_of_birth               x\nstate_of_birth              x\n-----------------------------\nObservations           329509\nS.E. type              hetero\nR2                          -\n-----------------------------"
  },
  {
    "objectID": "iv.html#example-returns-to-education-ivmodels",
    "href": "iv.html#example-returns-to-education-ivmodels",
    "title": "Instrumental Variables",
    "section": "Example: Returns to Education : ivmodels",
    "text": "Example: Returns to Education : ivmodels\n\nfrom patsy import dmatrices\nimport ivmodels\ny,X = dmatrices(\"log_wage ~ 0 + years_of_schooling\", data=factor_data, return_type='dataframe')\nZ,controls = dmatrices(\"q4 ~  C(year_of_birth) + C(state_of_birth)\", data=factor_data, return_type='dataframe')\ncontrols.drop(\"Intercept\", axis=1,inplace=True)\n\nivm = ivmodels.KClass(1).fit(X=X,y=y,Z=Z,C=controls)\nsw = ivm.summary(X=X,y=y,Z=Z,C=controls, test='wald', feature_names=[\"years_of_schooling\"])\nprint(sw)\n\nSummary based on the wald test.\n\n                    estimate  statistic    p-value          conf. set\nyears_of_schooling    0.0853      11.15  0.0008402  [0.03523, 0.1354]\n\nEndogenous model statistic: 11.15, p-value: 0.0008402\n(Multivariate) F-statistic: 61.09, p-value: 5.44e-15\n\n\n\nsa = ivm.summary(X=X,y=y,Z=Z,C=controls, test='anderson-rubin', feature_names=[\"years_of_schooling\"])\nprint(sa)\n\nSummary based on the anderson-rubin test.\n\n                    estimate  statistic   p-value          conf. set\nyears_of_schooling    0.0853      10.09  0.001495  [0.03477, 0.1382]\n\nEndogenous model statistic: 10.09, p-value: 0.001495\n(Multivariate) F-statistic: 61.09, p-value: 5.44e-15"
  },
  {
    "objectID": "iv.html#weak-vs-strong-instruments",
    "href": "iv.html#weak-vs-strong-instruments",
    "title": "Instrumental Variables",
    "section": "Weak vs Strong Instruments",
    "text": "Weak vs Strong Instruments\n\nStandard errors for instrumental variables are larger than for OLS.\nThe difference comes from the fact that our prediction of \\(T\\) from just \\(R\\) is imperfect.\nThe more correlated \\(R\\) is with \\(T\\), the better our prediction will be.\n\nIf \\(\\cov(R,T)\\) is large, we call \\(R\\) a strong instrument.\nIf \\(\\cov(R,T)\\) is small, we call \\(R\\) a weak instrument."
  },
  {
    "objectID": "iv.html#weak-vs-strong-instruments-1",
    "href": "iv.html#weak-vs-strong-instruments-1",
    "title": "Instrumental Variables",
    "section": "Weak vs Strong Instruments",
    "text": "Weak vs Strong Instruments\n\nTo explore the 2SLS estimator, we could use Monte-Carlo simulation.\nWe’ll simulate data from the following model:\n\n\n\n\\[\n\\begin{aligned}\n& X \\sim N\\left(0,2^2\\right) \\\\\n& U \\sim N\\left(0,2^2\\right) \\\\\n& T \\sim N\\left(1+0.5 U, 5^2\\right) \\\\\n& Y \\sim N\\left(2+X-0.5 U+2 T, 5^2\\right) \\\\\n& Z \\sim N\\left(T, \\sigma^2\\right) \\text { for } \\sigma^2 \\text { in } 0.1 \\text { to } 100\n\\end{aligned}\n\\]\n\n\n\ng = gr.Digraph()\ng.edge('U', 'T')\ng.edge('X', 'Y')\ng.edge('U', 'Y')\ng.edge('T', 'Y')\ng.edge('T', 'Z')\ng"
  },
  {
    "objectID": "iv.html#weak-vs-strong-instruments-2",
    "href": "iv.html#weak-vs-strong-instruments-2",
    "title": "Instrumental Variables",
    "section": "Weak vs Strong Instruments",
    "text": "Weak vs Strong Instruments\n\nimport numpy as np\nnp.random.seed(12)\nn = 10000\nX = np.random.normal(0, 2, n) # observable variable\nU = np.random.normal(0, 2, n) # unobservable (omitted) variable\nT = np.random.normal(1 + 0.5*U, 5, n) # treatment\nY = np.random.normal(2 + X - 0.5*U + 2*T, 5, n) # outcome\n\nstddevs = np.linspace(0.1, 100, 50)\nZs = {f\"Z_{z}\": np.random.normal(T, s, n) for z, s in enumerate(stddevs)} # instruments with decreasing Cov(Z, T)\n\nsim_data = pd.DataFrame(dict(U=U, T=T, Y=Y)).assign(**Zs)\n\nsim_data.head()\n\n\n\n\n\n\n\n\nU\nT\nY\nZ_0\nZ_1\nZ_2\nZ_3\nZ_4\nZ_5\nZ_6\n...\nZ_40\nZ_41\nZ_42\nZ_43\nZ_44\nZ_45\nZ_46\nZ_47\nZ_48\nZ_49\n\n\n\n\n0\n2.696148\n8.056988\n18.388910\n8.233315\n9.028779\n16.430365\n7.348864\n4.848165\n1.567900\n16.654138\n...\n50.836515\n-121.965878\n-9.118833\n-42.297891\n17.015717\n51.777219\n78.678074\n-164.581598\n-117.798705\n-13.485292\n\n\n1\n2.570240\n0.245067\n2.015052\n0.455988\n-0.901285\n-6.442245\n-2.824902\n7.327944\n-9.643833\n14.237652\n...\n29.966537\n141.855092\n79.040440\n66.249629\n-23.107153\n25.007034\n-73.621303\n85.867339\n-209.727577\n-70.792948\n\n\n2\n0.664741\n5.597510\n11.939170\n5.528384\n6.148148\n10.141348\n18.923875\n-5.550785\n4.495364\n-14.070897\n...\n-29.051441\n14.537511\n-95.846490\n-117.922132\n43.194916\n58.534855\n119.820024\n-173.513340\n60.562232\n47.619414\n\n\n3\n1.037725\n0.493532\n-5.077869\n0.382075\n0.790127\n-2.753808\n3.146698\n-7.152174\n-6.322238\n9.209916\n...\n-61.446478\n26.719702\n-40.753912\n63.725307\n22.462409\n97.200099\n-116.309759\n-26.328707\n78.136513\n-108.322304\n\n\n4\n-2.590591\n-6.263014\n-6.460508\n-6.197533\n-5.954731\n-19.295207\n-11.343303\n-7.231806\n5.556399\n-17.913401\n...\n-28.071993\n146.111732\n-21.991256\n88.258432\n62.211154\n-72.066362\n51.848504\n-117.858043\n78.776566\n-80.547214\n\n\n\n\n5 rows × 53 columns"
  },
  {
    "objectID": "iv.html#weak-vs-strong-instruments-3",
    "href": "iv.html#weak-vs-strong-instruments-3",
    "title": "Instrumental Variables",
    "section": "Weak vs Strong Instruments",
    "text": "Weak vs Strong Instruments\n\nNow we can run the 2SLS estimator for each value of \\(\\sigma\\), as the covariance between the instrument and treatment changes.\n\nWe can see that the standard error of the 2SLS estimator increases as the covariance decreases.\n\n\n\n\ncorr = (sim_data.corr()[\"T\"]\n    [lambda d: d.index.str.startswith(\"Z\")])\n\nse = []\nate = []\nfor z in range(len(Zs)):\n    formula = f'Y ~ 1 + X + [T ~ Z_{z}]'\n    iv = IV2SLS.from_formula(formula, sim_data).fit()\n    se.append(iv.std_errors[\"T\"])\n    ate.append(iv.params[\"T\"])\n\nplot_data = pd.DataFrame(dict(se=se, ate=ate, corr=corr)).sort_values(by=\"corr\")\n\nplt.scatter(plot_data[\"corr\"], plot_data[\"se\"])\nplt.xlabel(\"Corr(Z, T)\")\nplt.ylabel(\"IV Standard Error\");\nplt.title(\"Variance of the IV Estimates by 1st Stage Strength\");"
  },
  {
    "objectID": "iv.html#bias-of-ols",
    "href": "iv.html#bias-of-ols",
    "title": "Instrumental Variables",
    "section": "Bias of OLS",
    "text": "Bias of OLS\n\nNow, let’s look at the point estimates themselves\n\n\n\nplt.scatter(plot_data[\"corr\"], plot_data[\"ate\"])\nplt.fill_between(plot_data[\"corr\"],\n    plot_data[\"ate\"]+1.96*plot_data[\"se\"],\n    plot_data[\"ate\"]-1.96*plot_data[\"se\"], alpha=.5)\nplt.xlabel(\"Corr(Z, T)\")\nplt.ylabel(\"$\\hat{ATE}$\");\nplt.title(\"IV ATE Estimates by 1st Stage Strength\");"
  },
  {
    "objectID": "iv.html#bias-of-2sls",
    "href": "iv.html#bias-of-2sls",
    "title": "Instrumental Variables",
    "section": "Bias of 2SLS",
    "text": "Bias of 2SLS\n\nWe can see that the estimates from 2SLS are still biased.\nThis is because the instrument is not able to “mimic” the randomized experiment as well, there will always be some variation in \\(T\\) that is not explained by \\(Z\\).\n2SLS is biased in the same direction as OLS would be.\n\nBut it’s consistent!\nSo we can get rid of the bias by increasing the sample size."
  },
  {
    "objectID": "iv.html#instrumental-variables-estimator",
    "href": "iv.html#instrumental-variables-estimator",
    "title": "Instrumental Variables",
    "section": "Instrumental Variables Estimator",
    "text": "Instrumental Variables Estimator\n\nAssumed \\(0 = \\Er[Z_i \\epsilon_i] = \\Er[Z_i(y_i - X_i'\\beta_0)]\\)\nEstimate by solving \\[\n0 \\approx \\frac{1}{n} \\sum_{i=1}^n Z_i(y_i - X_i'\\hat{\\beta}^{2SLS})\n\\]\n\\(\\hat{\\beta}^{2SLS}  = (X'Z (Z'Z)^{-1} Z' X)^{-1} (X'Z(Z'Z)^{-1}Z'y)\\)"
  },
  {
    "objectID": "iv.html#two-stage-least-squares",
    "href": "iv.html#two-stage-least-squares",
    "title": "Instrumental Variables",
    "section": "Two Stage Least Squares",
    "text": "Two Stage Least Squares\n\\[\n\\begin{align*}\n\\hat{\\beta}^{2SLS} & = (X'Z (Z'Z)^{-1} Z' X)^{-1} (X'Z(Z'Z)^{-1}Z'y) \\\\\n& = (X'P_Z X)^{-1} (X' P_Z y) \\\\\n& = ((P_Z X)'(P_Z X))^{-1} ((P_Z X)'y)\n\\end{align*}\n\\]\n\nRegress \\(X\\) on \\(Z\\), let \\(\\hat{X} = P_Z X\\)\nRegress \\(y\\) on \\(\\hat{X}\\)"
  },
  {
    "objectID": "iv.html#iv-best-practices",
    "href": "iv.html#iv-best-practices",
    "title": "Instrumental Variables",
    "section": "IV Best Practices",
    "text": "IV Best Practices\n\nReport the first stage regression of \\(X\\) on \\(Z\\)\n\nCheck for relevance\n\nReport the reduced form regression of \\(Y\\) on \\(Z\\)\nIf relevance at all in doubt, use weak identification robust confidence intervals\n\nE.g. ivmodels.summary with test='lagrange multiplier'"
  },
  {
    "objectID": "iv.html#example-relevance",
    "href": "iv.html#example-relevance",
    "title": "Instrumental Variables",
    "section": "Example: Relevance",
    "text": "Example: Relevance\n\n\nimport matplotlib.pyplot as plt\ngroup_data = (df\n    .groupby([\"year_of_birth\", \"quarter_of_birth\"])\n    [[\"log_wage\", \"years_of_schooling\"]]\n    .mean()\n    .reset_index()\n    .assign(time_of_birth = lambda d: d[\"year_of_birth\"] + (d[\"quarter_of_birth\"])/4))\n\nplt.figure(figsize=(6,6))\nplt.plot(group_data[\"time_of_birth\"], group_data[\"years_of_schooling\"], zorder=-1)\nfor q in range(1, 5):\n    x = group_data.query(f\"quarter_of_birth=={q}\")[\"time_of_birth\"]\n    y = group_data.query(f\"quarter_of_birth=={q}\")[\"years_of_schooling\"]\n    plt.scatter(x, y, marker=\"s\", s=200, c=f\"C{q}\")\n    plt.scatter(x, y, marker=f\"${q}$\", s=100, c=f\"white\")\n\nplt.title(\"Years of Education by Quarter of Birth (first stage)\")\nplt.xlabel(\"Quarter of Birth\")\nplt.ylabel(\"Years of Schooling\");"
  },
  {
    "objectID": "iv.html#example-relevance-1",
    "href": "iv.html#example-relevance-1",
    "title": "Instrumental Variables",
    "section": "Example: Relevance",
    "text": "Example: Relevance\n\n\n# Convert the quarter of birth to dummy variables\nfactor_data = df.assign(**{f\"q{int(q)}\": (df[\"quarter_of_birth\"] == q).astype(int)\n                             for q in df[\"quarter_of_birth\"].unique()})\n\n# Run the first stage regression\nimport statsmodels.formula.api as smf\n\nfirst_stage = smf.ols(\"years_of_schooling ~ C(year_of_birth) + C(state_of_birth) + q4\", data=factor_data).fit()\n\nprint(\"q4 parameter estimate:, \", first_stage.params[\"q4\"])\nprint(\"q4 p-value:, \", first_stage.pvalues[\"q4\"])\n\n\nq4 parameter estimate:,  0.10085809272786722\nq4 p-value:,  5.4648294166122615e-15"
  },
  {
    "objectID": "iv.html#example-exogeneity",
    "href": "iv.html#example-exogeneity",
    "title": "Instrumental Variables",
    "section": "Example: Exogeneity",
    "text": "Example: Exogeneity\n\nNo way to test the exogeneity assumption\n\nSeems okay here\nHard to come up with other reason why quarter of birth would affect earnings"
  },
  {
    "objectID": "iv.html#example-reduced-form",
    "href": "iv.html#example-reduced-form",
    "title": "Instrumental Variables",
    "section": "Example: Reduced Form",
    "text": "Example: Reduced Form\n\n\n# Plot the reduced form\nplt.figure(figsize=(6,6))\nplt.plot(group_data[\"time_of_birth\"], group_data[\"log_wage\"], zorder=-1)\nfor q in range(1, 5):\n    x = group_data.query(f\"quarter_of_birth=={q}\")[\"time_of_birth\"]\n    y = group_data.query(f\"quarter_of_birth=={q}\")[\"log_wage\"]\n    plt.scatter(x, y, marker=\"s\", s=200, c=f\"C{q}\")\n    plt.scatter(x, y, marker=f\"${q}$\", s=100, c=f\"white\")\n\nplt.title(\"Average Weekly Wage by Quarter of Birth (reduced form)\")\nplt.xlabel(\"Year of Birth\")\nplt.ylabel(\"Log Weekly Earnings\");"
  },
  {
    "objectID": "iv.html#example-reduced-form-1",
    "href": "iv.html#example-reduced-form-1",
    "title": "Instrumental Variables",
    "section": "Example: Reduced Form",
    "text": "Example: Reduced Form\n\n\n# Run the reduced form\nreduced_form = smf.ols(\"log_wage ~ C(year_of_birth) + C(state_of_birth) + q4\", data=factor_data).fit()\n\nprint(\"q4 parameter estimate:, \", reduced_form.params[\"q4\"])\nprint(\"q4 p-value:, \", reduced_form.pvalues[\"q4\"])\n\n\nq4 parameter estimate:,  0.008603484260139821\nq4 p-value:,  0.001494912718366322"
  },
  {
    "objectID": "iv.html#example-2sls",
    "href": "iv.html#example-2sls",
    "title": "Instrumental Variables",
    "section": "Example: 2SLS",
    "text": "Example: 2SLS\n\n\nate_iv = reduced_form.params[\"q4\"] / first_stage.params[\"q4\"]\nprint(\"ATE (IV):\", ate_iv)\n\n\nATE (IV): 0.08530286492085"
  },
  {
    "objectID": "iv.html#example-linearmodels",
    "href": "iv.html#example-linearmodels",
    "title": "Instrumental Variables",
    "section": "Example: linearmodels",
    "text": "Example: linearmodels\n\nfrom linearmodels.iv import IV2SLS\n\nformula = 'log_wage ~ 1 + C(year_of_birth) + C(state_of_birth) + [years_of_schooling ~ q4]'\niv = IV2SLS.from_formula(formula, data=factor_data).fit()\nprint(iv.summary)\n\n                          IV-2SLS Estimation Summary                          \n==============================================================================\nDep. Variable:               log_wage   R-squared:                      0.1217\nEstimator:                    IV-2SLS   Adj. R-squared:                 0.1215\nNo. Observations:              329509   F-statistic:                 1.028e+04\nDate:                Wed, Nov 20 2024   P-value (F-stat)                0.0000\nTime:                        12:02:10   Distribution:                 chi2(60)\nCov. Estimator:                robust                                         \n                                                                              \n                                     Parameter Estimates                                     \n=============================================================================================\n                           Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n---------------------------------------------------------------------------------------------\nIntercept                     4.7468     0.2904     16.348     0.0000      4.1777      5.3158\nC(year_of_birth)[T.31.0]     -0.0078     0.0063    -1.2384     0.2156     -0.0201      0.0045\nC(year_of_birth)[T.32.0]     -0.0145     0.0073    -1.9698     0.0489     -0.0288   -7.25e-05\nC(year_of_birth)[T.33.0]     -0.0174     0.0086    -2.0367     0.0417     -0.0342     -0.0007\nC(year_of_birth)[T.34.0]     -0.0217     0.0094    -2.3012     0.0214     -0.0401     -0.0032\nC(year_of_birth)[T.35.0]     -0.0344     0.0108    -3.1821     0.0015     -0.0556     -0.0132\nC(year_of_birth)[T.36.0]     -0.0347     0.0118    -2.9309     0.0034     -0.0579     -0.0115\nC(year_of_birth)[T.37.0]     -0.0410     0.0132    -3.0976     0.0020     -0.0670     -0.0151\nC(year_of_birth)[T.38.0]     -0.0433     0.0152    -2.8520     0.0043     -0.0730     -0.0135\nC(year_of_birth)[T.39.0]     -0.0547     0.0161    -3.3887     0.0007     -0.0864     -0.0231\nC(state_of_birth)[T.2.0]      0.2272     0.1060     2.1433     0.0321      0.0194      0.4350\nC(state_of_birth)[T.4.0]      0.1215     0.0404     3.0083     0.0026      0.0424      0.2007\nC(state_of_birth)[T.5.0]      0.0431     0.0123     3.5103     0.0004      0.0190      0.0671\nC(state_of_birth)[T.6.0]      0.1351     0.0557     2.4268     0.0152      0.0260      0.2442\nC(state_of_birth)[T.8.0]      0.0920     0.0435     2.1143     0.0345      0.0067      0.1774\nC(state_of_birth)[T.9.0]      0.1051     0.0429     2.4511     0.0142      0.0211      0.1891\nC(state_of_birth)[T.10.0]     0.0796     0.0280     2.8408     0.0045      0.0247      0.1346\nC(state_of_birth)[T.11.0]     0.1162     0.0572     2.0317     0.0422      0.0041      0.2282\nC(state_of_birth)[T.12.0]    -0.0157     0.0278    -0.5651     0.5720     -0.0701      0.0387\nC(state_of_birth)[T.13.0]    -0.0155     0.0119    -1.3003     0.1935     -0.0389      0.0079\nC(state_of_birth)[T.15.0]     0.1256     0.0575     2.1857     0.0288      0.0130      0.2382\nC(state_of_birth)[T.16.0]     0.0718     0.0497     1.4450     0.1485     -0.0256      0.1693\nC(state_of_birth)[T.17.0]     0.1762     0.0429     4.1080     0.0000      0.0921      0.2603\nC(state_of_birth)[T.18.0]     0.1341     0.0288     4.6494     0.0000      0.0775      0.1906\nC(state_of_birth)[T.19.0]     0.0735     0.0382     1.9251     0.0542     -0.0013      0.1483\nC(state_of_birth)[T.20.0]     0.0420     0.0462     0.9081     0.3638     -0.0486      0.1326\nC(state_of_birth)[T.21.0]     0.1194     0.0153     7.7839     0.0000      0.0893      0.1495\nC(state_of_birth)[T.22.0]     0.0883     0.0150     5.8957     0.0000      0.0589      0.1176\nC(state_of_birth)[T.23.0]    -0.0236     0.0219    -1.0779     0.2811     -0.0666      0.0193\nC(state_of_birth)[T.24.0]     0.1048     0.0225     4.6641     0.0000      0.0608      0.1489\nC(state_of_birth)[T.25.0]     0.0783     0.0463     1.6916     0.0907     -0.0124      0.1691\nC(state_of_birth)[T.26.0]     0.2035     0.0342     5.9550     0.0000      0.1365      0.2705\nC(state_of_birth)[T.27.0]     0.1236     0.0393     3.1445     0.0017      0.0466      0.2006\nC(state_of_birth)[T.28.0]    -0.0170     0.0133    -1.2800     0.2005     -0.0431      0.0090\nC(state_of_birth)[T.29.0]     0.0955     0.0273     3.4927     0.0005      0.0419      0.1491\nC(state_of_birth)[T.30.0]     0.0479     0.0469     1.0222     0.3067     -0.0440      0.1398\nC(state_of_birth)[T.31.0]     0.0610     0.0442     1.3793     0.1678     -0.0257      0.1476\nC(state_of_birth)[T.32.0]     0.1220     0.0609     2.0047     0.0450      0.0027      0.2413\nC(state_of_birth)[T.33.0]     0.0036     0.0294     0.1226     0.9024     -0.0540      0.0611\nC(state_of_birth)[T.34.0]     0.1390     0.0453     3.0702     0.0021      0.0503      0.2277\nC(state_of_birth)[T.35.0]     0.0552     0.0285     1.9373     0.0527     -0.0006      0.1110\nC(state_of_birth)[T.36.0]     0.1254     0.0517     2.4242     0.0153      0.0240      0.2268\nC(state_of_birth)[T.37.0]    -0.0562     0.0100    -5.6174     0.0000     -0.0758     -0.0366\nC(state_of_birth)[T.38.0]     0.1076     0.0360     2.9885     0.0028      0.0370      0.1781\nC(state_of_birth)[T.39.0]     0.1429     0.0328     4.3559     0.0000      0.0786      0.2071\nC(state_of_birth)[T.40.0]     0.0679     0.0349     1.9475     0.0515     -0.0004      0.1362\nC(state_of_birth)[T.41.0]     0.1069     0.0515     2.0749     0.0380      0.0059      0.2079\nC(state_of_birth)[T.42.0]     0.1103     0.0304     3.6256     0.0003      0.0507      0.1699\nC(state_of_birth)[T.44.0]     0.0236     0.0352     0.6715     0.5019     -0.0454      0.0927\nC(state_of_birth)[T.45.0]    -0.0723     0.0163    -4.4362     0.0000     -0.1043     -0.0404\nC(state_of_birth)[T.46.0]     0.0500     0.0419     1.1946     0.2323     -0.0320      0.1321\nC(state_of_birth)[T.47.0]     0.0513     0.0115     4.4653     0.0000      0.0288      0.0738\nC(state_of_birth)[T.48.0]     0.0708     0.0262     2.7014     0.0069      0.0194      0.1221\nC(state_of_birth)[T.49.0]     0.1001     0.0590     1.6968     0.0897     -0.0155      0.2158\nC(state_of_birth)[T.50.0]    -0.0508     0.0276    -1.8360     0.0664     -0.1049      0.0034\nC(state_of_birth)[T.51.0]     0.0362     0.0126     2.8697     0.0041      0.0115      0.0610\nC(state_of_birth)[T.53.0]     0.1533     0.0511     2.9998     0.0027      0.0532      0.2535\nC(state_of_birth)[T.54.0]     0.1256     0.0112     11.224     0.0000      0.1037      0.1476\nC(state_of_birth)[T.55.0]     0.1051     0.0337     3.1212     0.0018      0.0391      0.1711\nC(state_of_birth)[T.56.0]     0.1100     0.0548     2.0086     0.0446      0.0027      0.2174\nyears_of_schooling            0.0853     0.0255     3.3399     0.0008      0.0352      0.1354\n=============================================================================================\n\nEndogenous: years_of_schooling\nInstruments: q4\nRobust Covariance (Heteroskedastic)\nDebiased: False"
  },
  {
    "objectID": "iv.html#example-pyfixest",
    "href": "iv.html#example-pyfixest",
    "title": "Instrumental Variables",
    "section": "Example: pyfixest",
    "text": "Example: pyfixest\n\nimport pyfixest as pf\nivpf = pf.feols('log_wage ~ 1 | state_of_birth + year_of_birth | years_of_schooling ~ q4', data=factor_data, vcov=\"HC1\")\npf.etable(ivpf, type='md')\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\nindex                    est1\n------------------  ---------\ndepvar               log_wage\n-----------------------------\nyears_of_schooling  0.085***\n                      (0.026)\n-----------------------------\nyear_of_birth               x\nstate_of_birth              x\n-----------------------------\nObservations           329509\nS.E. type              hetero\nR2                          -\n-----------------------------"
  },
  {
    "objectID": "iv.html#example-ivmodels",
    "href": "iv.html#example-ivmodels",
    "title": "Instrumental Variables",
    "section": "Example: ivmodels",
    "text": "Example: ivmodels\n\nfrom patsy import dmatrices\nimport ivmodels\ny,X = dmatrices(\"log_wage ~ 0 + years_of_schooling\", data=factor_data, return_type='dataframe')\nZ,controls = dmatrices(\"q4 ~  C(year_of_birth) + C(state_of_birth)\", data=factor_data, return_type='dataframe')\ncontrols.drop(\"Intercept\", axis=1,inplace=True)\n\nivm = ivmodels.KClass(1).fit(X=X,y=y,Z=Z,C=controls)\nsw = ivm.summary(X=X,y=y,Z=Z,C=controls, test='wald', feature_names=[\"years_of_schooling\"])\nprint(sw)\n\nSummary based on the wald test.\n\n                    estimate  statistic    p-value          conf. set\nyears_of_schooling    0.0853      11.15  0.0008402  [0.03523, 0.1354]\n\nEndogenous model statistic: 11.15, p-value: 0.0008402\n(Multivariate) F-statistic: 61.09, p-value: 5.44e-15"
  },
  {
    "objectID": "iv.html#weak-instruments-1",
    "href": "iv.html#weak-instruments-1",
    "title": "Instrumental Variables",
    "section": "Weak Instruments",
    "text": "Weak Instruments\n\nOne endogenous regressor, one instrument\n\nStructural equation \\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\nFirst stage \\[\nx_i = \\pi_0^x + \\pi_1^x z_i + u_i^x\n\\]\nReduced Form \\[\ny_i = \\pi_0^y + \\pi_1^y z_i + u_i^y\n\\]\n\n\\(\\text{2SLS} = \\frac{\\text{Reduced Form}}{\\text{First Stage}}\\)\n\\(\\hat{\\beta}^{2SLS}_1 = \\frac{\\hat{\\pi}_1^y}{\\hat{\\pi}_1^x}\\)"
  },
  {
    "objectID": "iv.html#weak-instruments-2",
    "href": "iv.html#weak-instruments-2",
    "title": "Instrumental Variables",
    "section": "Weak Instruments",
    "text": "Weak Instruments\n\n\\(\\hat{\\beta}^{2SLS}_1 = \\frac{\\hat{\\pi}_1^y}{\\hat{\\pi}_1^x}\\)\nIf \\(z\\) not relevant, then divide by \\(0\\)\nEven if \\(z\\) relevant, if \\(\\hat{\\pi}_1^x\\) is too likely near 0, then \\(\\hat{\\beta}^{2SLS}_1\\) will be messed up\n\nusual asymptotic normal distribution becomes a poor approximation to true finite sample distribution of \\(\\hat{\\beta}^{2SLS}_1\\)\n\n“too likely near 0” is more common than one might think"
  },
  {
    "objectID": "iv.html#weak-instrument-simulation",
    "href": "iv.html#weak-instrument-simulation",
    "title": "Instrumental Variables",
    "section": "Weak Instrument Simulation",
    "text": "Weak Instrument Simulation\n\nFirst stage \\(X = Z\\gamma + e\\), simulation with \\(\\Er[Z_i Z_i] = I\\) and \\(e \\sim N(0,0.25)\\), so first stage \\(t \\approx \\sqrt{n}\\gamma/0.5\\)\nDistribution of \\(\\hat{\\beta}^{2SLS}\\) with \\(\\gamma = 1\\), \\(\\gamma=0.5\\), \\(\\gamma=0.2\\), and \\(\\gamma=0.1\\) and \\(n=100\\)\n\nimplies first stage t-statistics of \\(20\\), \\(10\\), \\(4\\), and \\(2\\)"
  },
  {
    "objectID": "iv.html#weak-instrument-simulation-hatbeta2sls",
    "href": "iv.html#weak-instrument-simulation-hatbeta2sls",
    "title": "Instrumental Variables",
    "section": "Weak Instrument Simulation : \\(\\hat{\\beta}^{2SLS}\\)",
    "text": "Weak Instrument Simulation : \\(\\hat{\\beta}^{2SLS}\\)\n\nimport seaborn as sns\nimport numpy as np\nimport scipy\nn = 100\ngamma = [1,0.5, 0.2,0.1]\nsig = 0.5\nExe=1.0\n\ndef simiv(n,gamma,sig, b0, Exe):\n   z = np.random.randn(n)\n   xe = np.random.randn(n)\n   x = z*gamma + (xe*Exe + np.random.randn(n))/np.sqrt(1+Exe**2)*sig\n   e = xe + np.random.randn(n)\n   y = x*b0 + e\n   return(y,x,z)\n\ndef b2sls(y,x,z) :\n    return(np.dot(z,y)/np.dot(z,x))\n\nS = 10_000\nfig, ax = plt.subplots(2,2)\nx = np.linspace(-3,3,num=300)\nb0 = 1\n\nfor j in range(len(gamma)):\n    se = np.sqrt(2/n)/(gamma[j])\n    phix = scipy.stats.norm(loc=b0,scale=se).pdf(x*se + b0)\n    b = np.array([(lambda x: b2sls(x[0],x[1],x[2]))(simiv(n,gamma[j],sig,b0,Exe)) for _ in range(S)])\n    sns.histplot(b, ax=ax.flat[j], stat='density')\n    ax.flat[j].set_title(f\"First stage E[t]={np.sqrt(n)*gamma[j]/sig:.3}\")\n    ax.flat[j].plot(x*se+b0,phix)\n    ax.flat[j].set_xlim(-3*se +1,3*se+1)"
  },
  {
    "objectID": "iv.html#weak-instrument-simulation-hatbeta2sls-output",
    "href": "iv.html#weak-instrument-simulation-hatbeta2sls-output",
    "title": "Instrumental Variables",
    "section": "Weak Instrument Simulation : \\(\\hat{\\beta}^{2SLS}\\)",
    "text": "Weak Instrument Simulation : \\(\\hat{\\beta}^{2SLS}\\)"
  },
  {
    "objectID": "iv.html#weak-instrument-simulation-t-stat",
    "href": "iv.html#weak-instrument-simulation-t-stat",
    "title": "Instrumental Variables",
    "section": "Weak Instrument Simulation : t-stat",
    "text": "Weak Instrument Simulation : t-stat\n\ndef t2sls(y,x,z, b0=1) :\n    b = b2sls(y,x,z)\n    u = y - x*b\n    se = np.sqrt(u.var()*np.dot(z,z)/(np.dot(x,z)**2))\n    t = (b-b0)/se\n    return(t)\n\nS = 10_000\nfig, ax = plt.subplots(2,2)\nx = np.linspace(-3,3,num=300)\nphix = scipy.stats.norm.pdf(x)\nfor j in range(len(gamma)):\n    ts = np.array([(lambda x: t2sls(x[0],x[1],x[2],b0=b0))(simiv(n,gamma[j],sig,b0,Exe)) for _ in range(S)])\n    sns.histplot(ts, ax=ax.flat[j], stat='density')\n    ax.flat[j].set_title(f\"First stage E[t]={np.sqrt(n)*gamma[j]/sig:.3}\")\n    ax.flat[j].plot(x,phix)\n    ax.flat[j].set_xlim(-3,3)"
  },
  {
    "objectID": "iv.html#weak-instrument-simulation-t-stat-output",
    "href": "iv.html#weak-instrument-simulation-t-stat-output",
    "title": "Instrumental Variables",
    "section": "Weak Instrument Simulation : t-stat",
    "text": "Weak Instrument Simulation : t-stat"
  },
  {
    "objectID": "iv.html#weak-instruments-3",
    "href": "iv.html#weak-instruments-3",
    "title": "Instrumental Variables",
    "section": "Weak Instruments",
    "text": "Weak Instruments\n\nLessons from simulation:\n\nWhen \\(\\Er[Z_i X_i']\\) is small, usual asymptotic distribution is a poor approximation for the finite sample distribution of \\(\\hat{\\beta}^{IV}\\)\nThe approximation can be poor even when \\(H_0: \\gamma = 0\\) in \\(X = Z\\gamma + e\\) would be rejected"
  },
  {
    "objectID": "iv.html#first-stage-f-statistic",
    "href": "iv.html#first-stage-f-statistic",
    "title": "Instrumental Variables",
    "section": "First stage F-statistic",
    "text": "First stage F-statistic\n\nAlways report first stage “effective” F-statistic\n\n\ny,x,z = simiv(n,gamma[3],sig, b0, Exe)\nmodel=pf.feols('y ~ 1 | 0 | x ~ z', pd.DataFrame({'y':y,'x':x, 'z': z}))\nmodel.IV_Diag()\nmodel._eff_F\n\nnp.float64(10.154922831994867)\n\n\n\nIf \\(F &gt;&gt; 100\\), everything okay\n\n(old advice of \\(F&gt;10\\) is not enough)\nEven larger needed if more instruments\n\nElse, use different method for confidence intervals and testing"
  },
  {
    "objectID": "iv.html#identification-robust-inference",
    "href": "iv.html#identification-robust-inference",
    "title": "Instrumental Variables",
    "section": "Identification Robust Inference",
    "text": "Identification Robust Inference\n\n\nCan we find a better approximation to the finite sample distribution when \\(\\Er[Z_i X_i']\\) is small?\nYes, two approaches lead to the same answers:\n\nRequire uniform (over values of \\(\\Er[Z_iX_i']\\)) converge\nWeak instrument asymptotics (instead of \\(\\Er[Z_i X_i]\\) fixed with \\(n\\), suppose \\(\\Er[Z_i X_i'] = C/\\sqrt{n}\\))\n\nIdentification robust tests:\n\nAnderson Rubin (okay when just identified — number of instruments = number of endogenous regressors)\nConditional likelihood ratio\nLagrange multiplier"
  },
  {
    "objectID": "iv.html#identification-robust-inference-1",
    "href": "iv.html#identification-robust-inference-1",
    "title": "Instrumental Variables",
    "section": "Identification Robust Inference",
    "text": "Identification Robust Inference\n\nivmodels from Londschien and Bühlmann (2024) seems like best python package\n\nconvenient confidence intervals\ndownside: assumes homoscedasticity\n\n\n\n\nCode\ny,X = dmatrices(\"log_wage ~ 0 + years_of_schooling\", data=factor_data, return_type='dataframe')\nZ,controls = dmatrices(\"q4 ~  C(year_of_birth) + C(state_of_birth)\", data=factor_data, return_type='dataframe')\ncontrols.drop(\"Intercept\", axis=1,inplace=True)\nivm = ivmodels.KClass(1).fit(X=X,y=y,Z=Z,C=controls)\n\n\n\nsa = ivm.summary(X=X,y=y,Z=Z,C=controls, test='anderson-rubin', feature_names=[\"years_of_schooling\"])\nprint(sa)\n\nSummary based on the anderson-rubin test.\n\n                    estimate  statistic   p-value          conf. set\nyears_of_schooling    0.0853      10.09  0.001495  [0.03477, 0.1382]\n\nEndogenous model statistic: 10.09, p-value: 0.001495\n(Multivariate) F-statistic: 61.09, p-value: 5.44e-15\n\n\n\nsl = ivm.summary(X=X,y=y,Z=Z,C=controls, test='lagrange multiplier', feature_names=[\"years_of_schooling\"])\nprint(sl)\n\nSummary based on the lagrange multiplier test.\n\n                    estimate  statistic   p-value          conf. set\nyears_of_schooling    0.0853      10.09  0.001495  [0.03477, 0.1382]\n\nEndogenous model statistic: 10.09, p-value: 0.001495\n(Multivariate) F-statistic: 61.09, p-value: 5.44e-15"
  }
]